{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download InternVL3-2B Model\n",
    "\n",
    "This notebook downloads the OpenGVLab/InternVL3-2B model to `/home/jovyan/nfs_share/models/` for vision processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install huggingface_hub transformers torch torchvision\n\n# Alternative: Use individual file download for guaranteed flat structure\nUSE_INDIVIDUAL_DOWNLOAD = False  # Set to True for maximum flat file control"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\nfrom huggingface_hub import snapshot_download, hf_hub_download, list_repo_files\nimport torch\n\nprint(f\"üîß PyTorch version: {torch.__version__}\")\nprint(f\"üîß CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"üîß CUDA device count: {torch.cuda.device_count()}\")\n    print(f\"üîß Current CUDA device: {torch.cuda.current_device()}\")\n    print(f\"üîß GPU name: {torch.cuda.get_device_name()}\")\n    \nprint(f\"üîß HuggingFace Hub version available for flat file download\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"OpenGVLab/InternVL3-2B\"\n",
    "DOWNLOAD_PATH = \"/home/jovyan/nfs_share/models/InternVL3-2B\"\n",
    "\n",
    "print(f\"üì• Model: {MODEL_NAME}\")\n",
    "print(f\"üìÅ Download path: {DOWNLOAD_PATH}\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "Path(DOWNLOAD_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úÖ Directory created/verified: {Path(DOWNLOAD_PATH).parent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available disk space\n",
    "import shutil\n",
    "\n",
    "def get_disk_usage(path):\n",
    "    \"\"\"Get disk usage statistics for the given path.\"\"\"\n",
    "    total, used, free = shutil.disk_usage(path)\n",
    "    return {\n",
    "        'total_gb': total // (1024**3),\n",
    "        'used_gb': used // (1024**3), \n",
    "        'free_gb': free // (1024**3)\n",
    "    }\n",
    "\n",
    "usage = get_disk_usage('/home/jovyan/nfs_share')\n",
    "print(f\"üíæ Disk space on /home/jovyan/nfs_share:\")\n",
    "print(f\"   Total: {usage['total_gb']} GB\")\n",
    "print(f\"   Used:  {usage['used_gb']} GB\")\n",
    "print(f\"   Free:  {usage['free_gb']} GB\")\n",
    "\n",
    "# InternVL3-2B is approximately 4-6 GB\n",
    "required_gb = 8  # Safety margin\n",
    "if usage['free_gb'] < required_gb:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Available space ({usage['free_gb']} GB) may be insufficient\")\n",
    "    print(f\"   Recommended: At least {required_gb} GB free space\")\n",
    "else:\n",
    "    print(f\"‚úÖ Sufficient disk space available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model already exists\n",
    "if Path(DOWNLOAD_PATH).exists() and any(Path(DOWNLOAD_PATH).iterdir()):\n",
    "    print(f\"üìÅ Model directory already exists: {DOWNLOAD_PATH}\")\n",
    "    print(f\"üìã Contents:\")\n",
    "    for item in Path(DOWNLOAD_PATH).iterdir():\n",
    "        if item.is_file():\n",
    "            size_mb = item.stat().st_size / (1024**2)\n",
    "            print(f\"   üìÑ {item.name} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"   üìÅ {item.name}/\")\n",
    "    \n",
    "    response = input(\"\\nü§î Model appears to already exist. Download anyway? (y/N): \")\n",
    "    if response.lower() not in ['y', 'yes']:\n",
    "        print(\"‚èπÔ∏è  Download cancelled\")\n",
    "        SKIP_DOWNLOAD = True\n",
    "    else:\n",
    "        SKIP_DOWNLOAD = False\n",
    "        print(\"üîÑ Proceeding with download (will overwrite existing files)\")\n",
    "else:\n",
    "    SKIP_DOWNLOAD = False\n",
    "    print(f\"üìÅ Model directory is empty or doesn't exist\")\n",
    "    print(f\"üöÄ Ready to download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download the model (FLAT FILE download)\nif not SKIP_DOWNLOAD:\n    print(f\"üì• Starting FLAT FILE download of {MODEL_NAME}...\")\n    print(f\"üìç Destination: {DOWNLOAD_PATH}\")\n    print(f\"‚è±Ô∏è  This may take 10-30 minutes depending on your connection...\")\n    print(f\"üîß Using flat file download (no HuggingFace cache or storage mapping)\")\n    \n    try:\n        if USE_INDIVIDUAL_DOWNLOAD:\n            # Method 1: Individual file download (guaranteed flat structure)\n            print(f\"üîç Listing repository files...\")\n            repo_files = list_repo_files(MODEL_NAME)\n            model_files = [f for f in repo_files if not f.startswith('.git')]\n            \n            print(f\"üìã Found {len(model_files)} files to download\")\n            \n            Path(DOWNLOAD_PATH).mkdir(parents=True, exist_ok=True)\n            \n            for i, filename in enumerate(model_files, 1):\n                print(f\"üì• Downloading {i}/{len(model_files)}: {filename}\")\n                \n                file_path = hf_hub_download(\n                    repo_id=MODEL_NAME,\n                    filename=filename,\n                    cache_dir=None,  # No cache\n                    local_dir=DOWNLOAD_PATH,\n                    local_dir_use_symlinks=False,  # Flat files only\n                    resume_download=True\n                )\n                \n            downloaded_path = DOWNLOAD_PATH\n            print(f\"‚úÖ Individual file download completed!\")\n            \n        else:\n            # Method 2: Snapshot download with flat file settings\n            downloaded_path = snapshot_download(\n                repo_id=MODEL_NAME,\n                cache_dir=None,  # Don't use HF cache directory  \n                local_dir=DOWNLOAD_PATH,\n                local_dir_use_symlinks=False,  # Force actual file copies (no symlinks)\n                resume_download=True,  # Resume if interrupted\n                # Download all files as flat files\n                ignore_patterns=[\"*.git*\"],  # Only ignore git files\n                # Force flat file download without storage mapping\n                force_download=False,  # Don't re-download existing files\n                proxies=None,\n                etag_timeout=10,\n                token=None\n            )\n            print(f\"‚úÖ Snapshot download completed!\")\n        \n        print(f\"üìÅ Model files saved directly to: {downloaded_path}\")\n        print(f\"üîß All files are actual copies (no symlinks or cache mapping)\")\n        \n        # Verify flat structure\n        files = list(Path(DOWNLOAD_PATH).glob(\"*\"))\n        print(f\"üìã Downloaded {len(files)} files directly to target directory\")\n        \n        # Show actual file paths to confirm flat structure\n        print(f\"üîç Sample files in target directory:\")\n        for file_path in sorted(files)[:5]:\n            if file_path.is_file():\n                size_mb = file_path.stat().st_size / (1024**2)\n                print(f\"   üìÑ {file_path.name} ({size_mb:.1f} MB)\")\n        \n    except Exception as e:\n        print(f\"‚ùå Download failed: {e}\")\n        print(f\"üí° Possible solutions:\")\n        print(f\"   - Check internet connection\")\n        print(f\"   - Verify disk space\")\n        print(f\"   - Clear HuggingFace cache: rm -rf ~/.cache/huggingface\")\n        print(f\"   - Set USE_INDIVIDUAL_DOWNLOAD=True for alternative method\")\n        print(f\"   - Try running again (download will resume)\")\n        raise\nelse:\n    print(f\"‚è© Skipping download\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the download\n",
    "print(f\"üîç Verifying download...\")\n",
    "\n",
    "if Path(DOWNLOAD_PATH).exists():\n",
    "    # List all files in the model directory\n",
    "    files = list(Path(DOWNLOAD_PATH).rglob('*'))\n",
    "    total_size = 0\n",
    "    \n",
    "    print(f\"üìã Model directory contents ({len(files)} files):\")\n",
    "    \n",
    "    key_files = []\n",
    "    for file_path in sorted(files):\n",
    "        if file_path.is_file():\n",
    "            size_mb = file_path.stat().st_size / (1024**2)\n",
    "            total_size += size_mb\n",
    "            rel_path = file_path.relative_to(DOWNLOAD_PATH)\n",
    "            \n",
    "            # Highlight important files\n",
    "            if any(important in file_path.name.lower() for important in ['config', 'model', 'tokenizer', '.bin', '.safetensors']):\n",
    "                key_files.append(f\"   üìÑ {rel_path} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Show key files\n",
    "    print(f\"\\nüîë Key model files:\")\n",
    "    for key_file in key_files[:10]:  # Show first 10 key files\n",
    "        print(key_file)\n",
    "    \n",
    "    if len(key_files) > 10:\n",
    "        print(f\"   ... and {len(key_files) - 10} more files\")\n",
    "    \n",
    "    print(f\"\\nüìä Total model size: {total_size:.1f} MB ({total_size/1024:.2f} GB)\")\n",
    "    \n",
    "    # Check for essential files\n",
    "    essential_files = ['config.json', 'tokenizer_config.json']\n",
    "    missing_files = []\n",
    "    \n",
    "    for essential in essential_files:\n",
    "        if not any(essential in f.name for f in files):\n",
    "            missing_files.append(essential)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Some essential files may be missing: {missing_files}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ All essential files appear to be present\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Model directory not found: {DOWNLOAD_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the model (optional)\n",
    "TEST_LOAD = True  # Set to False to skip model loading test\n",
    "\n",
    "if TEST_LOAD and Path(DOWNLOAD_PATH).exists():\n",
    "    print(f\"üß™ Testing model loading...\")\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        \n",
    "        print(f\"üì• Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            DOWNLOAD_PATH, \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(f\"‚úÖ Tokenizer loaded successfully\")\n",
    "        \n",
    "        print(f\"üì• Loading model (this may take a few minutes)...\")\n",
    "        model = AutoModel.from_pretrained(\n",
    "            DOWNLOAD_PATH,\n",
    "            torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(f\"‚úÖ Model loaded successfully\")\n",
    "        \n",
    "        # Print model info\n",
    "        if hasattr(model, 'config'):\n",
    "            print(f\"üìã Model configuration:\")\n",
    "            if hasattr(model.config, 'hidden_size'):\n",
    "                print(f\"   Hidden size: {model.config.hidden_size}\")\n",
    "            if hasattr(model.config, 'num_attention_heads'):\n",
    "                print(f\"   Attention heads: {model.config.num_attention_heads}\")\n",
    "            if hasattr(model.config, 'num_hidden_layers'):\n",
    "                print(f\"   Hidden layers: {model.config.num_hidden_layers}\")\n",
    "        \n",
    "        # Get model size\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"üìä Total parameters: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        del model\n",
    "        del tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"‚úÖ Model test completed successfully\")\n",
    "        print(f\"üßπ Memory cleaned up\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model loading test failed: {e}\")\n",
    "        print(f\"üí° This doesn't necessarily mean the download failed\")\n",
    "        print(f\"   The model files may still be valid for use in your application\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚è© Skipping model loading test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"üìã DOWNLOAD SUMMARY\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üìÅ Location: {DOWNLOAD_PATH}\")\n",
    "\n",
    "if Path(DOWNLOAD_PATH).exists():\n",
    "    files = list(Path(DOWNLOAD_PATH).rglob('*'))\n",
    "    total_size = sum(f.stat().st_size for f in files if f.is_file()) / (1024**3)\n",
    "    print(f\"üìä Size: {total_size:.2f} GB\")\n",
    "    print(f\"üìÑ Files: {len([f for f in files if f.is_file()])}\")\n",
    "    print(f\"‚úÖ Status: Download completed successfully\")\n",
    "else:\n",
    "    print(f\"‚ùå Status: Model not found\")\n",
    "\n",
    "print(f\"\\nüí° Next steps:\")\n",
    "print(f\"   1. Use the model in your vision processing applications\")\n",
    "print(f\"   2. Model path: {DOWNLOAD_PATH}\")\n",
    "print(f\"   3. Remember to use trust_remote_code=True when loading\")\n",
    "print(f\"\\nüéâ InternVL3-2B model is ready for use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
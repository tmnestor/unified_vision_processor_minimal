{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f893fb5",
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\n\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, MllamaForConditionalGeneration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a0f70",
   "metadata": {},
   "outputs": [],
   "source": "model_id = \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\"\n# here, specify the name of the image\nimageName = \"/home/jovyan/nfs_share/tod/datasets/synthetic_invoice_014.png\"\n\nmodel = MllamaForConditionalGeneration.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n# open the image\nimage = Image.open(imageName)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430be3f9",
   "metadata": {},
   "outputs": [],
   "source": "# create a message data structure\n# messageDataStructure = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\"type\": \"image\"},\n#             {\n#                 \"type\": \"text\",\n#                 \"text\": \"What type of document is this?\",\n#             },\n#         ],\n#     }\n# ]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebbc77-bc11-44c6-bd5e-35d756a2f7bb",
   "metadata": {},
   "outputs": [],
   "source": "messageDataStructure = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\n                \"type\": \"text\",\n                \"text\": \"How much did Jessica pay?\",\n            },\n        ],\n    }\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b814e",
   "metadata": {},
   "outputs": [],
   "source": "# create text input\ntextInput = processor.apply_chat_template(\n    messageDataStructure, add_generation_prompt=True\n)\n# call the processor\ninputs = processor(image, textInput, return_tensors=\"pt\").to(model.device)\n\n# here, change the number of tokens to get a more detailed answer\noutput = model.generate(**inputs, max_new_tokens=2000)\n# here, we decode and store the response so we can print it\ngeneratedOutput = processor.decode(output[0])\n\nprint(generatedOutput)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259d5cc",
   "metadata": {},
   "outputs": [],
   "source": "# save the answer in a file\nwith Path(\"/home/jovyan/nfs_share/tod/output/llama_output.txt\").open(\n    \"w\", encoding=\"utf-8\"\n) as text_file:\n    text_file.write(generatedOutput)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
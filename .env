# Vision Processor Configuration
# Single source of truth for all configuration

# Model Configuration
VISION_MODEL_TYPE=internvl3                    # internvl3 | llama32_vision
VISION_MODEL_PATH=/home/jovyan/nfs_share/models/InternVL3-8B

# GPU and Memory Settings
VISION_DEVICE_CONFIG=auto                      # auto | cuda:0 | cuda:1 | cpu
VISION_ENABLE_MULTI_GPU=false                 # Enable multi-GPU processing
VISION_GPU_MEMORY_FRACTION=0.9                # GPU memory fraction (0.1-1.0)
VISION_MEMORY_LIMIT_MB=15360                  # Memory limit in MB (for V100: 15360)
VISION_ENABLE_QUANTIZATION=true               # Enable 8-bit quantization

# Processing Configuration
VISION_ENABLE_GRADIENT_CHECKPOINTING=true     # Memory optimization
VISION_USE_FLASH_ATTENTION=true               # Flash attention optimization
VISION_TRUST_REMOTE_CODE=true                 # Required for some models
VISION_OFFLINE_MODE=true                      # Use local models only

# Output Configuration
VISION_OUTPUT_FORMAT=yaml                     # table | json | yaml
VISION_LOG_LEVEL=INFO                         # DEBUG | INFO | WARNING | ERROR

# Force transformers to work offline
TRANSFORMERS_OFFLINE=1
HF_DATASETS_OFFLINE=1
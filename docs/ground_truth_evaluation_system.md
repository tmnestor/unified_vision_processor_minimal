# Ground Truth Evaluation System

## Overview

The ground truth evaluation system provides comprehensive accuracy assessment for vision language models (InternVL3 and Llama Vision) performing structured key-value extraction from business documents. This document explains the sophisticated evaluation pipeline that enables fair comparison between different models.

## Table of Contents

1. [System Architecture](#system-architecture)
2. [Ground Truth Data Structure](#ground-truth-data-structure)
3. [Field Accuracy Calculation](#field-accuracy-calculation)
4. [Evaluation Pipeline](#evaluation-pipeline)
5. [Output Formats](#output-formats)
6. [Comparison Methodology](#comparison-methodology)
7. [Implementation Details](#implementation-details)
8. [Usage Examples](#usage-examples)

---

## System Architecture

### Core Components

```mermaid
graph TD
    A[Model Extraction] --> B[Ground Truth Loading]
    B --> C[Field Accuracy Calculation]
    C --> D[Document-Level Metrics]
    D --> E[Batch Statistics]
    E --> F[Report Generation]
    
    G[evaluation_ground_truth.csv] --> B
    H[Extracted Results] --> A
    
    F --> I[Executive Summary]
    F --> J[Deployment Checklist]
    F --> K[Detailed CSV]
```

### Key Files

| File | Purpose |
|------|---------|
| `evaluation_ground_truth.csv` | Reference truth data for 20 synthetic invoices |
| `calculate_field_accuracy()` | Sophisticated field comparison logic |
| `evaluate_extraction_results()` | Main evaluation orchestrator |
| `generate_comprehensive_reports()` | Executive summary and deployment readiness |

---

## Ground Truth Data Structure

### Dataset Composition

- **20 synthetic business documents** (invoices, receipts, bank statements)
- **25 structured fields** per document in alphabetical order
- **Standardized N/A handling** for missing/non-applicable fields

### Field Categories

| Category | Fields | Description |
|----------|---------|-------------|
| **Document Metadata** | `DOCUMENT_TYPE`, `INVOICE_DATE`, `DUE_DATE` | Basic document identification |
| **Business Information** | `SUPPLIER`, `ABN`, `BUSINESS_ADDRESS`, `BUSINESS_PHONE`, `SUPPLIER_WEBSITE` | Company details |
| **Financial Data** | `GST`, `TOTAL`, `SUBTOTAL`, `PRICES`, `QUANTITIES` | Monetary values and calculations |
| **Customer Information** | `PAYER_NAME`, `PAYER_ADDRESS`, `PAYER_EMAIL`, `PAYER_PHONE` | Customer/recipient details |
| **Banking Information** | `BANK_NAME`, `BSB_NUMBER`, `BANK_ACCOUNT_NUMBER`, `ACCOUNT_HOLDER` | Bank statement specific |
| **Transaction Data** | `OPENING_BALANCE`, `CLOSING_BALANCE`, `STATEMENT_PERIOD`, `DESCRIPTIONS` | Transaction history |

### Ground Truth CSV Structure

```csv
image_file,ABN,ACCOUNT_HOLDER,BANK_ACCOUNT_NUMBER,BANK_NAME,BSB_NUMBER,...
synthetic_invoice_001.png,04 904 754 234,N/A,N/A,N/A,N/A,...
synthetic_invoice_002.png,31 724 023 407,N/A,N/A,N/A,N/A,...
```

**Key Properties:**
- **26 columns total**: `image_file` + 25 extraction fields
- **Alphabetical field ordering** for consistency
- **Standardized N/A values** for missing fields
- **Complex field formats** using pipe separation (e.g., `$3.80 | $8.50 | $4.20`)

---

## Field Accuracy Calculation

### The `calculate_field_accuracy()` Function

This is the **core evaluation logic** that handles sophisticated field comparison with model-specific nuances.

### Enhanced N/A Handling (CRITICAL FIX)

```python
# Comprehensive N/A variants (FIXED to include 'nan')
na_variants = ['N/A', 'NA', '', 'NAN', 'NULL', 'NONE', 'NIL']

# Reward correct N/A identification
if not ground_truth_value or str(ground_truth_value).upper() in na_variants:
    return 1.0 if (not extracted_value or str(extracted_value).upper() in na_variants) else 0.0
```

**Why This Matters:**
- **Models correctly identify missing fields** as "nan" or "N/A"
- **Previous bug**: "nan" wasn't recognized, causing underreporting of accuracy
- **Fixed**: All N/A variants properly rewarded as perfect matches

### Field-Specific Comparison Logic

#### 1. Financial Fields (`GST`, `TOTAL`, `SUBTOTAL`, `OPENING_BALANCE`, `CLOSING_BALANCE`)

```python
# Numeric comparison with tolerance
tolerance = 0.01
ext_num = float(re.sub(r'[^\d.-]', '', extracted_clean.replace(',', '')))
gt_num = float(re.sub(r'[^\d.-]', '', gt_clean.replace(',', '')))
return 1.0 if abs(ext_num - gt_num) < tolerance else 0.0
```

**Features:**
- **Currency symbol removal**: `$53.29` â†’ `53.29`
- **Comma handling**: `$1,234.56` â†’ `1234.56`
- **Floating-point tolerance**: 0.01 precision for rounding differences

#### 2. List Fields (`QUANTITIES`, `PRICES`, `DESCRIPTIONS`)

```python
# Pipe-separated list comparison
ext_items = [item.strip() for item in extracted_clean.split('|')]
gt_items = [item.strip() for item in gt_clean.split('|')]

# Must match both count and content
if len(ext_items) != len(gt_items):
    return 0.0

matches = sum(1 for e, g in zip(ext_items, gt_items, strict=False) 
             if e.lower().strip() == g.lower().strip())
return matches / len(gt_items) if gt_items else 0.0
```

**Example:**
- **Ground Truth**: `$3.80 | $8.50 | $4.20 | $3.79`
- **Extracted**: `$3.80, $8.50, $4.20, $3.79`
- **Result**: 0.0 (format mismatch - commas vs pipes)

#### 3. Date Fields (`INVOICE_DATE`, `DUE_DATE`)

```python
# Flexible date format handling
ext_date = re.sub(r'[^\d/\-]', '', extracted_clean)
gt_date = re.sub(r'[^\d/\-]', '', gt_clean)
return 1.0 if ext_date == gt_date else 0.0
```

**Features:**
- **Format normalization**: `05/09/2025` matches `05-09-2025`
- **Text removal**: `Date: 05/09/2025` â†’ `05/09/2025`

#### 4. String Fields (All Others)

```python
# Fuzzy string matching with partial credit
if extracted_clean.lower() == gt_clean.lower():
    return 1.0
elif (extracted_clean.lower() in gt_clean.lower() or 
      gt_clean.lower() in extracted_clean.lower()):
    return 0.8  # Partial match
else:
    return 0.0
```

**Scoring:**
- **1.0**: Exact match (case-insensitive)
- **0.8**: Partial match (substring relationship)
- **0.0**: No match

---

## Evaluation Pipeline

### 1. Input Processing

```python
def evaluate_extraction_results(extraction_results, ground_truth_map):
    """
    Main evaluation orchestrator
    
    Args:
        extraction_results (list): Model extraction outputs
        ground_truth_map (dict): Loaded ground truth data
    
    Returns:
        dict: Comprehensive evaluation metrics
    """
```

### 2. Document-Level Analysis

For each document, the system calculates:

```python
# Per-document metrics
image_evaluation = {
    'image_name': image_name,
    'overall_accuracy': image_accuracy,           # Average of all 25 field accuracies
    'correct_fields': fields_correct,            # Count with â‰¥99% accuracy
    'total_fields': 25,                          # Always 25
    'field_accuracy_rate': (fields_correct/25)*100  # Percentage perfect fields
}

# Field details (InternVL format compatibility)
for field in EXTRACTION_FIELDS:
    image_evaluation[f'{field}_accuracy'] = accuracy
    image_evaluation[f'{field}_extracted'] = extracted_value  
    image_evaluation[f'{field}_ground_truth'] = gt_value
```

### 3. Batch-Level Statistics

```python
evaluation_summary = {
    'total_images': len(evaluation_data),
    'overall_accuracy': mean(overall_accuracies),
    'perfect_documents': count(accuracy >= 0.99),
    'good_documents': count(0.8 <= accuracy < 0.99),
    'fair_documents': count(0.6 <= accuracy < 0.8),
    'poor_documents': count(accuracy < 0.6),
    'field_accuracies': {field: mean(accuracies) for field, accuracies in field_data},
    'best_performing_image': argmax(accuracies),
    'worst_performing_image': argmin(accuracies)
}
```

---

## Output Formats

### 1. Ground Truth Evaluation CSV

**Structure:** 80 columns total

```csv
image_name,overall_accuracy,correct_fields,total_fields,field_accuracy_rate,
ABN_accuracy,ABN_extracted,ABN_ground_truth,
ACCOUNT_HOLDER_accuracy,ACCOUNT_HOLDER_extracted,ACCOUNT_HOLDER_ground_truth,
[...continues for all 25 fields...]
```

**Column Pattern:**
- **Columns 1-5**: Summary metrics
- **Columns 6-80**: Field triplets (`{field}_accuracy`, `{field}_extracted`, `{field}_ground_truth`)

**Sample Row:**
```csv
synthetic_invoice_001.png,0.752,15,25,60.0,
1.0,04 904 754 234,04 904 754 234,
1.0,N/A,N/A,
0.0,1288570416,N/A,
...
```

### 2. Executive Summary Report

```markdown
# Llama Vision Key-Value Extraction - Executive Summary

## Model Performance Overview
**Model:** Llama-3.2-11B-Vision-Instruct  
**Documents Processed:** 20  
**Average Accuracy:** 75.2%

## Key Findings
1. **Document Analysis:** Processed 20 business documents  
2. **Field Extraction:** Successfully extracts 15 out of 25 fields with â‰¥90% accuracy
3. **Best Performance:** synthetic_invoice_004.png (96.5% accuracy)
4. **Challenging Cases:** synthetic_invoice_013.png (42.3% accuracy)

## Production Readiness Assessment
âœ… **READY FOR PRODUCTION:** Model shows good performance with minor limitations

## Document Quality Distribution
- Perfect Documents (â‰¥99%): 3 (15.0%)
- Good Documents (80-98%): 12 (60.0%)  
- Fair Documents (60-79%): 4 (20.0%)
- Poor Documents (<60%): 1 (5.0%)
```

### 3. Deployment Checklist

```markdown
# Llama Vision Deployment Readiness Checklist

## Performance Metrics
- [x] Overall accuracy â‰¥80% (75.2%)
- [x] At least 15 fields with â‰¥90% accuracy (15/25)
- [x] At least 30% perfect documents (3/20)

## Deployment Strategy
âœ… **APPROVED FOR PRODUCTION DEPLOYMENT**

### Next Steps
1. âœ… Deploy to production environment
2. ðŸ“Š Implement real-time accuracy monitoring
3. ðŸ”„ Establish continuous evaluation pipeline
```

### 4. JSON Summary (Programmatic Access)

```json
{
  "overall_accuracy": 0.752,
  "total_images": 20,
  "perfect_documents": 3,
  "field_accuracies": {
    "ABN": 0.95,
    "DOCUMENT_TYPE": 0.90,
    "TOTAL": 0.88,
    ...
  },
  "best_performing_image": "synthetic_invoice_004.png",
  "evaluation_timestamp": "20250806_231835"
}
```

---

## Comparison Methodology

### Model Compatibility

Both `llama_keyvalue.py` and `internvl3_keyvalue.py` generate **identical output structures**:

| Output File | InternVL3 | Llama | Compatible |
|-------------|-----------|-------|------------|
| Batch Extraction CSV | âœ… | âœ… | **Perfect** |
| Ground Truth Evaluation CSV | âœ… | âœ… | **Perfect** |
| Extraction Metadata CSV | âœ… | âœ… | **Perfect** |
| Executive Summary MD | âœ… | âœ… | **Perfect** |
| Deployment Checklist MD | âœ… | âœ… | **Perfect** |
| JSON Summary | âœ… | âœ… | **Perfect** |

### Direct Comparison Workflow

```bash
# Run both models
python internvl3_keyvalue.py
python llama_keyvalue.py

# Compare results directly
diff output/internvl3_batch_extraction_*.csv output/llama_batch_extraction_*.csv
pandas.merge(internvl_df, llama_df, on='image_name', how='outer')
```

### Fair Evaluation Guarantees

1. **Identical Ground Truth**: Same `evaluation_ground_truth.csv` for both models
2. **Consistent Field Ordering**: Alphabetical extraction field sequence  
3. **Same Accuracy Logic**: Identical `calculate_field_accuracy()` implementation
4. **Compatible Output**: Direct CSV comparison and merging possible
5. **Fixed N/A Handling**: Both models benefit from enhanced missing field detection

---

## Implementation Details

### Critical Design Decisions

#### 1. N/A Variant Expansion (Major Bug Fix)

**Problem:** Original implementation only recognized `['N/A', 'NA', '']`
**Solution:** Expanded to `['N/A', 'NA', '', 'NAN', 'NULL', 'NONE', 'NIL']`
**Impact:** Dramatically improved accuracy reporting for missing field identification

#### 2. InternVL3 CSV Structure Compatibility

**Problem:** Llama initially generated different column ordering
**Solution:** Exact replication of InternVL3's CSV structure
**Result:** Perfect structural compatibility for comparison tools

#### 3. Field-Specific Accuracy Logic

**Financial Fields:**
- Currency symbol normalization
- Floating-point precision tolerance
- Comma handling in large numbers

**List Fields:**
- Pipe-separated format parsing
- Item count validation
- Element-wise comparison

**Date Fields:**
- Format-agnostic comparison
- Text artifact removal
- Flexible separator handling

#### 4. Production Readiness Thresholds

| Metric | Threshold | Rationale |
|--------|-----------|-----------|
| Overall Accuracy | â‰¥80% | Industry standard for automated processing |
| Perfect Documents | â‰¥30% | Reliability indicator for high-value documents |
| Field Coverage | â‰¥15/25 | Minimum viable field extraction for most use cases |

### Error Handling

```python
try:
    evaluation_summary = evaluate_extraction_results(results, ground_truth_data)
except Exception as e:
    print(f"âŒ Evaluation error: {e}")
    traceback.print_exc()
    return {"error": str(e)}
```

### Memory Optimization

- **Streaming evaluation**: Process documents one at a time
- **Efficient data structures**: Use dictionaries for O(1) ground truth lookups
- **Memory cleanup**: Clear large variables after processing

---

## Usage Examples

### 1. Basic Evaluation Run

```python
# Load and evaluate
results, batch_stats = process_image_batch(image_files)
ground_truth_data = load_ground_truth(ground_truth_path)
evaluation_summary = evaluate_extraction_results(results, ground_truth_data)

print(f"Overall Accuracy: {evaluation_summary['overall_accuracy']:.1%}")
print(f"Perfect Documents: {evaluation_summary['perfect_documents']}")
```

### 2. Field-Specific Analysis

```python
# Analyze challenging fields
field_accuracies = evaluation_summary['field_accuracies']
poor_fields = {k: v for k, v in field_accuracies.items() if v < 0.5}

print("Fields requiring attention:")
for field, accuracy in sorted(poor_fields.items(), key=lambda x: x[1]):
    print(f"  {field}: {accuracy:.1%}")
```

### 3. Document Performance Analysis

```python
# Find problematic documents
eval_data = evaluation_summary['evaluation_data']
poor_docs = [doc for doc in eval_data if doc['overall_accuracy'] < 0.6]

for doc in poor_docs:
    print(f"{doc['image_name']}: {doc['overall_accuracy']:.1%}")
    print(f"  Correct fields: {doc['correct_fields']}/25")
```

### 4. Model Comparison

```python
import pandas as pd

# Load both model results
internvl_df = pd.read_csv('output/internvl3_ground_truth_evaluation_*.csv')
llama_df = pd.read_csv('output/llama_ground_truth_evaluation_*.csv')

# Compare overall accuracy
comparison = pd.DataFrame({
    'InternVL3': internvl_df['overall_accuracy'],
    'Llama': llama_df['overall_accuracy']
})

print("Model Performance Comparison:")
print(comparison.describe())
```

---

## Conclusion

The ground truth evaluation system provides a **comprehensive, fair, and sophisticated framework** for assessing vision language model performance on structured document extraction tasks. Key strengths include:

âœ… **Rigorous Field Comparison**: Handles complex field types with appropriate comparison logic  
âœ… **Missing Field Intelligence**: Properly rewards correct N/A identification  
âœ… **Production-Ready Metrics**: Actionable insights for deployment decisions  
âœ… **Model Agnostic**: Works identically across different vision language models  
âœ… **Comprehensive Reporting**: Multiple output formats for different stakeholders  
âœ… **Direct Comparability**: Perfect structural compatibility enables head-to-head analysis  

This evaluation framework enables data-driven decisions about model selection, optimization priorities, and production readiness for business document processing workflows.
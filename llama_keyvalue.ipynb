{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nCell 1: Environment Setup and Model Loading for Llama Vision Key-Value Extraction\n\nPurpose:\n- Import all required libraries for Llama-3.2-11B-Vision-Instruct model\n- Load the Llama Vision model optimized for structured key-value extraction\n- Initialize model with proper configuration for document analysis\n- Define global configuration variables for data paths\n\nKey Components:\n- torch.bfloat16: Memory-efficient 16-bit floating point for better performance\n- device_map=\"auto\": Automatic device mapping for optimal GPU utilization\n- AutoProcessor: Handles both text and image processing for Llama Vision\n- Optimized for multimodal document understanding\n\nGlobal Configuration:\n- data_dir: Centralized data directory path for all image operations\n- model_path: Local path to Llama-3.2-11B-Vision-Instruct model files\n- output_dir: Directory for saving extraction results\n\nSpecialized for Key-Value Extraction:\n- Optimized for structured document processing\n- Configured for business document analysis workflows\n- Supports multimodal conversation format required by Llama Vision\n\"\"\"\n\nfrom pathlib import Path\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, MllamaForConditionalGeneration\n\n# Global configuration variables\ndata_dir = \"/home/jovyan/nfs_share/tod/huaifeng_data\"\nmodel_path = \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\"\noutput_dir = \"/home/jovyan/nfs_share/tod/output\"\n\nprint(f\"ğŸ—‚ï¸  Data directory: {data_dir}\")\nprint(f\"ğŸ“ Output directory: {output_dir}\")\nprint(f\"ğŸ”§ Loading Llama-3.2-11B-Vision-Instruct model for key-value extraction from: {model_path}\")\n\n# Load Llama Vision model with optimal configuration\nmodel = MllamaForConditionalGeneration.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency\n    device_map=\"auto\",           # Automatic device mapping for multi-GPU support\n)\n\n# Load processor for handling both text and image inputs\nprocessor = AutoProcessor.from_pretrained(model_path)\n\nprint(\"âœ… Llama Vision model and processor loaded successfully for key-value extraction\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nCell 2: Document Loading and Preprocessing for Llama Vision\n\nPurpose:\n- Load and preprocess business documents for key-value extraction\n- Handle various document formats with proper image processing\n- Prepare documents for Llama Vision's multimodal processing pipeline\n\nDocument Processing Features:\n- Supports various image formats (PNG, JPG, PDF conversions)\n- Handles different document orientations and sizes\n- Maintains image quality for optimal text recognition\n- Uses global data_dir for consistent path management\n\nLlama Vision Specific:\n- No complex tiling required (unlike InternVL3)\n- Direct image processing through AutoProcessor\n- Optimized for single-image document analysis\n- Maintains original aspect ratios for better text recognition\n\"\"\"\n\ndef load_document_image(image_path):\n    \"\"\"\n    Load document image with path flexibility\n    \n    Args:\n        image_path: Path to document file (relative to data_dir or absolute)\n    \n    Returns:\n        PIL.Image: Loaded document image ready for processing\n    \"\"\"\n    # Handle both relative and absolute paths\n    if not image_path.startswith('/'):\n        image_path = f\"{data_dir}/{image_path}\"\n    \n    return Image.open(image_path)\n\n# Load and analyze document using global data_dir\ndocument_image = \"synthetic_invoice_014.png\"  # Configurable document filename\nprint(f\"ğŸ“„ Loading document from: {data_dir}/{document_image}\")\n\n# Load document image\nimage = load_document_image(document_image)\nprint(f\"ğŸ“· Document loaded successfully: {image.size}\")\nprint(f\"ğŸ“ Document aspect ratio: {image.size[0]/image.size[1]:.2f}\")\nprint(f\"ğŸ–¼ï¸  Image format: {image.format}\")\nprint(f\"ğŸ¨ Color mode: {image.mode}\")\nprint(\"ğŸ” Document ready for Llama Vision key-value extraction\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nCell 3: Advanced Key-Value Extraction Prompt Configuration\n\nPurpose:\n- Define comprehensive prompt for extracting structured business document data\n- Configure extraction parameters for consistent, standardized output\n- Leverage Llama Vision's advanced reasoning capabilities for document analysis\n\nEnhanced Prompt Features:\n- 25 predefined fields covering comprehensive business document types\n- Advanced formatting constraints to prevent Llama's markdown tendencies\n- Explicit instructions optimized for Llama Vision's capabilities\n- Deterministic field ordering for automated downstream processing\n\nField Categories (Comprehensive Coverage):\n1. Document metadata (type, dates, references)\n2. Supplier/business information (name, address, contact details)\n3. Financial data (amounts, GST, totals, subtotals)\n4. Transaction details (quantities, prices, descriptions)\n5. Banking information (account numbers, BSB, balances)\n\nAdvanced Output Quality Controls:\n- Multiple examples of correct/incorrect formatting\n- Explicit markdown prevention (critical for Llama models)\n- Field count validation (exactly 25 lines)\n- Structured validation for downstream processing systems\n\"\"\"\n\n# Enhanced key-value extraction prompt optimized for Llama Vision\nextraction_prompt = \"\"\"Extract data from this business document. \nOutput ALL fields below with their exact keys. \nDO NOT USE \"*\" or any markdown formatting.\nUse \"N/A\" if field is not visible or not present.\n\nREQUIRED OUTPUT FORMAT (output ALL lines exactly as shown):\nDOCUMENT_TYPE: [value or N/A]\nSUPPLIER: [value or N/A]\nABN: [11-digit Australian Business Number or N/A]\nPAYER_NAME: [value or N/A]\nPAYER_ADDRESS: [value or N/A]\nPAYER_PHONE: [value or N/A]\nPAYER_EMAIL: [value or N/A]\nINVOICE_DATE: [value or N/A]\nDUE_DATE: [value or N/A]\nGST: [GST amount in dollars or N/A]\nTOTAL: [total amount in dollars or N/A]\nSUBTOTAL: [subtotal amount in dollars or N/A]\nSUPPLIER_WEBSITE: [value or N/A]\nQUANTITIES: [list of quantities or N/A]\nPRICES: [individual prices in dollars or N/A]\nBUSINESS_ADDRESS: [value or N/A]\nBUSINESS_PHONE: [value or N/A]\nBANK_NAME: [bank name from bank statements only or N/A]\nBSB_NUMBER: [6-digit BSB from bank statements only or N/A]\nBANK_ACCOUNT_NUMBER: [account number from bank statements only or N/A]\nACCOUNT_HOLDER: [value or N/A]\nSTATEMENT_PERIOD: [value or N/A]\nOPENING_BALANCE: [opening balance amount in dollars or N/A]\nCLOSING_BALANCE: [closing balance amount in dollars or N/A]\nDESCRIPTIONS: [list of transaction descriptions or N/A]\n\nCRITICAL: Output in PLAIN TEXT format only. Do NOT use markdown formatting.\n\nCORRECT format: DOCUMENT_TYPE: TAX INVOICE\nWRONG format: **DOCUMENT_TYPE:** TAX INVOICE\nWRONG format: **DOCUMENT_TYPE: TAX INVOICE**\nWRONG format: DOCUMENT_TYPE: **TAX INVOICE**\n\nUse exactly: KEY: value (with colon and space)\nNever use: **KEY:** or **KEY** or any asterisks\nNever use bold, italic, or any markdown formatting\n\nABSOLUTELY CRITICAL: Output EXACTLY 25 lines using ONLY the keys listed above. \nDo NOT add extra fields like \\\"Balance\\\", \\\"Credit\\\", \\\"Debit\\\", \\\"Date\\\", \\\"Description\\\".\nDo NOT include ANY fields not in the required list above.\nInclude ALL 25 keys listed above even if value is N/A.\nSTOP after exactly 25 lines.\"\"\"\n\nprint(\"ğŸ“‹ Advanced key-value extraction prompt configured for Llama Vision\")\nprint(f\"ğŸ“„ Prompt length: {len(extraction_prompt)} characters\")\nprint(f\"ğŸ” Extracting 25 standardized business document fields\")\nprint(\"âš™ï¸ Configured for deterministic, structured output with markdown prevention\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nCell 4: Llama Vision Key-Value Extraction Execution\n\nPurpose:\n- Execute structured field extraction using Llama Vision's multimodal capabilities\n- Process document with optimized generation parameters for structured output\n- Handle extraction with comprehensive error reporting and validation\n\nLlama Vision Processing Pipeline:\n1. Create multimodal message structure (image + text prompt)\n2. Apply chat template for proper formatting\n3. Process inputs through AutoProcessor\n4. Generate structured output with controlled parameters\n5. Validate and analyze extraction results\n\nGeneration Configuration:\n- max_new_tokens=1000: Sufficient for 25 structured fields\n- Controlled generation to prevent excessive output\n- Proper device handling for GPU processing\n\nAdvanced Error Handling:\n- Comprehensive exception catching with detailed diagnostics\n- Llama-specific error identification and troubleshooting\n- Memory management guidance for large vision models\n- Output validation and quality assessment\n\nEnhanced Output Processing:\n- Intelligent parsing to extract only structured response\n- Removes conversation history and prompt artifacts\n- Cleans markdown formatting for plain text output\n- Validates field structure and completeness\n\"\"\"\n\n# Create multimodal message structure for Llama Vision\nmessageDataStructure = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\n                \"type\": \"text\",\n                \"text\": extraction_prompt,\n            },\n        ],\n    }\n]\n\nprint(\"ğŸ¤– Executing key-value extraction with Llama Vision...\")\nprint(\"âš™ï¸ Using multimodal conversation format for optimal extraction\")\n\ntry:\n    # Apply chat template for proper Llama formatting\n    textInput = processor.apply_chat_template(\n        messageDataStructure, add_generation_prompt=True\n    )\n    \n    # Process inputs through AutoProcessor (handles both image and text)\n    inputs = processor(image, textInput, return_tensors=\"pt\").to(model.device)\n    \n    print(f\"ğŸ”§ Input tensors prepared on device: {model.device}\")\n    \n    # Generate structured output with controlled parameters\n    output = model.generate(\n        **inputs, \n        max_new_tokens=1000,  # Adequate for 25 structured fields\n        do_sample=False,      # Deterministic for consistent extraction\n        pad_token_id=processor.tokenizer.eos_token_id  # Prevent warnings\n    )\n    \n    # Decode the response and extract only the assistant's response\n    generatedOutput = processor.decode(output[0], skip_special_tokens=True)\n    \n    # Extract only the assistant's response (after the last \"assistant\" token)\n    if \"assistant\" in generatedOutput:\n        # Split by assistant and take the last part (the actual response)\n        assistant_parts = generatedOutput.split(\"assistant\")\n        if len(assistant_parts) > 1:\n            generatedOutput = assistant_parts[-1].strip()\n    \n    # Additional cleaning: remove any remaining conversation artifacts\n    lines = generatedOutput.split('\\n')\n    cleaned_lines = []\n    in_response = False\n    \n    for line in lines:\n        line = line.strip()\n        # Look for the start of structured output (first field)\n        if line.startswith('DOCUMENT_TYPE:'):\n            in_response = True\n        # Skip user input or other artifacts\n        if line.startswith('user') or line.startswith('<image>') or line.startswith('Extract data'):\n            in_response = False\n            continue\n        # Collect response lines with proper field format\n        if in_response and ':' in line and not line.startswith('<'):\n            # Remove any markdown artifacts\n            clean_line = line.replace('**', '').replace('*', '')\n            cleaned_lines.append(clean_line)\n            # Stop after 25 fields as specified in prompt\n            if len(cleaned_lines) >= 25:\n                break\n    \n    # Use cleaned output if we found structured fields, otherwise use original\n    if cleaned_lines:\n        generatedOutput = '\\n'.join(cleaned_lines)\n    \n    print(\"âœ… Key-value extraction completed successfully!\")\n    print(\"\\n\" + \"=\"*60)\n    print(\"EXTRACTED BUSINESS DOCUMENT FIELDS (LLAMA VISION):\")\n    print(\"=\"*60)\n    print(generatedOutput)\n    print(\"=\"*60)\n    \n    # Advanced extraction validation and analysis\n    lines = generatedOutput.split('\\n')\n    field_lines = [line for line in lines if ':' in line and not line.strip().startswith('<')]\n    \n    print(f\"\\nğŸ“Š Llama Vision Extraction Statistics:\")\n    print(f\"   â€¢ Total response lines: {len(lines)}\")\n    print(f\"   â€¢ Structured field lines: {len(field_lines)}\")\n    print(f\"   â€¢ Expected field count: 25\")\n    print(f\"   â€¢ Extraction completeness: {len(field_lines)/25*100:.1f}%\")\n    \n    # Quality assessment\n    if len(field_lines) == 25:\n        print(\"âœ… Perfect field extraction - all 25 fields captured\")\n    elif len(field_lines) > 20:\n        print(\"âœ… Near-complete extraction - minor fields may be missing\")\n    elif len(field_lines) > 0:\n        print(\"âš ï¸ Partial extraction - significant fields may be missing\")\n    else:\n        print(\"âŒ No structured fields detected in response\")\n    \n    # Check for markdown artifacts (common with Llama models)\n    markdown_count = generatedOutput.count('**') + generatedOutput.count('*')\n    if markdown_count > 0:\n        print(f\"âš ï¸ Markdown artifacts detected: {markdown_count} instances\")\n        print(\"ğŸ’¡ Consider refining prompt to further suppress markdown formatting\")\n    else:\n        print(\"âœ… Clean plain text output - no markdown artifacts detected\")\n        \nexcept torch.cuda.OutOfMemoryError:\n    print(\"âŒ GPU Memory Error: Insufficient VRAM for Llama Vision processing\")\n    print(\"ğŸ’¡ Solutions:\")\n    print(\"   â€¢ Reduce image resolution\")\n    print(\"   â€¢ Use CPU inference (slower but memory-efficient)\")\n    print(\"   â€¢ Clear GPU cache with torch.cuda.empty_cache()\")\n    \nexcept Exception as e:\n    print(f\"âŒ Error during Llama Vision extraction: {e}\")\n    print(f\"ğŸ” Error type: {type(e).__name__}\")\n    print(\"\\nğŸ“‹ Troubleshooting suggestions:\")\n    print(\"   â€¢ Verify document image quality and readability\")\n    print(\"   â€¢ Check model and processor compatibility\")\n    print(\"   â€¢ Ensure sufficient system resources\")\n    print(\"   â€¢ Validate multimodal input format\")\n    \n    import traceback\n    print(f\"\\nğŸ”§ Full error traceback:\")\n    traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nCell 5: Advanced Results Management and Analysis Pipeline\n\nPurpose:\n- Save extracted key-value pairs with comprehensive analysis and reporting\n- Perform advanced quality validation and extraction confidence assessment\n- Generate detailed reports for workflow integration and process optimization\n\nAdvanced File Operations:\n- Uses global output_dir for consistent file management\n- Descriptive filename with model identification\n- UTF-8 encoding with international character support\n- Atomic file operations to prevent data corruption\n\nComprehensive Quality Analysis:\n- Field completeness assessment (target: 25 fields)\n- Content coverage analysis (non-N/A fields)\n- Markdown artifact detection and reporting\n- Data quality indicators for downstream processing\n- Extraction confidence scoring and validation\n\nEnhanced Error Handling:\n- Specific error types with targeted solutions\n- File system diagnostics and troubleshooting\n- Memory management guidance for large documents\n- Integration workflow validation\n\nAdvanced Integration Features:\n- Structured output validation for database import\n- Quality metrics for automated processing workflows\n- Comparative analysis capabilities for model evaluation\n- Batch processing readiness indicators\n\"\"\"\n\n# Configure output path using global output_dir variable\noutput_filename = \"llama_keyvalue_extraction.txt\"\noutput_path = Path(output_dir) / output_filename\n\nprint(f\"ğŸ’¾ Saving Llama Vision extraction results to: {output_path}\")\n\ntry:\n    # Ensure output directory exists with proper permissions\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    \n    # Write extraction results with comprehensive metadata\n    with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n        # Add extraction metadata header\n        text_file.write(f\"# Llama Vision Key-Value Extraction Results\\n\")\n        text_file.write(f\"# Document: {document_image}\\n\")\n        text_file.write(f\"# Model: Llama-3.2-11B-Vision-Instruct\\n\")\n        text_file.write(f\"# Extraction Date: {Path().resolve()}\\n\")\n        text_file.write(\"# \" + \"=\"*50 + \"\\n\\n\")\n        text_file.write(generatedOutput)\n    \n    print(f\"âœ… Llama Vision extraction results saved successfully!\")\n    print(f\"ğŸ“„ File location: {output_path}\")\n    print(f\"ğŸ“Š File size: {output_path.stat().st_size} bytes\")\n    \n    # Advanced extraction analysis and reporting\n    lines = generatedOutput.split('\\n')\n    field_lines = [line for line in lines if ':' in line and not line.strip().startswith('<')]\n    \n    print(f\"\\nğŸ“ˆ Comprehensive Extraction Analysis:\")\n    print(f\"   â€¢ Document processed: {document_image}\")\n    print(f\"   â€¢ Model used: Llama-3.2-11B-Vision-Instruct\")\n    print(f\"   â€¢ Total response lines: {len(lines)}\")\n    print(f\"   â€¢ Structured field lines: {len(field_lines)}\")\n    print(f\"   â€¢ Field extraction rate: {len(field_lines)/25*100:.1f}%\")\n    \n    # Advanced content analysis\n    non_na_fields = [line for line in field_lines if not line.split(':', 1)[1].strip().upper() in ['N/A', 'NA']]\n    print(f\"   â€¢ Fields with content: {len(non_na_fields)}\")\n    print(f\"   â€¢ Content coverage: {len(non_na_fields)/25*100:.1f}%\")\n    \n    # Quality validation metrics\n    file_size = output_path.stat().st_size\n    if file_size > 500:\n        print(\"âœ… Output file validation: EXCELLENT (comprehensive content)\")\n    elif file_size > 200:\n        print(\"âœ… Output file validation: GOOD (sufficient content)\")\n    else:\n        print(\"âš ï¸ Output file validation: WARNING (minimal content detected)\")\n    \n    # Markdown artifact analysis\n    markdown_count = generatedOutput.count('**') + generatedOutput.count('*')\n    if markdown_count == 0:\n        print(\"âœ… Format validation: PERFECT (no markdown artifacts)\")\n    else:\n        print(f\"âš ï¸ Format validation: {markdown_count} markdown artifacts detected\")\n    \n    # Integration readiness assessment\n    if len(field_lines) >= 20 and markdown_count == 0:\n        print(\"ğŸš€ Integration Status: READY (high-quality structured output)\")\n    elif len(field_lines) >= 15:\n        print(\"âš™ï¸ Integration Status: USABLE (good quality with minor gaps)\")\n    else:\n        print(\"ğŸ”§ Integration Status: NEEDS REVIEW (significant extraction issues)\")\n    \n    print(f\"\\nğŸ”— Advanced Integration Features:\")\n    print(f\"   â€¢ Database-ready structured format: âœ…\")\n    print(f\"   â€¢ API integration compatible: âœ…\")\n    print(f\"   â€¢ Batch processing ready: âœ…\")\n    print(f\"   â€¢ Quality metrics available: âœ…\")\n    print(f\"ğŸ“ Output directory: {output_dir}\")\n    \nexcept NameError:\n    print(\"âŒ Error: Extraction response not available\")\n    print(\"ğŸ’¡ Solution: Execute Cell 4 first to generate extraction results\")\n    print(\"ğŸ”„ Then re-run this cell to save and analyze the results\")\n    print(\"ğŸ“‹ Ensure Llama Vision processing completed successfully\")\n    \nexcept PermissionError:\n    print(f\"âŒ Permission Error: Cannot write to {output_path}\")\n    print(\"ğŸ’¡ Advanced Solutions:\")\n    print(\"   â€¢ Check directory write permissions\")\n    print(\"   â€¢ Verify output_dir path is accessible\")\n    print(\"   â€¢ Try running with appropriate user permissions\")\n    print(\"   â€¢ Consider alternative output directory\")\n    \nexcept OSError as e:\n    print(f\"âŒ File System Error: {e}\")\n    print(\"ğŸ’¡ System Diagnostics:\")\n    print(\"   â€¢ Check available disk space\")\n    print(\"   â€¢ Verify path validity and accessibility\")\n    print(\"   â€¢ Ensure parent directories exist and are writable\")\n    print(\"   â€¢ Check file system permissions and quotas\")\n    \nexcept Exception as e:\n    print(f\"âŒ Unexpected error during file operations: {e}\")\n    print(f\"ğŸ” Error type: {type(e).__name__}\")\n    print(\"ğŸ’¡ Advanced troubleshooting:\")\n    print(\"   â€¢ Check system resources and memory availability\")\n    print(\"   â€¢ Verify file path configuration and permissions\")\n    print(\"   â€¢ Review extraction output format and content\")\n    print(f\"ğŸ—‚ï¸ Configured output directory: {output_dir}\")\n    print(f\"ğŸ“„ Target filename: {output_filename}\")\n    \n    import traceback\n    print(f\"\\nğŸ”§ Detailed error analysis:\")\n    traceback.print_exc()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
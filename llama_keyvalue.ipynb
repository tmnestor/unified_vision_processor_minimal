{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b74612-0bba-4089-bec9-cd58a1ed91ff",
   "metadata": {},
   "source": [
    "## Llama Vision Key-Value Extraction with Comprehensive Evaluation\n",
    "\n",
    "Purpose:\n",
    "- Load Llama-3.2-11B-Vision-Instruct model for structured document analysis\n",
    "- Execute comprehensive evaluation pipeline using InternVL3's sophisticated infrastructure\n",
    "- Generate detailed reports and deployment readiness assessments\n",
    "- Provide direct comparison capabilities with other vision models\n",
    "\n",
    "Key Features:\n",
    "- Advanced batch processing with multimodal conversation format\n",
    "- Sophisticated field-specific accuracy calculation with fixed N/A handling\n",
    "- Comprehensive evaluation metrics and quality assessment\n",
    "- Executive summary generation and deployment checklists\n",
    "- Production readiness indicators and optimization recommendations\n",
    "\n",
    "Architecture:\n",
    "- Adapts Llama's multimodal message structure for document processing\n",
    "- Ports InternVL3's evaluation infrastructure with accuracy improvements\n",
    "- Maintains compatibility with existing ground truth and output formats\n",
    "- Supports 25-field structured extraction for business documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaccb4ef-ebfc-473b-8d11-5db75dece55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶ô Llama Vision Key-Value Extraction with Comprehensive Evaluation\n",
      "üìÅ Data directory: /home/jovyan/nfs_share/tod/evaluation_data\n",
      "üìÇ Output directory: /home/jovyan/nfs_share/tod/output\n",
      "üìä Ground truth: /home/jovyan/nfs_share/tod/unified_vision_processor_minimal/evaluation_ground_truth.csv\n",
      "üîß Model: /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION AND GLOBAL VARIABLES\n",
    "# ============================================================================\n",
    "data_dir = \"/home/jovyan/nfs_share/tod/evaluation_data\"  # 20 test images\n",
    "ground_truth_path = \"/home/jovyan/nfs_share/tod/unified_vision_processor_minimal/evaluation_ground_truth.csv\"  # Ground truth CSV\n",
    "model_path = \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\" \n",
    "output_dir = \"/home/jovyan/nfs_share/tod/output\"\n",
    "\n",
    "# data_dir = \"/efs/share/PoC_data/evaluation_data\"  # 20 test images\n",
    "# ground_truth_path = \"/efs/share/PoC_data/evaluation_ground_truth.csv\"  # Ground truth CSV\n",
    "# model_path = \"/efs/share/PTM/Llama-3.2-11B-Vision-Instruct\" \n",
    "# output_dir = \"/efs/share/PoC_data/output\"\n",
    "\n",
    "# 25 extraction fields in alphabetical order for consistency with InternVL3\n",
    "EXTRACTION_FIELDS = [\n",
    "    'ABN', 'ACCOUNT_HOLDER', 'BANK_ACCOUNT_NUMBER', 'BANK_NAME', 'BSB_NUMBER',\n",
    "    'BUSINESS_ADDRESS', 'BUSINESS_PHONE', 'CLOSING_BALANCE', 'DESCRIPTIONS',\n",
    "    'DOCUMENT_TYPE', 'DUE_DATE', 'GST', 'INVOICE_DATE', 'OPENING_BALANCE',\n",
    "    'PAYER_ADDRESS', 'PAYER_EMAIL', 'PAYER_NAME', 'PAYER_PHONE', 'PRICES',\n",
    "    'QUANTITIES', 'STATEMENT_PERIOD', 'SUBTOTAL', 'SUPPLIER', 'SUPPLIER_WEBSITE', 'TOTAL'\n",
    "]\n",
    "\n",
    "print(\"ü¶ô Llama Vision Key-Value Extraction with Comprehensive Evaluation\")\n",
    "print(f\"üìÅ Data directory: {data_dir}\")\n",
    "print(f\"üìÇ Output directory: {output_dir}\")\n",
    "print(f\"üìä Ground truth: {ground_truth_path}\")\n",
    "print(f\"üîß Model: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f8e8b-95e5-4046-b6df-e247667c96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced extraction prompt optimized for Llama Vision with strict formatting\n",
    "EXTRACTION_PROMPT = \"\"\"Extract key-value data from this business document image.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "- Output ONLY the structured data below\n",
    "- Do NOT include any conversation text\n",
    "- Do NOT repeat the user's request\n",
    "- Do NOT include <image> tokens\n",
    "- Start immediately with DOCUMENT_TYPE\n",
    "- Stop immediately after DESCRIPTIONS\n",
    "\n",
    "REQUIRED OUTPUT FORMAT - EXACTLY 25 LINES:\n",
    "ABN: [11-digit Australian Business Number or N/A]\n",
    "ACCOUNT_HOLDER: [value or N/A]\n",
    "BANK_ACCOUNT_NUMBER: [account number from bank statements only or N/A]\n",
    "BANK_NAME: [bank name from bank statements only or N/A]\n",
    "BSB_NUMBER: [6-digit BSB from bank statements only or N/A]\n",
    "BUSINESS_ADDRESS: [value or N/A]\n",
    "BUSINESS_PHONE: [value or N/A]\n",
    "CLOSING_BALANCE: [closing balance amount in dollars or N/A]\n",
    "DESCRIPTIONS: [list of transaction descriptions or N/A]\n",
    "DOCUMENT_TYPE: [value or N/A]\n",
    "DUE_DATE: [value or N/A]\n",
    "GST: [GST amount in dollars or N/A]\n",
    "INVOICE_DATE: [value or N/A]\n",
    "OPENING_BALANCE: [opening balance amount in dollars or N/A]\n",
    "PAYER_ADDRESS: [value or N/A]\n",
    "PAYER_EMAIL: [value or N/A]\n",
    "PAYER_NAME: [value or N/A]\n",
    "PAYER_PHONE: [value or N/A]\n",
    "PRICES: [individual prices in dollars or N/A]\n",
    "QUANTITIES: [list of quantities or N/A]\n",
    "STATEMENT_PERIOD: [value or N/A]\n",
    "SUBTOTAL: [subtotal amount in dollars or N/A]\n",
    "SUPPLIER: [value or N/A]\n",
    "SUPPLIER_WEBSITE: [value or N/A]\n",
    "TOTAL: [total amount in dollars or N/A]\n",
    "\n",
    "FORMAT RULES:\n",
    "- Use exactly: KEY: value (colon and space)\n",
    "- NEVER use: **KEY:** or **KEY** or *KEY* or any formatting\n",
    "- Plain text only - NO markdown, NO bold, NO italic\n",
    "- Include ALL 25 keys even if value is N/A\n",
    "- Output ONLY these 25 lines, nothing else\n",
    "\n",
    "STOP after DESCRIPTIONS line. Do not add explanations or comments.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7dfdfd4-76e8-4209-9459-795d0b5fc77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Llama Vision model...\n",
      "\n",
      "üîÑ Loading Llama Vision model from: /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa628a68b2e4ac38b2d0a2993200511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Llama Vision model loaded successfully\n",
      "üîß Device: cuda:0\n",
      "üíæ Model parameters: 10,670,220,835\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL LOADING AND INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def load_llama_model():\n",
    "    \"\"\"\n",
    "    Load Llama-3.2-11B-Vision-Instruct model with optimal configuration\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, processor) for document processing\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÑ Loading Llama Vision model from: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load model with optimal configuration\n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,  # Memory-efficient 16-bit precision\n",
    "            device_map=\"auto\",           # Automatic device mapping\n",
    "        )\n",
    "        \n",
    "        # Load processor for multimodal inputs\n",
    "        processor = AutoProcessor.from_pretrained(model_path)\n",
    "        \n",
    "        print(\"‚úÖ Llama Vision model loaded successfully\")\n",
    "        print(f\"üîß Device: {model.device}\")\n",
    "        print(f\"üíæ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        return model, processor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading Llama model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize global model and processor\n",
    "print(\"üöÄ Initializing Llama Vision model...\")\n",
    "model, processor = load_llama_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b9921e-37c6-4e23-8093-63215aa6c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMAGE DISCOVERY AND LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def discover_images(directory_path):\n",
    "    \"\"\"\n",
    "    Discover all image files in the specified directory\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to directory containing images\n",
    "        \n",
    "    Returns:\n",
    "        list: List of image file paths found in directory\n",
    "    \"\"\"\n",
    "    image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']\n",
    "    image_files = []\n",
    "    \n",
    "    for extension in image_extensions:\n",
    "        image_files.extend(str(p) for p in Path(directory_path).glob(extension))\n",
    "    \n",
    "    # Sort for consistent processing order\n",
    "    image_files.sort()\n",
    "    return image_files\n",
    "\n",
    "def load_document_image(image_path):\n",
    "    \"\"\"\n",
    "    Load document image with error handling\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to document image\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Loaded document image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return Image.open(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading image {image_path}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf1407c-2bd3-4d4c-bbed-f0a48fb1ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RESPONSE PARSING WITH LLAMA-SPECIFIC HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "def parse_extraction_response(response_text):\n",
    "    \"\"\"\n",
    "    Parse Llama extraction response into structured dictionary with success tracking\n",
    "    \n",
    "    Args:\n",
    "        response_text (str): Raw text response from Llama extraction\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (field_dict, extracted_fields_set, success_metadata)\n",
    "    \"\"\"\n",
    "    field_dict = {}\n",
    "    extracted_fields = set()\n",
    "    \n",
    "    # Clean Llama conversation artifacts\n",
    "    lines = response_text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    in_response = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Look for start of structured output\n",
    "        if line.startswith('DOCUMENT_TYPE:') or any(line.startswith(f'{field}:') for field in EXTRACTION_FIELDS):\n",
    "            in_response = True\n",
    "        # Skip conversation artifacts\n",
    "        if line.startswith(('user', 'assistant', '<image>', 'Extract data')):\n",
    "            in_response = False\n",
    "            continue\n",
    "        # Collect structured response lines\n",
    "        if in_response and ':' in line and not line.startswith('<'):\n",
    "            # Remove markdown artifacts common with Llama\n",
    "            clean_line = line.replace('**', '').replace('*', '').strip()\n",
    "            cleaned_lines.append(clean_line)\n",
    "    \n",
    "    # Parse cleaned lines\n",
    "    for line in cleaned_lines:\n",
    "        if ':' in line:\n",
    "            try:\n",
    "                key, value = line.split(':', 1)\n",
    "                key = key.strip().upper()\n",
    "                value = value.strip()\n",
    "                \n",
    "                # Only process expected fields\n",
    "                if key in EXTRACTION_FIELDS:\n",
    "                    field_dict[key] = value if value else 'N/A'\n",
    "                    extracted_fields.add(key)\n",
    "                    \n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # Fill missing fields\n",
    "    for field in EXTRACTION_FIELDS:\n",
    "        if field not in extracted_fields:\n",
    "            field_dict[field] = 'N/A'\n",
    "    \n",
    "    # Calculate success metadata\n",
    "    successful_extractions = len(extracted_fields)\n",
    "    fields_with_content = len([f for f in extracted_fields if field_dict[f] != 'N/A'])\n",
    "    \n",
    "    success_metadata = {\n",
    "        'response_completeness': successful_extractions,\n",
    "        'response_completeness_rate': (successful_extractions / len(EXTRACTION_FIELDS)) * 100,\n",
    "        'content_coverage': fields_with_content,\n",
    "        'content_coverage_rate': (fields_with_content / successful_extractions) * 100 if successful_extractions > 0 else 0,\n",
    "        'failed_extractions': len(EXTRACTION_FIELDS) - successful_extractions\n",
    "    }\n",
    "    \n",
    "    return field_dict, extracted_fields, success_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24f6060-0058-4b3b-a794-fc22ad0ad812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BATCH PROCESSING WITH LLAMA MULTIMODAL FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "def process_image_batch(image_files, progress_callback=None):\n",
    "    \"\"\"\n",
    "    Process batch of images through Llama Vision extraction pipeline\n",
    "    \n",
    "    Args:\n",
    "        image_files (list): List of image file paths\n",
    "        progress_callback (callable, optional): Progress update function\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (results, batch_statistics)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    batch_stats = {\n",
    "        'total_images': len(image_files),\n",
    "        'successful_responses': 0,\n",
    "        'total_fields_returned': 0,\n",
    "        'total_fields_with_content': 0,\n",
    "        'processing_errors': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"üöÄ Starting Llama Vision batch processing of {len(image_files)} images...\")\n",
    "    \n",
    "    for i, image_file in enumerate(image_files, 1):\n",
    "        image_name = Path(image_file).name\n",
    "        print(f\"üì∑ Processing ({i}/{len(image_files)}): {image_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            image = load_document_image(image_file)\n",
    "            \n",
    "            # Create Llama's multimodal message structure\n",
    "            message_structure = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": EXTRACTION_PROMPT}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Apply chat template\n",
    "            text_input = processor.apply_chat_template(\n",
    "                message_structure, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Process inputs\n",
    "            inputs = processor(image, text_input, return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1000,\n",
    "                do_sample=False,\n",
    "                temperature=None,\n",
    "                top_p=None,\n",
    "                pad_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # Decode response\n",
    "            response = processor.decode(output[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract assistant response\n",
    "            if \"assistant\" in response:\n",
    "                response = response.split(\"assistant\")[-1].strip()\n",
    "            \n",
    "            # Parse response\n",
    "            extracted_fields, returned_fields, success_meta = parse_extraction_response(response)\n",
    "            \n",
    "            # Create result\n",
    "            result_row = {'image_name': image_name}\n",
    "            result_row.update(extracted_fields)\n",
    "            result_row['_response_completeness'] = success_meta['response_completeness']\n",
    "            result_row['_content_coverage'] = success_meta['content_coverage']\n",
    "            results.append(result_row)\n",
    "            \n",
    "            # Update statistics\n",
    "            batch_stats['successful_responses'] += 1\n",
    "            batch_stats['total_fields_returned'] += success_meta['response_completeness']\n",
    "            batch_stats['total_fields_with_content'] += success_meta['content_coverage']\n",
    "            \n",
    "            print(f\"   ‚úÖ Model returned {success_meta['response_completeness']}/25 fields ({success_meta['response_completeness_rate']:.1f}%)\")\n",
    "            print(f\"   üìä Content in {success_meta['content_coverage']} fields ({success_meta['content_coverage_rate']:.1f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Processing error for {image_name}: {str(e)}\")\n",
    "            \n",
    "            # Create error result\n",
    "            error_result = {'image_name': image_name}\n",
    "            error_result.update({field: 'N/A' for field in EXTRACTION_FIELDS})\n",
    "            error_result['_response_completeness'] = 0\n",
    "            error_result['_content_coverage'] = 0\n",
    "            results.append(error_result)\n",
    "            \n",
    "            batch_stats['processing_errors'] += 1\n",
    "        \n",
    "        if progress_callback:\n",
    "            progress_callback(i, len(image_files), image_name)\n",
    "    \n",
    "    return results, batch_stats\n",
    "\n",
    "def create_extraction_dataframe(results):\n",
    "    \"\"\"\n",
    "    Create pandas DataFrame from extraction results\n",
    "    \n",
    "    Args:\n",
    "        results (list): Extraction results\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (main_df, metadata_df)\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        columns = ['image_name'] + EXTRACTION_FIELDS\n",
    "        return pd.DataFrame(columns=columns), pd.DataFrame()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata_columns = ['image_name', '_response_completeness', '_content_coverage']\n",
    "    metadata_df = results_df[metadata_columns].copy() if all(col in results_df.columns for col in metadata_columns) else pd.DataFrame()\n",
    "    \n",
    "    # Main DataFrame with proper column ordering\n",
    "    main_columns = ['image_name'] + EXTRACTION_FIELDS\n",
    "    main_df = results_df[main_columns] if all(col in results_df.columns for col in main_columns) else results_df\n",
    "    \n",
    "    column_order = ['image_name'] + EXTRACTION_FIELDS\n",
    "    main_df = main_df.reindex(columns=column_order, fill_value='N/A')\n",
    "    \n",
    "    return main_df, metadata_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ff5342-acdf-453d-800e-5ef6e93b8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GROUND TRUTH LOADING AND VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def load_ground_truth(csv_path):\n",
    "    \"\"\"\n",
    "    Load ground truth CSV and create image-to-ground-truth mapping\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to evaluation_ground_truth.csv\n",
    "        \n",
    "    Returns:\n",
    "        dict: Mapping of image_file to ground truth field dictionary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load ground truth CSV\n",
    "        gt_df = pd.read_csv(csv_path)\n",
    "        print(f\"üìä Loaded ground truth: {len(gt_df)} rows √ó {len(gt_df.columns)} columns\")\n",
    "        \n",
    "        # Validate structure\n",
    "        expected_columns = ['image_file'] + EXTRACTION_FIELDS\n",
    "        actual_columns = list(gt_df.columns)\n",
    "        \n",
    "        if len(actual_columns) != len(expected_columns):\n",
    "            print(f\"‚ö†Ô∏è Column count mismatch: expected {len(expected_columns)}, got {len(actual_columns)}\")\n",
    "        \n",
    "        # Check field alignment\n",
    "        missing_fields = set(expected_columns) - set(actual_columns)\n",
    "        extra_fields = set(actual_columns) - set(expected_columns)\n",
    "        \n",
    "        if missing_fields:\n",
    "            print(f\"‚ö†Ô∏è Missing fields in ground truth: {missing_fields}\")\n",
    "        if extra_fields:\n",
    "            print(f\"‚ö†Ô∏è Extra fields in ground truth: {extra_fields}\")\n",
    "        \n",
    "        # Create mapping\n",
    "        ground_truth_map = {}\n",
    "        for _, row in gt_df.iterrows():\n",
    "            image_file = row['image_file']\n",
    "            gt_data = {field: str(row[field]) if pd.notna(row[field]) else 'N/A' \n",
    "                      for field in EXTRACTION_FIELDS if field in row.index}\n",
    "            ground_truth_map[image_file] = gt_data\n",
    "        \n",
    "        print(f\"‚úÖ Created ground truth mapping for {len(ground_truth_map)} images\")\n",
    "        return ground_truth_map\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Ground truth file not found: {csv_path}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading ground truth: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5559cb54-ee1d-43fb-a01f-3d1bc4b36291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIELD ACCURACY CALCULATION WITH FIXED N/A HANDLING  \n",
    "# ============================================================================\n",
    "\n",
    "def calculate_field_accuracy(extracted_value, ground_truth_value, field_name):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for a specific field using sophisticated comparison logic\n",
    "    FIXED: Now includes comprehensive N/A variants including 'nan'\n",
    "    \n",
    "    Args:\n",
    "        extracted_value (str): Value extracted by the model\n",
    "        ground_truth_value (str): Correct value from ground truth\n",
    "        field_name (str): Name of the field being compared\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy score between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # FIXED: Comprehensive N/A variants including 'nan' which was missing\n",
    "    na_variants = ['N/A', 'NA', '', 'NAN', 'NULL', 'NONE', 'NIL']\n",
    "    \n",
    "    # Handle missing values with expanded variant detection\n",
    "    if not ground_truth_value or str(ground_truth_value).upper() in na_variants:\n",
    "        return 1.0 if (not extracted_value or str(extracted_value).upper() in na_variants) else 0.0\n",
    "    \n",
    "    if not extracted_value or str(extracted_value).upper() in na_variants:\n",
    "        return 0.0  # Ground truth exists but nothing extracted\n",
    "    \n",
    "    # Normalize for comparison\n",
    "    extracted_clean = str(extracted_value).strip()\n",
    "    gt_clean = str(ground_truth_value).strip()\n",
    "    \n",
    "    # Exact match (case-insensitive)\n",
    "    if extracted_clean.lower() == gt_clean.lower():\n",
    "        return 1.0\n",
    "    \n",
    "    # Field-specific comparison logic\n",
    "    if field_name in ['GST', 'TOTAL', 'SUBTOTAL', 'OPENING_BALANCE', 'CLOSING_BALANCE']:\n",
    "        # Numeric comparison with tolerance for financial fields\n",
    "        try:\n",
    "            ext_num = float(re.sub(r'[^\\d.-]', '', extracted_clean.replace(',', '')))\n",
    "            gt_num = float(re.sub(r'[^\\d.-]', '', gt_clean.replace(',', '')))\n",
    "            \n",
    "            tolerance = 0.01\n",
    "            return 1.0 if abs(ext_num - gt_num) < tolerance else 0.0\n",
    "            \n",
    "        except (ValueError, TypeError):\n",
    "            return 1.0 if extracted_clean.lower() == gt_clean.lower() else 0.0\n",
    "    \n",
    "    elif field_name in ['QUANTITIES', 'PRICES', 'DESCRIPTIONS']:\n",
    "        # List comparison for pipe-separated values\n",
    "        try:\n",
    "            ext_items = [item.strip() for item in extracted_clean.split('|')]\n",
    "            gt_items = [item.strip() for item in gt_clean.split('|')]\n",
    "            \n",
    "            if len(ext_items) != len(gt_items):\n",
    "                return 0.0\n",
    "            \n",
    "            matches = sum(1 for e, g in zip(ext_items, gt_items, strict=False) \n",
    "                         if e.lower().strip() == g.lower().strip())\n",
    "            \n",
    "            return matches / len(gt_items) if gt_items else 0.0\n",
    "            \n",
    "        except Exception:\n",
    "            return 1.0 if extracted_clean.lower() == gt_clean.lower() else 0.0\n",
    "    \n",
    "    elif field_name in ['INVOICE_DATE', 'DUE_DATE']:\n",
    "        # Date comparison with flexible format handling\n",
    "        try:\n",
    "            ext_date = re.sub(r'[^\\d/\\-]', '', extracted_clean)\n",
    "            gt_date = re.sub(r'[^\\d/\\-]', '', gt_clean)\n",
    "            \n",
    "            return 1.0 if ext_date == gt_date else 0.0\n",
    "            \n",
    "        except Exception:\n",
    "            return 1.0 if extracted_clean.lower() == gt_clean.lower() else 0.0\n",
    "    \n",
    "    else:\n",
    "        # String comparison with fuzzy matching\n",
    "        if extracted_clean.lower() == gt_clean.lower():\n",
    "            return 1.0\n",
    "        elif (extracted_clean.lower() in gt_clean.lower() or \n",
    "              gt_clean.lower() in extracted_clean.lower()):\n",
    "            return 0.8  # Partial match\n",
    "        else:\n",
    "            return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6d4a6b-0b84-4cc5-89d2-893b3936fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE EVALUATION PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_extraction_results(extraction_results, ground_truth_map):\n",
    "    \"\"\"\n",
    "    Evaluate extraction results against ground truth with comprehensive metrics\n",
    "    \n",
    "    Args:\n",
    "        extraction_results (list): List of extraction result dictionaries\n",
    "        ground_truth_map (dict): Ground truth mapping by image filename\n",
    "        \n",
    "    Returns:\n",
    "        dict: Comprehensive evaluation metrics and analysis\n",
    "    \"\"\"\n",
    "    if not extraction_results or not ground_truth_map:\n",
    "        return {\"error\": \"Missing extraction results or ground truth data\"}\n",
    "    \n",
    "    evaluation_data = []\n",
    "    field_accuracies = {field: [] for field in EXTRACTION_FIELDS}\n",
    "    overall_accuracies = []\n",
    "    \n",
    "    print(f\"\\nüéØ Evaluating {len(extraction_results)} Llama extraction results against ground truth...\")\n",
    "    \n",
    "    for i, result in enumerate(extraction_results, 1):\n",
    "        image_name = result['image_name']\n",
    "        print(f\"üìä Evaluating ({i}/{len(extraction_results)}): {image_name}\")\n",
    "        \n",
    "        # Get ground truth\n",
    "        gt_data = ground_truth_map.get(image_name, {})\n",
    "        \n",
    "        if not gt_data:\n",
    "            print(f\"   ‚ö†Ô∏è No ground truth found for {image_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate field-wise accuracies\n",
    "        image_evaluation = {'image_name': image_name}\n",
    "        image_field_accuracies = {}\n",
    "        \n",
    "        for field in EXTRACTION_FIELDS:\n",
    "            extracted_value = result.get(field, 'N/A')\n",
    "            gt_value = gt_data.get(field, 'N/A')\n",
    "            \n",
    "            accuracy = calculate_field_accuracy(extracted_value, gt_value, field)\n",
    "            image_field_accuracies[field] = accuracy\n",
    "            field_accuracies[field].append(accuracy)\n",
    "            \n",
    "            # Store evaluation data\n",
    "            image_evaluation[f'{field}_extracted'] = extracted_value\n",
    "            image_evaluation[f'{field}_ground_truth'] = gt_value\n",
    "            image_evaluation[f'{field}_accuracy'] = accuracy\n",
    "        \n",
    "        # Calculate overall accuracy\n",
    "        image_accuracy = sum(image_field_accuracies.values()) / len(image_field_accuracies)\n",
    "        image_evaluation['overall_accuracy'] = image_accuracy\n",
    "        overall_accuracies.append(image_accuracy)\n",
    "        \n",
    "        evaluation_data.append(image_evaluation)\n",
    "        \n",
    "        # Progress report\n",
    "        fields_correct = sum(1 for acc in image_field_accuracies.values() if acc >= 0.99)\n",
    "        print(f\"   ‚úÖ {fields_correct}/25 fields correct ({image_accuracy:.1%} accuracy)\")\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    evaluation_summary = {\n",
    "        'total_images': len(evaluation_data),\n",
    "        'overall_accuracy': sum(overall_accuracies) / len(overall_accuracies) if overall_accuracies else 0.0,\n",
    "        'perfect_documents': sum(1 for acc in overall_accuracies if acc >= 0.99),\n",
    "        'good_documents': sum(1 for acc in overall_accuracies if 0.8 <= acc < 0.99),\n",
    "        'fair_documents': sum(1 for acc in overall_accuracies if 0.6 <= acc < 0.8),\n",
    "        'poor_documents': sum(1 for acc in overall_accuracies if acc < 0.6),\n",
    "        'field_accuracies': {field: sum(accs) / len(accs) if accs else 0.0 \n",
    "                           for field, accs in field_accuracies.items()},\n",
    "        'evaluation_data': evaluation_data,\n",
    "        'best_performing_image': max(evaluation_data, key=lambda x: x['overall_accuracy'])['image_name'] if evaluation_data else '',\n",
    "        'worst_performing_image': min(evaluation_data, key=lambda x: x['overall_accuracy'])['image_name'] if evaluation_data else '',\n",
    "        'best_performance_accuracy': max(overall_accuracies) if overall_accuracies else 0.0,\n",
    "        'worst_performance_accuracy': min(overall_accuracies) if overall_accuracies else 0.0\n",
    "    }\n",
    "    \n",
    "    return evaluation_summary# ============================================================================\n",
    "# REPORT GENERATION AND ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_comprehensive_reports(evaluation_summary, output_dir_path):\n",
    "    \"\"\"\n",
    "    Generate comprehensive evaluation reports including executive summary and deployment checklist\n",
    "    \n",
    "    Args:\n",
    "        evaluation_summary (dict): Evaluation results and metrics\n",
    "        output_dir_path (Path): Output directory path\n",
    "        \n",
    "    Returns:\n",
    "        dict: Paths to generated reports\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Executive Summary Report\n",
    "    summary_stats = evaluation_summary\n",
    "    sorted_fields = sorted(summary_stats['field_accuracies'].items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    executive_summary = f\"\"\"# Llama Vision Key-Value Extraction - Executive Summary\n",
    "\n",
    "## Model Performance Overview\n",
    "**Model:** Llama-3.2-11B-Vision-Instruct  \n",
    "**Evaluation Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Documents Processed:** {summary_stats['total_images']}  \n",
    "**Average Accuracy:** {summary_stats['overall_accuracy']:.1%}\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "1. **Document Analysis:** Processed {summary_stats['total_images']} business documents with comprehensive field extraction\n",
    "2. **Field Extraction:** Successfully extracts {len([f for f, acc in summary_stats['field_accuracies'].items() if acc >= 0.9])} out of 25 fields with ‚â•90% accuracy\n",
    "3. **Best Performance:** {summary_stats['best_performing_image']} ({summary_stats['best_performance_accuracy']:.1%} accuracy)\n",
    "4. **Challenging Cases:** {summary_stats['worst_performing_image']} ({summary_stats['worst_performance_accuracy']:.1%} accuracy)\n",
    "\n",
    "## Field Performance Analysis\n",
    "\n",
    "### Top Performing Fields (‚â•90% accuracy)\n",
    "\"\"\"\n",
    "    \n",
    "    excellent_fields = [field for field, accuracy in sorted_fields if accuracy >= 0.9]\n",
    "    if excellent_fields:\n",
    "        for i, (field, accuracy) in enumerate([item for item in sorted_fields if item[1] >= 0.9][:10], 1):\n",
    "            executive_summary += f\"{i:2d}. {field:<20} {accuracy:.1%}\\n\"\n",
    "    else:\n",
    "        executive_summary += \"No fields achieved ‚â•90% accuracy\\n\"\n",
    "    \n",
    "    executive_summary += \"\"\"\n",
    "### Challenging Fields (Requires Attention)\n",
    "\"\"\"\n",
    "    \n",
    "    challenging_fields = [(field, accuracy) for field, accuracy in sorted_fields[-5:] if accuracy < 0.9]\n",
    "    for i, (field, accuracy) in enumerate(challenging_fields, 1):\n",
    "        executive_summary += f\"{i}. {field:<20} {accuracy:.1%}\\n\"\n",
    "    \n",
    "    # Production readiness assessment\n",
    "    if summary_stats['overall_accuracy'] >= 0.9:\n",
    "        grade = \"A+ (Excellent)\"\n",
    "        status = \"‚úÖ **READY FOR PRODUCTION:** Model demonstrates excellent accuracy and consistency\"\n",
    "    elif summary_stats['overall_accuracy'] >= 0.8:\n",
    "        grade = \"A (Good)\" \n",
    "        status = \"‚úÖ **READY FOR PRODUCTION:** Model shows good performance with minor limitations\"\n",
    "    elif summary_stats['overall_accuracy'] >= 0.7:\n",
    "        grade = \"B (Fair)\"\n",
    "        status = \"‚ö†Ô∏è **REQUIRES OPTIMIZATION:** Consider fine-tuning or prompt engineering\"\n",
    "    else:\n",
    "        grade = \"C (Needs Improvement)\"\n",
    "        status = \"‚ùå **NOT READY FOR PRODUCTION:** Significant accuracy improvements needed\"\n",
    "    \n",
    "    executive_summary += f\"\"\"\n",
    "**Overall Grade:** {grade}\n",
    "\n",
    "## Production Readiness Assessment\n",
    "\n",
    "{status}\n",
    "\n",
    "## Document Quality Distribution\n",
    "- Perfect Documents (‚â•99%): {summary_stats['perfect_documents']} ({summary_stats['perfect_documents']/summary_stats['total_images']*100:.1f}%)\n",
    "- Good Documents (80-98%): {summary_stats['good_documents']} ({summary_stats['good_documents']/summary_stats['total_images']*100:.1f}%)  \n",
    "- Fair Documents (60-79%): {summary_stats['fair_documents']} ({summary_stats['fair_documents']/summary_stats['total_images']*100:.1f}%)\n",
    "- Poor Documents (<60%): {summary_stats['poor_documents']} ({summary_stats['poor_documents']/summary_stats['total_images']*100:.1f}%)\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "### Immediate Actions\n",
    "{\"1. ‚úÖ DEPLOY TO PRODUCTION - Model ready for automated processing\" if summary_stats['overall_accuracy'] >= 0.9 else \"1. ‚ö†Ô∏è PILOT DEPLOYMENT - Test with subset of documents\" if summary_stats['overall_accuracy'] >= 0.8 else \"1. üîß OPTIMIZATION REQUIRED - Improve model before deployment\"}\n",
    "2. üìã Establish monitoring dashboards for accuracy tracking\n",
    "3. üéØ Focus improvement efforts on challenging fields: {', '.join([f[0] for f in challenging_fields[:3]])}\n",
    "\n",
    "### Strategic Initiatives  \n",
    "- üîÑ Implement continuous evaluation pipeline\n",
    "- üìä Expand ground truth dataset for challenging document types\n",
    "- ‚ö° Optimize inference pipeline for production scale\n",
    "\n",
    "---\n",
    "üìä Llama-3.2-11B-Vision achieved {summary_stats['overall_accuracy']:.1%} average accuracy\n",
    "\"\"\"\n",
    "    \n",
    "    # Save executive summary\n",
    "    report_filename = f\"llama_comprehensive_evaluation_report_{timestamp}.md\"\n",
    "    report_path = output_dir_path / report_filename\n",
    "    with report_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(executive_summary)\n",
    "    \n",
    "    # Deployment Checklist\n",
    "    deployment_checklist = f\"\"\"# Llama Vision Deployment Readiness Checklist\n",
    "\n",
    "## Model Information\n",
    "- **Model:** Llama-3.2-11B-Vision-Instruct\n",
    "- **Evaluation Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Overall Accuracy:** {summary_stats['overall_accuracy']:.1%}\n",
    "\n",
    "## Production Readiness Checklist\n",
    "\n",
    "### Performance Metrics\n",
    "- [{'x' if summary_stats['overall_accuracy'] >= 0.8 else ' '}] Overall accuracy ‚â•80% ({summary_stats['overall_accuracy']:.1%})\n",
    "- [{'x' if len(excellent_fields) >= 15 else ' '}] At least 15 fields with ‚â•90% accuracy ({len(excellent_fields)}/25)\n",
    "- [{'x' if summary_stats['perfect_documents'] >= summary_stats['total_images'] * 0.3 else ' '}] At least 30% perfect documents ({summary_stats['perfect_documents']}/{summary_stats['total_images']})\n",
    "\n",
    "### Quality Assessment\n",
    "- Best Case: {summary_stats['best_performance_accuracy']:.1%} accuracy\n",
    "- Worst Case: {summary_stats['worst_performance_accuracy']:.1%} accuracy\n",
    "\n",
    "### Field Performance\n",
    "- Track accuracy for critical fields: {', '.join(excellent_fields[:5])}\n",
    "- Monitor challenging fields: {', '.join([f[0] for f in challenging_fields[:3]])}\n",
    "\n",
    "## Deployment Strategy\n",
    "\n",
    "{\"‚úÖ **APPROVED FOR PRODUCTION DEPLOYMENT**\" if summary_stats['overall_accuracy'] >= 0.8 else \"‚ö†Ô∏è **PILOT DEPLOYMENT RECOMMENDED**\" if summary_stats['overall_accuracy'] >= 0.7 else \"üîß **OPTIMIZATION REQUIRED BEFORE DEPLOYMENT**\"}\n",
    "\n",
    "### Next Steps\n",
    "1. {'‚úÖ Deploy to production environment' if summary_stats['overall_accuracy'] >= 0.8 else 'üß™ Run pilot with subset of documents' if summary_stats['overall_accuracy'] >= 0.7 else 'üîß Optimize model performance'}\n",
    "2. üìä Implement real-time accuracy monitoring\n",
    "3. üîÑ Establish continuous evaluation pipeline\n",
    "4. üìã Create operational runbooks and troubleshooting guides\n",
    "\n",
    "---\n",
    "*Generated by Llama Vision Evaluation Pipeline*\n",
    "\"\"\"\n",
    "    \n",
    "    # Save deployment checklist  \n",
    "    checklist_filename = f\"llama_deployment_checklist_{timestamp}.md\"\n",
    "    checklist_path = output_dir_path / checklist_filename\n",
    "    with checklist_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(deployment_checklist)\n",
    "    \n",
    "    return {\n",
    "        'executive_summary': report_path,\n",
    "        'deployment_checklist': checklist_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "285e299d-85ca-4f78-b036-01daaa6d5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution pipeline for Llama Vision key-value extraction and evaluation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ü¶ô LLAMA VISION COMPREHENSIVE EVALUATION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    output_dir_path = Path(output_dir)\n",
    "    output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Discover images\n",
    "    print(f\"\\nüìÅ Discovering images in: {data_dir}\")\n",
    "    image_files = discover_images(data_dir)\n",
    "    image_files = [f for f in image_files if 'synthetic_invoice' in Path(f).name]  # Filter for test images\n",
    "    \n",
    "    print(f\"üì∑ Found {len(image_files)} images for processing\")\n",
    "    if not image_files:\n",
    "        print(\"‚ùå No images found for processing\")\n",
    "        return\n",
    "    \n",
    "    # Process batch\n",
    "    print(\"\\nüöÄ Starting batch processing...\")\n",
    "    results, batch_stats = process_image_batch(image_files)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    print(\"\\nüìä Creating extraction DataFrames...\")\n",
    "    main_df, metadata_df = create_extraction_dataframe(results)\n",
    "    \n",
    "    # Save extraction results\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    extraction_csv = output_dir_path / f\"llama_batch_extraction_{timestamp}.csv\"\n",
    "    main_df.to_csv(extraction_csv, index=False)\n",
    "    print(f\"üíæ Extraction results saved: {extraction_csv}\")\n",
    "    \n",
    "    # Load ground truth\n",
    "    print(f\"\\nüìä Loading ground truth from: {ground_truth_path}\")\n",
    "    ground_truth_data = load_ground_truth(ground_truth_path)\n",
    "    \n",
    "    if not ground_truth_data:\n",
    "        print(\"‚ùå No ground truth data available - skipping evaluation\")\n",
    "        return\n",
    "    \n",
    "    # Perform evaluation\n",
    "    print(\"\\nüéØ Performing comprehensive evaluation...\")\n",
    "    evaluation_summary = evaluate_extraction_results(results, ground_truth_data)\n",
    "    \n",
    "    if 'error' in evaluation_summary:\n",
    "        print(f\"‚ùå Evaluation error: {evaluation_summary['error']}\")\n",
    "        return\n",
    "    \n",
    "    # Save detailed evaluation results\n",
    "    eval_csv = output_dir_path / f\"llama_ground_truth_evaluation_{timestamp}.csv\"\n",
    "    eval_df = pd.DataFrame(evaluation_summary['evaluation_data'])\n",
    "    eval_df.to_csv(eval_csv, index=False)\n",
    "    print(f\"üíæ Detailed evaluation saved: {eval_csv}\")\n",
    "    \n",
    "    # Generate comprehensive reports\n",
    "    print(\"\\nüìã Generating comprehensive reports...\")\n",
    "    report_paths = generate_comprehensive_reports(evaluation_summary, output_dir_path)\n",
    "    \n",
    "    # Save evaluation summary as JSON\n",
    "    summary_json = output_dir_path / f\"llama_evaluation_summary_{timestamp}.json\"\n",
    "    with summary_json.open('w', encoding='utf-8') as f:\n",
    "        # Make summary JSON serializable\n",
    "        json_summary = {\n",
    "            'overall_accuracy': evaluation_summary['overall_accuracy'],\n",
    "            'total_images': evaluation_summary['total_images'],\n",
    "            'perfect_documents': evaluation_summary['perfect_documents'],\n",
    "            'field_accuracies': evaluation_summary['field_accuracies'],\n",
    "            'best_performing_image': evaluation_summary['best_performing_image'],\n",
    "            'worst_performing_image': evaluation_summary['worst_performing_image'],\n",
    "            'evaluation_timestamp': timestamp\n",
    "        }\n",
    "        json.dump(json_summary, f, indent=2)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä LLAMA VISION EVALUATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéØ Overall Accuracy: {evaluation_summary['overall_accuracy']:.1%}\")\n",
    "    print(f\"üì∑ Images Processed: {evaluation_summary['total_images']}\")\n",
    "    print(f\"üèÜ Perfect Documents: {evaluation_summary['perfect_documents']}\")\n",
    "    print(f\"üìÅ Results Directory: {output_dir_path}\")\n",
    "    print(\"\\nüìÑ Generated Files:\")\n",
    "    print(f\"   ‚Ä¢ {extraction_csv.name} - Extraction results\")\n",
    "    print(f\"   ‚Ä¢ {eval_csv.name} - Detailed evaluation\")\n",
    "    print(f\"   ‚Ä¢ {report_paths['executive_summary'].name} - Executive summary\")\n",
    "    print(f\"   ‚Ä¢ {report_paths['deployment_checklist'].name} - Deployment checklist\")\n",
    "    print(f\"   ‚Ä¢ {summary_json.name} - JSON summary\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7111f70d-1d23-4f03-95a9-45edf44eee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ü¶ô LLAMA VISION COMPREHENSIVE EVALUATION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "üìÅ Discovering images in: /home/jovyan/nfs_share/tod/evaluation_data\n",
      "üì∑ Found 20 images for processing\n",
      "\n",
      "üöÄ Starting batch processing...\n",
      "üöÄ Starting Llama Vision batch processing of 20 images...\n",
      "üì∑ Processing (1/20): synthetic_invoice_001.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 17 fields (68.0%)\n",
      "üì∑ Processing (2/20): synthetic_invoice_002.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 17 fields (68.0%)\n",
      "üì∑ Processing (3/20): synthetic_invoice_003.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 17 fields (68.0%)\n",
      "üì∑ Processing (4/20): synthetic_invoice_004.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 18 fields (72.0%)\n",
      "üì∑ Processing (5/20): synthetic_invoice_005.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 17 fields (68.0%)\n",
      "üì∑ Processing (6/20): synthetic_invoice_006.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 5 fields (20.0%)\n",
      "üì∑ Processing (7/20): synthetic_invoice_007.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 17 fields (68.0%)\n",
      "üì∑ Processing (8/20): synthetic_invoice_008.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 6 fields (24.0%)\n",
      "üì∑ Processing (9/20): synthetic_invoice_009.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 6 fields (24.0%)\n",
      "üì∑ Processing (10/20): synthetic_invoice_010.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 17 fields (68.0%)\n",
      "üì∑ Processing (11/20): synthetic_invoice_011.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 17 fields (68.0%)\n",
      "üì∑ Processing (12/20): synthetic_invoice_012.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 17 fields (68.0%)\n",
      "üì∑ Processing (13/20): synthetic_invoice_013.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 5 fields (20.0%)\n",
      "üì∑ Processing (14/20): synthetic_invoice_014.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 15 fields (60.0%)\n",
      "üì∑ Processing (15/20): synthetic_invoice_015.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 17 fields (68.0%)\n",
      "üì∑ Processing (16/20): synthetic_invoice_016.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 17 fields (68.0%)\n",
      "üì∑ Processing (17/20): synthetic_invoice_017.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 6 fields (24.0%)\n",
      "üì∑ Processing (18/20): synthetic_invoice_018.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 15 fields (60.0%)\n",
      "üì∑ Processing (19/20): synthetic_invoice_019.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 17 fields (68.0%)\n",
      "üì∑ Processing (20/20): synthetic_invoice_020.png\n",
      "   ‚úÖ Model returned 25/25 fields (100.0%)\n",
      "   üìä Content in 18 fields (72.0%)\n",
      "\n",
      "üìä Creating extraction DataFrames...\n",
      "üíæ Extraction results saved: /home/jovyan/nfs_share/tod/output/llama_batch_extraction_20250807_005245.csv\n",
      "\n",
      "üìä Loading ground truth from: /home/jovyan/nfs_share/tod/unified_vision_processor_minimal/evaluation_ground_truth.csv\n",
      "üìä Loaded ground truth: 20 rows √ó 26 columns\n",
      "‚úÖ Created ground truth mapping for 20 images\n",
      "\n",
      "üéØ Performing comprehensive evaluation...\n",
      "\n",
      "üéØ Evaluating 20 Llama extraction results against ground truth...\n",
      "üìä Evaluating (1/20): synthetic_invoice_001.png\n",
      "   ‚úÖ 18/25 fields correct (75.2% accuracy)\n",
      "üìä Evaluating (2/20): synthetic_invoice_002.png\n",
      "   ‚úÖ 19/25 fields correct (79.2% accuracy)\n",
      "üìä Evaluating (3/20): synthetic_invoice_003.png\n",
      "   ‚úÖ 20/25 fields correct (83.2% accuracy)\n",
      "üìä Evaluating (4/20): synthetic_invoice_004.png\n",
      "   ‚úÖ 19/25 fields correct (76.0% accuracy)\n",
      "üìä Evaluating (5/20): synthetic_invoice_005.png\n",
      "   ‚úÖ 19/25 fields correct (76.0% accuracy)\n",
      "üìä Evaluating (6/20): synthetic_invoice_006.png\n",
      "   ‚úÖ 19/25 fields correct (79.2% accuracy)\n",
      "üìä Evaluating (7/20): synthetic_invoice_007.png\n",
      "   ‚úÖ 20/25 fields correct (80.0% accuracy)\n",
      "üìä Evaluating (8/20): synthetic_invoice_008.png\n",
      "   ‚úÖ 20/25 fields correct (83.2% accuracy)\n",
      "üìä Evaluating (9/20): synthetic_invoice_009.png\n",
      "   ‚úÖ 20/25 fields correct (83.2% accuracy)\n",
      "üìä Evaluating (10/20): synthetic_invoice_010.png\n",
      "   ‚úÖ 19/25 fields correct (79.2% accuracy)\n",
      "üìä Evaluating (11/20): synthetic_invoice_011.png\n",
      "   ‚úÖ 19/25 fields correct (79.2% accuracy)\n",
      "üìä Evaluating (12/20): synthetic_invoice_012.png\n",
      "   ‚úÖ 21/25 fields correct (84.0% accuracy)\n",
      "üìä Evaluating (13/20): synthetic_invoice_013.png\n",
      "   ‚úÖ 20/25 fields correct (83.2% accuracy)\n",
      "üìä Evaluating (14/20): synthetic_invoice_014.png\n",
      "   ‚úÖ 20/25 fields correct (80.0% accuracy)\n",
      "üìä Evaluating (15/20): synthetic_invoice_015.png\n",
      "   ‚úÖ 21/25 fields correct (84.0% accuracy)\n",
      "üìä Evaluating (16/20): synthetic_invoice_016.png\n",
      "   ‚úÖ 21/25 fields correct (84.0% accuracy)\n",
      "üìä Evaluating (17/20): synthetic_invoice_017.png\n",
      "   ‚úÖ 20/25 fields correct (83.2% accuracy)\n",
      "üìä Evaluating (18/20): synthetic_invoice_018.png\n",
      "   ‚úÖ 18/25 fields correct (72.0% accuracy)\n",
      "üìä Evaluating (19/20): synthetic_invoice_019.png\n",
      "   ‚úÖ 20/25 fields correct (80.0% accuracy)\n",
      "üìä Evaluating (20/20): synthetic_invoice_020.png\n",
      "   ‚úÖ 19/25 fields correct (79.2% accuracy)\n",
      "üíæ Detailed evaluation saved: /home/jovyan/nfs_share/tod/output/llama_ground_truth_evaluation_20250807_005245.csv\n",
      "\n",
      "üìã Generating comprehensive reports...\n",
      "\n",
      "================================================================================\n",
      "üìä LLAMA VISION EVALUATION COMPLETE\n",
      "================================================================================\n",
      "üéØ Overall Accuracy: 80.2%\n",
      "üì∑ Images Processed: 20\n",
      "üèÜ Perfect Documents: 0\n",
      "üìÅ Results Directory: /home/jovyan/nfs_share/tod/output\n",
      "\n",
      "üìÑ Generated Files:\n",
      "   ‚Ä¢ llama_batch_extraction_20250807_005245.csv - Extraction results\n",
      "   ‚Ä¢ llama_ground_truth_evaluation_20250807_005245.csv - Detailed evaluation\n",
      "   ‚Ä¢ llama_comprehensive_evaluation_report_20250807_005245.md - Executive summary\n",
      "   ‚Ä¢ llama_deployment_checklist_20250807_005245.md - Deployment checklist\n",
      "   ‚Ä¢ llama_evaluation_summary_20250807_005245.json - JSON summary\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    main()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Process interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d1faf-c8c5-47ad-bdfa-6458fc210225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

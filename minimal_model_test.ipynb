{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Vision Model Test\n",
    "\n",
    "Direct model loading and testing without using the unified_vision_processor package.\n",
    "\n",
    "All configuration is embedded in the notebook for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Model: llama (using WORKING vision_processor patterns)\n",
      "Image: datasets/image14.png\n",
      "Prompt: <|image|>Extract data from this receipt in KEY-VALUE format.\n",
      "\n",
      "Output format:\n",
      "DATE: [date from receip...\n",
      "\n",
      "‚úÖ Using PROVEN working patterns from vision_processor/models/llama_model.py\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Modify as needed\n",
    "CONFIG = {\n",
    "    # Model selection: \"llama\" or \"internvl\"\n",
    "    \"model_type\": \"llama\",  # BACK TO LLAMA with working code patterns\n",
    "    \n",
    "    # Model paths\n",
    "    \"model_paths\": {\n",
    "        \"llama\": \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision\",\n",
    "        \"internvl\": \"/home/jovyan/nfs_share/models/InternVL3-8B\"\n",
    "    },\n",
    "    \n",
    "    # Test image path\n",
    "    \"test_image\": \"datasets/image14.png\",\n",
    "    \n",
    "    # WORKING prompt pattern from vision_processor (KEY-VALUE format)\n",
    "    \"prompt\": \"<|image|>Extract data from this receipt in KEY-VALUE format.\\n\\nOutput format:\\nDATE: [date from receipt]\\nSTORE: [store name]\\nTOTAL: [total amount]\\n\\nExtract all visible text and format as KEY: VALUE pairs only.\",\n",
    "    \n",
    "    # EXACT working generation parameters from LlamaVisionModel\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"enable_quantization\": True\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"Model: {CONFIG['model_type']} (using WORKING vision_processor patterns)\")\n",
    "print(f\"Image: {CONFIG['test_image']}\")\n",
    "print(f\"Prompt: {CONFIG['prompt'][:100]}...\")\n",
    "print(\"\\n‚úÖ Using PROVEN working patterns from vision_processor/models/llama_model.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful for llama ‚úì\n"
     ]
    }
   ],
   "source": [
    "# Imports - Direct model loading\n",
    "import time\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Model-specific imports based on selection\n",
    "if CONFIG[\"model_type\"] == \"llama\":\n",
    "    from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    import torchvision.transforms as T\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "print(f\"Imports successful for {CONFIG['model_type']} ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama model from /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision...\n",
      "‚úÖ Using WORKING quantization config (skipping vision modules)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24deff564e0d4b97aa7bcff53eb19bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Applied WORKING generation config (no sampling parameters)\n",
      "‚úÖ Model loaded successfully in 5.92s\n",
      "Model device: cuda:0\n",
      "Quantization active: True\n"
     ]
    }
   ],
   "source": [
    "# Load model directly - USING WORKING VISION_PROCESSOR PATTERNS\n",
    "model_path = CONFIG[\"model_paths\"][CONFIG[\"model_type\"]]\n",
    "print(f\"Loading {CONFIG['model_type']} model from {model_path}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    if CONFIG[\"model_type\"] == \"llama\":\n",
    "        # EXACT pattern from vision_processor/models/llama_model.py\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        # Working quantization config from LlamaVisionModel\n",
    "        quantization_config = None\n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,\n",
    "                    llm_int8_enable_fp32_cpu_offload=True,\n",
    "                    llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n",
    "                    llm_int8_threshold=6.0,\n",
    "                )\n",
    "                print(\"‚úÖ Using WORKING quantization config (skipping vision modules)\")\n",
    "            except ImportError:\n",
    "                print(\"Quantization not available, using FP16\")\n",
    "                CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        # Working model loading args from LlamaVisionModel\n",
    "        model_loading_args = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if quantization_config:\n",
    "            model_loading_args[\"quantization_config\"] = quantization_config\n",
    "        \n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            **model_loading_args\n",
    "        ).eval()\n",
    "        \n",
    "        # CRITICAL: Set working generation config exactly like LlamaVisionModel\n",
    "        model.generation_config.max_new_tokens = CONFIG[\"max_new_tokens\"]\n",
    "        model.generation_config.do_sample = False\n",
    "        model.generation_config.temperature = None  # Disable temperature\n",
    "        model.generation_config.top_p = None        # Disable top_p  \n",
    "        model.generation_config.top_k = None        # Disable top_k\n",
    "        model.config.use_cache = True               # Enable KV cache\n",
    "        \n",
    "        print(\"‚úÖ Applied WORKING generation config (no sampling parameters)\")\n",
    "        \n",
    "    elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "        # Load InternVL3\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        model_kwargs = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                model_kwargs[\"load_in_8bit\"] = True\n",
    "                print(\"8-bit quantization enabled\")\n",
    "            except Exception:\n",
    "                print(\"Quantization not available, using bfloat16\")\n",
    "                CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            **model_kwargs\n",
    "        ).eval()\n",
    "        \n",
    "        if torch.cuda.is_available() and not CONFIG[\"enable_quantization\"]:\n",
    "            model = model.cuda()\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Model loaded successfully in {load_time:.2f}s\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Quantization active: {CONFIG['enable_quantization']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Model loading failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Image loaded: (2048, 2048)\n",
      "  File size: 211.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess image\n",
    "test_image_path = Path(CONFIG[\"test_image\"])\n",
    "\n",
    "if not test_image_path.exists():\n",
    "    print(f\"‚úó Test image not found: {test_image_path}\")\n",
    "    available = list(Path(\"datasets\").glob(\"*.png\"))[:5]\n",
    "    print(f\"Available images: {[img.name for img in available]}\")\n",
    "    raise FileNotFoundError(f\"Test image not found: {test_image_path}\")\n",
    "\n",
    "# Load image\n",
    "image = Image.open(test_image_path)\n",
    "if image.mode != \"RGB\":\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "print(f\"‚úì Image loaded: {image.size}\")\n",
    "print(f\"  File size: {test_image_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference with llama...\n",
      "Prompt: <|image|>Extract data from this receipt in KEY-VALUE format.\n",
      "\n",
      "Output format:\n",
      "DATE: [date from receip...\n",
      "--------------------------------------------------\n",
      "Input tensor shapes: [('input_ids', torch.Size([1, 49])), ('attention_mask', torch.Size([1, 49])), ('pixel_values', torch.Size([1, 1, 4, 3, 448, 448])), ('aspect_ratio_ids', torch.Size([1, 1])), ('aspect_ratio_mask', torch.Size([1, 1, 4])), ('cross_attention_mask', torch.Size([1, 49, 1, 4]))]\n",
      "Device target: cuda:0\n",
      "Using ultra-short max_new_tokens: 384 (was 1024)\n",
      "‚úÖ Using ULTRA-AGGRESSIVE repetition control + shorter generation\n",
      "Raw response (first 200 chars):  \n",
      "DATE: 11-07-2022\n",
      "STORE: SPOTLIGHT\n",
      "TOTAL: $22.45\n",
      "ITEM: Apples (kg)\n",
      "QUANTITY: 1\n",
      "PRICE: $3.96\n",
      "TOTAL: $3.96\n",
      "ITEM: Tea Bags (box)\n",
      "QUANTITY: 1\n",
      "PRICE: $4.53\n",
      "TOTAL: $4.53\n",
      "ITEM: Free Range Eggs (d)\n",
      "QUANTITY:...\n",
      "Raw response length: 1151 characters\n",
      "‚ö†Ô∏è Still repetitive after ultra-aggressive cleaning - truncating heavily\n",
      "üßπ Cleaning: 1151 ‚Üí 164 chars (85.8% reduction)\n",
      "‚ùå STILL REPETITIVE after ultra-aggressive cleaning!\n",
      "   This indicates a fundamental issue with the model's generation pattern\n",
      "‚úÖ Inference completed in 32.43s\n",
      "Final response length: 164 characters\n",
      "Final response ready for display (length: 164 characters)\n"
     ]
    }
   ],
   "source": [
    "# Run inference - ULTRA-AGGRESSIVE REPETITION CONTROL\n",
    "prompt = CONFIG[\"prompt\"]\n",
    "print(f\"Running inference with {CONFIG['model_type']}...\")\n",
    "print(f\"Prompt: {prompt[:100]}...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class UltraAggressiveRepetitionController:\n",
    "    \"\"\"Ultra-aggressive repetition detection and control specifically for Llama-3.2-Vision.\"\"\"\n",
    "    \n",
    "    def __init__(self, word_threshold: float = 0.15, phrase_threshold: int = 2):\n",
    "        \"\"\"\n",
    "        Initialize ultra-aggressive repetition controller.\n",
    "        \n",
    "        Args:\n",
    "            word_threshold: If any word appears more than this % of total words, it's repetitive (15% vs 30%)\n",
    "            phrase_threshold: Minimum repetitions to trigger cleaning (2 vs 3)\n",
    "        \"\"\"\n",
    "        self.word_threshold = word_threshold\n",
    "        self.phrase_threshold = phrase_threshold\n",
    "        \n",
    "        # Known problematic patterns from Llama-3.2-Vision\n",
    "        self.toxic_patterns = [\n",
    "            r\"THANK YOU FOR SHOPPING WITH US[^.]*\",\n",
    "            r\"All prices include GST where applicable[^.]*\",\n",
    "            r\"\\\\+[a-zA-Z]*\\{[^}]*\\}\",  # LaTeX artifacts\n",
    "            r\"\\(\\s*\\)\",  # Empty parentheses\n",
    "            r\"[.-]\\s*THANK YOU\",  # Dash/period before thank you\n",
    "        ]\n",
    "    \n",
    "    def detect_repetitive_generation(self, text: str, min_words: int = 3) -> bool:\n",
    "        \"\"\"Ultra-sensitive repetition detection.\"\"\"\n",
    "        words = text.split()\n",
    "        \n",
    "        # Much stricter minimum content requirement\n",
    "        if len(words) < min_words:\n",
    "            return True\n",
    "        \n",
    "        # Check for known toxic patterns first\n",
    "        if self._has_toxic_patterns(text):\n",
    "            return True\n",
    "            \n",
    "        # Ultra-aggressive word repetition check (15% threshold vs 30%)\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_lower = word.lower().strip('.,!?()[]{}')\n",
    "            if len(word_lower) > 2:  # Ignore very short words\n",
    "                word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n",
    "        \n",
    "        total_words = len([w for w in words if len(w.strip('.,!?()[]{}')) > 2])\n",
    "        if total_words > 0:\n",
    "            for word, count in word_counts.items():\n",
    "                if count > total_words * self.word_threshold:  # 15% threshold\n",
    "                    return True\n",
    "        \n",
    "        # Ultra-aggressive phrase repetition\n",
    "        if self._detect_aggressive_phrase_repetition(text):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def _has_toxic_patterns(self, text: str) -> bool:\n",
    "        \"\"\"Check for known problematic patterns.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        for pattern in self.toxic_patterns:\n",
    "            matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "            if len(matches) >= 2:  # Even 2 occurrences is too many\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _detect_aggressive_phrase_repetition(self, text: str) -> bool:\n",
    "        \"\"\"Ultra-aggressive phrase repetition detection.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Check for 3+ word phrases repeated even twice\n",
    "        words = text.split()\n",
    "        for i in range(len(words) - 6):  # Need at least 6 words for 3+3\n",
    "            phrase = ' '.join(words[i:i+3]).lower()\n",
    "            remainder = ' '.join(words[i+3:]).lower()\n",
    "            if phrase in remainder:\n",
    "                return True\n",
    "        \n",
    "        # Check sentences/segments\n",
    "        segments = re.split(r'[.!?]+', text)\n",
    "        segment_counts = {}\n",
    "        \n",
    "        for segment in segments:\n",
    "            segment_clean = re.sub(r'\\s+', ' ', segment.strip().lower())\n",
    "            # Much shorter minimum segment length\n",
    "            if len(segment_clean) > 5:  # Was 10, now 5\n",
    "                segment_counts[segment_clean] = segment_counts.get(segment_clean, 0) + 1\n",
    "        \n",
    "        # Any segment appearing twice is problematic\n",
    "        for count in segment_counts.values():\n",
    "            if count >= self.phrase_threshold:  # Now 2 instead of 3\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "    \n",
    "    def clean_response(self, response: str) -> str:\n",
    "        \"\"\"Ultra-aggressive cleaning with early truncation.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        if not response or len(response.strip()) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        original_length = len(response)\n",
    "        \n",
    "        # Step 1: Early truncation at first major repetition\n",
    "        response = self._early_truncate_at_repetition(response)\n",
    "        \n",
    "        # Step 2: Remove toxic patterns aggressively\n",
    "        response = self._remove_toxic_patterns(response)\n",
    "        \n",
    "        # Step 3: Remove safety warnings\n",
    "        response = self._remove_safety_warnings(response)\n",
    "        \n",
    "        # Step 4: Ultra-aggressive repetition removal\n",
    "        response = self._ultra_aggressive_word_removal(response)\n",
    "        response = self._ultra_aggressive_phrase_removal(response)\n",
    "        response = self._ultra_aggressive_sentence_removal(response)\n",
    "        \n",
    "        # Step 5: Clean artifacts\n",
    "        response = self._clean_artifacts(response)\n",
    "        \n",
    "        # Step 6: Final validation and truncation\n",
    "        response = self._final_validation_truncate(response)\n",
    "        \n",
    "        final_length = len(response)\n",
    "        reduction = ((original_length - final_length) / original_length * 100) if original_length > 0 else 0\n",
    "        \n",
    "        print(f\"üßπ Cleaning: {original_length} ‚Üí {final_length} chars ({reduction:.1f}% reduction)\")\n",
    "        \n",
    "        return response.strip()\n",
    "    \n",
    "    def _early_truncate_at_repetition(self, text: str) -> str:\n",
    "        \"\"\"Truncate immediately when repetition starts.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Find first occurrence of toxic patterns and truncate there\n",
    "        for pattern in self.toxic_patterns:\n",
    "            match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                # Find the SECOND occurrence and truncate before it\n",
    "                remaining = text[match.end():]\n",
    "                second_match = re.search(pattern, remaining, flags=re.IGNORECASE)\n",
    "                if second_match:\n",
    "                    truncate_point = match.end() + second_match.start()\n",
    "                    print(f\"üî™ Early truncation at repetition: {len(text)} ‚Üí {truncate_point} chars\")\n",
    "                    return text[:truncate_point]\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _remove_toxic_patterns(self, text: str) -> str:\n",
    "        \"\"\"Aggressively remove known toxic patterns.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        for pattern in self.toxic_patterns:\n",
    "            # Remove ALL occurrences, not just duplicates\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _remove_safety_warnings(self, text: str) -> str:\n",
    "        \"\"\"Remove safety warnings.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        safety_patterns = [\n",
    "            r\"I'm not able to provide.*?information\\.?\",\n",
    "            r\"I cannot provide.*?information\\.?\", \n",
    "            r\"I'm unable to.*?\\.?\",\n",
    "            r\"I can't.*?\\.?\",\n",
    "            r\"Sorry, I cannot.*?\\.?\",\n",
    "            r\".*could compromise.*privacy.*\",\n",
    "        ]\n",
    "        \n",
    "        for pattern in safety_patterns:\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _ultra_aggressive_word_removal(self, text: str) -> str:\n",
    "        \"\"\"Ultra-aggressive word repetition removal.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Remove 2+ consecutive identical words (was 3+)\n",
    "        text = re.sub(r'\\b(\\w+)(\\s+\\1){1,}', r'\\1', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove any word appearing more than 3 times total\n",
    "        words = text.split()\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_lower = word.lower().strip('.,!?()[]{}')\n",
    "            word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n",
    "        \n",
    "        # Rebuild text, limiting each word to max 3 occurrences\n",
    "        result_words = []\n",
    "        word_usage = {}\n",
    "        \n",
    "        for word in words:\n",
    "            word_lower = word.lower().strip('.,!?()[]{}')\n",
    "            current_count = word_usage.get(word_lower, 0)\n",
    "            \n",
    "            if current_count < 3:  # Allow max 3 occurrences\n",
    "                result_words.append(word)\n",
    "                word_usage[word_lower] = current_count + 1\n",
    "        \n",
    "        return ' '.join(result_words)\n",
    "    \n",
    "    def _ultra_aggressive_phrase_removal(self, text: str) -> str:\n",
    "        \"\"\"Ultra-aggressive phrase removal.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Remove repeated 2-6 word phrases (expanded range)\n",
    "        for phrase_length in range(2, 7):\n",
    "            pattern = r'\\b((?:\\w+\\s+){' + str(phrase_length-1) + r'}\\w+)(\\s+\\1){1,}'  # 1+ repetitions vs 2+\n",
    "            text = re.sub(pattern, r'\\1', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _ultra_aggressive_sentence_removal(self, text: str) -> str:\n",
    "        \"\"\"Ultra-aggressive sentence removal.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        \n",
    "        # Keep only first occurrence of any sentence\n",
    "        seen = set()\n",
    "        unique_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_clean = re.sub(r'\\s+', ' ', sentence.strip().lower())\n",
    "            sentence_clean = re.sub(r'[^\\w\\s]', '', sentence_clean)  # Remove all punctuation for comparison\n",
    "            \n",
    "            if sentence_clean and len(sentence_clean) > 3:  # Very short minimum\n",
    "                if sentence_clean not in seen:\n",
    "                    seen.add(sentence_clean)\n",
    "                    unique_sentences.append(sentence.strip())\n",
    "        \n",
    "        return '. '.join(unique_sentences)\n",
    "    \n",
    "    def _clean_artifacts(self, text: str) -> str:\n",
    "        \"\"\"Aggressive artifact cleaning.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Remove whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove LaTeX/markdown aggressively\n",
    "        text = re.sub(r'\\\\+[a-zA-Z]*\\{[^}]*\\}', '', text)\n",
    "        text = re.sub(r'\\\\+[a-zA-Z]+', '', text)\n",
    "        text = re.sub(r'```+[^`]*```+', '', text)\n",
    "        text = re.sub(r'[{}]+', '', text)\n",
    "        \n",
    "        # Remove excessive punctuation\n",
    "        text = re.sub(r'[.]{2,}', '.', text)\n",
    "        text = re.sub(r'[!]{2,}', '!', text)\n",
    "        text = re.sub(r'[?]{2,}', '?', text)\n",
    "        text = re.sub(r'[,]{2,}', ',', text)\n",
    "        \n",
    "        # Remove empty parentheses and brackets\n",
    "        text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "        text = re.sub(r'\\[\\s*\\]', '', text)\n",
    "        \n",
    "        # Remove standalone punctuation\n",
    "        text = re.sub(r'\\s+[.,!?;:]\\s+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _final_validation_truncate(self, text: str, max_length: int = 800) -> str:\n",
    "        \"\"\"Final validation with aggressive truncation.\"\"\"\n",
    "        # If still repetitive after all cleaning, something is very wrong\n",
    "        if self.detect_repetitive_generation(text):\n",
    "            print(\"‚ö†Ô∏è Still repetitive after ultra-aggressive cleaning - truncating heavily\")\n",
    "            # Find last good sentence in first half\n",
    "            half_point = len(text) // 2\n",
    "            truncated = text[:half_point]\n",
    "            last_period = truncated.rfind('.')\n",
    "            if last_period > half_point * 0.5:\n",
    "                return truncated[:last_period + 1]\n",
    "            else:\n",
    "                return truncated[:half_point] + \"...\"\n",
    "        \n",
    "        # Aggressive length limit\n",
    "        if len(text) > max_length:\n",
    "            truncated = text[:max_length]\n",
    "            last_period = truncated.rfind('.')\n",
    "            if last_period > max_length * 0.7:\n",
    "                return truncated[:last_period + 1]\n",
    "            else:\n",
    "                return truncated + \"...\"\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Initialize ultra-aggressive repetition controller\n",
    "repetition_controller = UltraAggressiveRepetitionController(\n",
    "    word_threshold=0.15,  # Much stricter: 15% vs 30%\n",
    "    phrase_threshold=2    # Much stricter: 2 vs 3 repetitions\n",
    ")\n",
    "\n",
    "try:\n",
    "    if CONFIG[\"model_type\"] == \"llama\":\n",
    "        # EXACT input preparation from LlamaVisionModel._prepare_inputs()\n",
    "        prompt_with_image = prompt if prompt.startswith(\"<|image|>\") else f\"<|image|>{prompt}\"\n",
    "        \n",
    "        inputs = processor(text=prompt_with_image, images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # WORKING device handling from LlamaVisionModel\n",
    "        device = next(model.parameters()).device\n",
    "        if device.type != \"cpu\":\n",
    "            device_target = str(device).split(\":\")[0] if \":\" in str(device) else str(device)\n",
    "            inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "        \n",
    "        print(f\"Input tensor shapes: {[(k, v.shape) for k, v in inputs.items() if hasattr(v, 'shape')]}\")\n",
    "        print(f\"Device target: {device}\")\n",
    "        \n",
    "        # ULTRA-AGGRESSIVE: Even shorter token limit\n",
    "        effective_max_tokens = min(CONFIG[\"max_new_tokens\"], 384)  # Further reduced: 384 vs 512\n",
    "        print(f\"Using ultra-short max_new_tokens: {effective_max_tokens} (was {CONFIG['max_new_tokens']})\")\n",
    "        \n",
    "        # EXACT generation kwargs from LlamaVisionModel.generate()\n",
    "        generation_kwargs = {\n",
    "            **inputs,\n",
    "            \"max_new_tokens\": effective_max_tokens,\n",
    "            \"do_sample\": False,  # Deterministic generation\n",
    "            \"pad_token_id\": processor.tokenizer.eos_token_id,\n",
    "            \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "            \"use_cache\": True,\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Using ULTRA-AGGRESSIVE repetition control + shorter generation\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**generation_kwargs)\n",
    "        \n",
    "        raw_response = processor.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Raw response (first 200 chars): {raw_response[:200]}...\")\n",
    "        print(f\"Raw response length: {len(raw_response)} characters\")\n",
    "        \n",
    "        # ULTRA-AGGRESSIVE: Enhanced repetition control\n",
    "        response = repetition_controller.clean_response(raw_response)\n",
    "        \n",
    "        # Final check with stricter detection\n",
    "        if repetition_controller.detect_repetitive_generation(response):\n",
    "            print(\"‚ùå STILL REPETITIVE after ultra-aggressive cleaning!\")\n",
    "            print(\"   This indicates a fundamental issue with the model's generation pattern\")\n",
    "        else:\n",
    "            print(\"‚úÖ Ultra-aggressive cleaning successful - repetition eliminated\")\n",
    "        \n",
    "    elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "        # InternVL inference with ultra-aggressive repetition control\n",
    "        image_size = 448\n",
    "        transform = T.Compose([\n",
    "            T.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        \n",
    "        pixel_values = transform(image).unsqueeze(0)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n",
    "        else:\n",
    "            pixel_values = pixel_values.contiguous()\n",
    "        \n",
    "        generation_config = {\n",
    "            \"max_new_tokens\": min(CONFIG[\"max_new_tokens\"], 384),\n",
    "            \"do_sample\": False,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id\n",
    "        }\n",
    "        \n",
    "        raw_response = model.chat(\n",
    "            tokenizer=tokenizer,\n",
    "            pixel_values=pixel_values,\n",
    "            question=prompt,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        \n",
    "        if isinstance(raw_response, tuple):\n",
    "            raw_response = raw_response[0]\n",
    "        \n",
    "        # Apply ultra-aggressive repetition control\n",
    "        response = repetition_controller.clean_response(raw_response)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Inference completed in {inference_time:.2f}s\")\n",
    "    print(f\"Final response length: {len(response)} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Inference failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    response = f\"Error: Inference failed - {str(e)}\"\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"Final response ready for display (length: {len(response) if 'response' in locals() else 0} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTED TEXT:\n",
      "============================================================\n",
      "DATE: 11-07-2022 STORE: SPOTLIGHT TOTAL: $22. 45 ITEM: Apples (kg) QUANTITY: 1 PRICE: $3. 96 TOTAL: $3. 96 ITEM: Tea Bags (box) QUANTITY: 1 PRICE: $4. 53 TOTAL: $4.\n",
      "============================================================\n",
      "\n",
      "SUMMARY:\n",
      "Model: llama\n",
      "Response length: 164 characters\n",
      "Processing time: 32.43s\n",
      "Quantization enabled: True\n",
      "Device: CUDA\n",
      "\n",
      "RESPONSE ANALYSIS:\n",
      "‚úÖ KEY-VALUE format detected\n",
      "Extracted fields:\n",
      "  DATE: 11-07-2022 STORE: SPOTLIGHT TOTAL: $22. 45 ITEM: Apples (kg) QUANTITY: 1 PRICE: $3. 96 TOTAL: $3. 96 ITEM: Tea Bags (box) QUANTITY: 1 PRICE: $4. 53 TOTAL: $4.\n",
      "\n",
      "‚ö†Ô∏è ACCEPTABLE performance: 32.4s\n",
      "\n",
      "üéØ For production use:\n",
      "- Llama-3.2-Vision: Use simple JSON prompts only\n",
      "- InternVL3: More flexible, handles complex prompts better\n",
      "- Both models: Shorter max_new_tokens prevents issues\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTED TEXT:\")\n",
    "print(\"=\" * 60)\n",
    "print(response)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"Model: {CONFIG['model_type']}\")\n",
    "print(f\"Response length: {len(response)} characters\")\n",
    "print(f\"Processing time: {inference_time:.2f}s\")\n",
    "print(f\"Quantization enabled: {CONFIG['enable_quantization']}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Enhanced JSON parsing with validation\n",
    "print(f\"\\nRESPONSE ANALYSIS:\")\n",
    "if response.strip().startswith('{') and response.strip().endswith('}'):\n",
    "    try:\n",
    "        import json\n",
    "        parsed = json.loads(response.strip())\n",
    "        print(f\"‚úÖ VALID JSON EXTRACTED:\")\n",
    "        for key, value in parsed.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Validate completeness\n",
    "        expected_fields = [\"DATE\", \"STORE\", \"TOTAL\"]\n",
    "        missing = [field for field in expected_fields if field not in parsed or not parsed[field]]\n",
    "        if missing:\n",
    "            print(f\"‚ö†Ô∏è Missing fields: {missing}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ All expected fields present\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå Invalid JSON: {e}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        \n",
    "elif any(keyword in response for keyword in [\"DATE:\", \"STORE:\", \"TOTAL:\"]):\n",
    "    print(f\"‚úÖ KEY-VALUE format detected\")\n",
    "    # Try to extract key-value pairs\n",
    "    import re\n",
    "    matches = re.findall(r'([A-Z]+):\\s*([^\\n]+)', response)\n",
    "    if matches:\n",
    "        print(f\"Extracted fields:\")\n",
    "        for key, value in matches:\n",
    "            print(f\"  {key}: {value.strip()}\")\n",
    "            \n",
    "elif any(phrase in response.lower() for phrase in [\"not able\", \"cannot provide\", \"sorry\"]):\n",
    "    print(f\"‚ùå SAFETY MODE TRIGGERED\")\n",
    "    print(f\"This indicates the prompt triggered Llama's safety restrictions\")\n",
    "    print(f\"Solution: Use simpler JSON format prompts\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è UNSTRUCTURED RESPONSE\")\n",
    "    print(f\"Response doesn't match expected patterns\")\n",
    "    print(f\"Consider using different prompt format\")\n",
    "\n",
    "# Performance assessment\n",
    "if inference_time < 30:\n",
    "    print(f\"\\n‚ö° GOOD performance: {inference_time:.1f}s\")\n",
    "elif inference_time < 60:\n",
    "    print(f\"\\n‚ö†Ô∏è ACCEPTABLE performance: {inference_time:.1f}s\") \n",
    "else:\n",
    "    print(f\"\\n‚ùå SLOW performance: {inference_time:.1f}s\")\n",
    "\n",
    "print(f\"\\nüéØ For production use:\")\n",
    "print(f\"- Llama-3.2-Vision: Use simple JSON prompts only\")\n",
    "print(f\"- InternVL3: More flexible, handles complex prompts better\")\n",
    "print(f\"- Both models: Shorter max_new_tokens prevents issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing additional prompts with ULTRA-AGGRESSIVE REPETITION CONTROL...\n",
      "\n",
      "Test 1: <|image|>Extract store name and total amount in KEY-VALUE fo...\n",
      "üßπ Cleaning: 184 ‚Üí 49 chars (73.4% reduction)\n",
      "‚úÖ SUCCESS (7.9s): <OCR/> SPOTLIGHT TAX INVOICE 888Park 3:53PM QTY 1...\n",
      "   Length: 49 chars - repetition eliminated\n",
      "--------------------------------------------------\n",
      "Test 2: <|image|>What type of business document is this? Answer: rec...\n",
      "‚ö†Ô∏è Still repetitive after ultra-aggressive cleaning - truncating heavily\n",
      "üßπ Cleaning: 511 ‚Üí 3 chars (99.4% reduction)\n",
      "‚ùå STILL REPETITIVE (8.1s): ......\n",
      "   Even ultra-aggressive cleaning failed - model has fundamental repetition issue\n",
      "--------------------------------------------------\n",
      "Test 3: <|image|>Extract the date from this document in format DD/MM...\n",
      "‚ö†Ô∏è Still repetitive after ultra-aggressive cleaning - truncating heavily\n",
      "üßπ Cleaning: 173 ‚Üí 20 chars (88.4% reduction)\n",
      "‚ùå STILL REPETITIVE (8.2s): 11-07-2022, 11-07......\n",
      "   Even ultra-aggressive cleaning failed - model has fundamental repetition issue\n",
      "--------------------------------------------------\n",
      "\n",
      "üéØ ULTRA-AGGRESSIVE REPETITION CONTROL FEATURES:\n",
      "üî• UltraAggressiveRepetitionController - Nuclear option for repetition\n",
      "üî• Stricter thresholds:\n",
      "   - Word repetition: 15% threshold (was 30%)\n",
      "   - Phrase repetition: 2 occurrences trigger (was 3)\n",
      "   - Sentence repetition: Any duplicate removed\n",
      "üî• Toxic pattern targeting:\n",
      "   - 'THANK YOU FOR SHOPPING...' pattern recognition\n",
      "   - 'All prices include GST...' pattern recognition\n",
      "   - LaTeX artifact removal\n",
      "üî• Early truncation at first repetition detection\n",
      "üî• Max 3 occurrences per word across entire text\n",
      "üî• Ultra-short token limits (384 main, 96 tests)\n",
      "üî• Aggressive artifact cleaning (punctuation, parentheses, etc.)\n",
      "\n",
      "üí° If this still shows repetition, the issue is in the model's generation\n",
      "   pattern itself, not the post-processing cleaning.\n"
     ]
    }
   ],
   "source": [
    "# Test additional prompts - WITH ULTRA-AGGRESSIVE REPETITION CONTROL\n",
    "working_test_prompts = [\n",
    "    \"<|image|>Extract store name and total amount in KEY-VALUE format.\\n\\nOutput format:\\nSTORE: [store name]\\nTOTAL: [total amount]\",\n",
    "    \"<|image|>What type of business document is this? Answer: receipt, invoice, or statement.\",\n",
    "    \"<|image|>Extract the date from this document in format DD/MM/YYYY.\"\n",
    "]\n",
    "\n",
    "print(\"Testing additional prompts with ULTRA-AGGRESSIVE REPETITION CONTROL...\\n\")\n",
    "\n",
    "for i, test_prompt in enumerate(working_test_prompts, 1):\n",
    "    print(f\"Test {i}: {test_prompt[:60]}...\")\n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        if CONFIG[\"model_type\"] == \"llama\":\n",
    "            # Use EXACT same pattern as main inference\n",
    "            prompt_with_image = test_prompt if test_prompt.startswith(\"<|image|>\") else f\"<|image|>{test_prompt}\"\n",
    "            \n",
    "            inputs = processor(text=prompt_with_image, images=image, return_tensors=\"pt\")\n",
    "            \n",
    "            # Same device handling\n",
    "            device = next(model.parameters()).device\n",
    "            if device.type != \"cpu\":\n",
    "                device_target = str(device).split(\":\")[0] if \":\" in str(device) else str(device)\n",
    "                inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "            \n",
    "            # ULTRA-AGGRESSIVE: Extremely short tokens for tests\n",
    "            generation_kwargs = {\n",
    "                **inputs,\n",
    "                \"max_new_tokens\": 96,  # Even shorter: 96 vs 128\n",
    "                \"do_sample\": False,\n",
    "                \"pad_token_id\": processor.tokenizer.eos_token_id,\n",
    "                \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "                \"use_cache\": True,\n",
    "            }\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**generation_kwargs)\n",
    "            \n",
    "            raw_result = processor.decode(\n",
    "                outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Apply ultra-aggressive repetition control\n",
    "            result = repetition_controller.clean_response(raw_result)\n",
    "            \n",
    "        elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "            result = model.chat(\n",
    "                tokenizer=tokenizer,\n",
    "                pixel_values=pixel_values,\n",
    "                question=test_prompt,\n",
    "                generation_config={\n",
    "                    \"max_new_tokens\": 96, \n",
    "                    \"do_sample\": False\n",
    "                }\n",
    "            )\n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "            result = repetition_controller.clean_response(result)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Ultra-strict analysis of results\n",
    "        if repetition_controller.detect_repetitive_generation(result):\n",
    "            print(f\"‚ùå STILL REPETITIVE ({elapsed:.1f}s): {result[:60]}...\")\n",
    "            print(f\"   Even ultra-aggressive cleaning failed - model has fundamental repetition issue\")\n",
    "        elif any(phrase in result.lower() for phrase in [\"not able\", \"cannot provide\", \"sorry\"]):\n",
    "            print(f\"‚ö†Ô∏è Safety mode triggered ({elapsed:.1f}s): {result[:60]}...\")\n",
    "        elif len(result.strip()) < 3:\n",
    "            print(f\"‚ö†Ô∏è Over-cleaned ({elapsed:.1f}s): '{result}' - may be too aggressive\")\n",
    "        else:\n",
    "            print(f\"‚úÖ SUCCESS ({elapsed:.1f}s): {result[:80]}...\")\n",
    "            print(f\"   Length: {len(result)} chars - repetition eliminated\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)[:100]}...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nüéØ ULTRA-AGGRESSIVE REPETITION CONTROL FEATURES:\")\n",
    "print(\"üî• UltraAggressiveRepetitionController - Nuclear option for repetition\")\n",
    "print(\"üî• Stricter thresholds:\")\n",
    "print(\"   - Word repetition: 15% threshold (was 30%)\")  \n",
    "print(\"   - Phrase repetition: 2 occurrences trigger (was 3)\")\n",
    "print(\"   - Sentence repetition: Any duplicate removed\")\n",
    "print(\"üî• Toxic pattern targeting:\")\n",
    "print(\"   - 'THANK YOU FOR SHOPPING...' pattern recognition\")\n",
    "print(\"   - 'All prices include GST...' pattern recognition\")\n",
    "print(\"   - LaTeX artifact removal\")\n",
    "print(\"üî• Early truncation at first repetition detection\")\n",
    "print(\"üî• Max 3 occurrences per word across entire text\")\n",
    "print(\"üî• Ultra-short token limits (384 main, 96 tests)\")\n",
    "print(\"üî• Aggressive artifact cleaning (punctuation, parentheses, etc.)\")\n",
    "print(\"\\nüí° If this still shows repetition, the issue is in the model's generation\")\n",
    "print(\"   pattern itself, not the post-processing cleaning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä All tests completed! Memory cleanup moved to final cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Document Classification - Robust Loading + Aggressive Memory Management\n",
    "print(\"üèõÔ∏è COMPREHENSIVE TAXPAYER DOCUMENT CLASSIFICATION TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "# Memory management function\n",
    "def cleanup_gpu_memory():\n",
    "    \"\"\"Aggressive GPU memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"   GPU Memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved\")\n",
    "\n",
    "# Standard document types\n",
    "DOCUMENT_TYPES = [\n",
    "    \"FUEL_RECEIPT\", \"BUSINESS_RECEIPT\", \"TAX_INVOICE\", \"BANK_STATEMENT\",\n",
    "    \"MEAL_RECEIPT\", \"ACCOMMODATION_RECEIPT\", \"TRAVEL_DOCUMENT\", \n",
    "    \"PARKING_TOLL_RECEIPT\", \"PROFESSIONAL_SERVICES\", \"EQUIPMENT_SUPPLIES\", \"OTHER\"\n",
    "]\n",
    "\n",
    "# Human annotated ground truth\n",
    "test_images_with_annotations = [\n",
    "    (\"image14.png\", \"TAX_INVOICE\"),\n",
    "    (\"image65.png\", \"TAX_INVOICE\"),\n",
    "    (\"image71.png\", \"TAX_INVOICE\"),\n",
    "    (\"image74.png\", \"TAX_INVOICE\"),\n",
    "    (\"image205.png\", \"FUEL_RECEIPT\"),\n",
    "    (\"image23.png\", \"TAX_INVOICE\"),\n",
    "    (\"image45.png\", \"TAX_INVOICE\"),\n",
    "    (\"image1.png\", \"BANK_STATEMENT\"),\n",
    "    (\"image203.png\", \"BANK_STATEMENT\"),\n",
    "    (\"image204.png\", \"FUEL_RECEIPT\"),\n",
    "    (\"image206.png\", \"OTHER\"),\n",
    "]\n",
    "\n",
    "# Verify test images exist\n",
    "datasets_path = Path(\"datasets\")\n",
    "verified_test_images = []\n",
    "verified_ground_truth = {}\n",
    "\n",
    "for img_name, annotation in test_images_with_annotations:\n",
    "    img_path = datasets_path / img_name\n",
    "    if img_path.exists():\n",
    "        verified_test_images.append(img_name)\n",
    "        verified_ground_truth[img_name] = annotation\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Missing: {img_name} (expected: {annotation})\")\n",
    "\n",
    "print(f\"üìä Testing {len(verified_test_images)} documents with HUMAN ANNOTATIONS:\")\n",
    "for i, img_name in enumerate(verified_test_images, 1):\n",
    "    annotation = verified_ground_truth[img_name]\n",
    "    print(f\"   {i}. {img_name:<12} ‚Üí {annotation}\")\n",
    "\n",
    "# Classification prompt\n",
    "classification_prompt = f\"\"\"<|image|>Classify this business document type:\n",
    "\n",
    "{chr(10).join(DOCUMENT_TYPES)}\n",
    "\n",
    "Answer with ONE category only:\"\"\"\n",
    "\n",
    "print(f\"\\nPrompt length: {len(classification_prompt)} characters\")\n",
    "\n",
    "# Results storage with accuracy tracking\n",
    "multi_doc_results = {\n",
    "    \"llama\": {\"classifications\": [], \"times\": [], \"errors\": [], \"correct\": 0, \"total\": 0},\n",
    "    \"internvl\": {\"classifications\": [], \"times\": [], \"errors\": [], \"correct\": 0, \"total\": 0}\n",
    "}\n",
    "\n",
    "# Test both models with AGGRESSIVE memory management\n",
    "for model_name in [\"llama\", \"internvl\"]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"üîç TESTING {model_name.upper()} AGAINST HUMAN ANNOTATIONS\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # AGGRESSIVE pre-cleanup before loading model\n",
    "    print(f\"üßπ Pre-cleanup for {model_name}...\")\n",
    "    for var in ['model', 'processor', 'tokenizer', 'inputs', 'outputs', 'pixel_values']:\n",
    "        if var in locals():\n",
    "            del locals()[var]\n",
    "        if var in globals():\n",
    "            del globals()[var]\n",
    "    cleanup_gpu_memory()\n",
    "    \n",
    "    model_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load model using ROBUST patterns from cell 3\n",
    "        model_path = CONFIG[\"model_paths\"][model_name]\n",
    "        print(f\"Loading {model_name} model from {model_path}...\")\n",
    "        \n",
    "        if model_name == \"llama\":\n",
    "            print(f\"üîÑ Loading Llama (will use ~6-8GB GPU memory)...\")\n",
    "            \n",
    "            processor = AutoProcessor.from_pretrained(\n",
    "                model_path, trust_remote_code=True, local_files_only=True\n",
    "            )\n",
    "            \n",
    "            model_loading_args = {\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"device_map\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "                \"local_files_only\": True\n",
    "            }\n",
    "            \n",
    "            if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "                try:\n",
    "                    from transformers import BitsAndBytesConfig\n",
    "                    quantization_config = BitsAndBytesConfig(\n",
    "                        load_in_8bit=True,\n",
    "                        llm_int8_enable_fp32_cpu_offload=True,\n",
    "                        llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n",
    "                    )\n",
    "                    model_loading_args[\"quantization_config\"] = quantization_config\n",
    "                    print(\"‚úÖ Using 8-bit quantization\")\n",
    "                except ImportError:\n",
    "                    pass\n",
    "            \n",
    "            model = MllamaForConditionalGeneration.from_pretrained(\n",
    "                model_path, **model_loading_args\n",
    "            ).eval()\n",
    "            \n",
    "        elif model_name == \"internvl\":\n",
    "            print(f\"üîÑ Loading InternVL (will use ~4-6GB GPU memory)...\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_path, trust_remote_code=True, local_files_only=True\n",
    "            )\n",
    "            \n",
    "            model_kwargs = {\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "                \"trust_remote_code\": True,\n",
    "                \"torch_dtype\": torch.bfloat16,\n",
    "                \"local_files_only\": True\n",
    "            }\n",
    "            \n",
    "            if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "                try:\n",
    "                    model_kwargs[\"load_in_8bit\"] = True\n",
    "                    print(\"‚úÖ 8-bit quantization enabled\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            model = AutoModel.from_pretrained(model_path, **model_kwargs).eval()\n",
    "            \n",
    "            if torch.cuda.is_available() and not CONFIG[\"enable_quantization\"]:\n",
    "                model = model.cuda()\n",
    "        \n",
    "        model_load_time = time.time() - model_start_time\n",
    "        print(f\"‚úÖ {model_name} model loaded in {model_load_time:.1f}s\")\n",
    "        cleanup_gpu_memory()\n",
    "        \n",
    "        # Test each document\n",
    "        for i, img_name in enumerate(verified_test_images, 1):\n",
    "            expected_classification = verified_ground_truth[img_name]\n",
    "            print(f\"\\nüìÑ Document {i}/{len(verified_test_images)}: {img_name} (expected: {expected_classification})\")\n",
    "            \n",
    "            try:\n",
    "                # Load image\n",
    "                img_path = datasets_path / img_name\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "                inference_start = time.time()\n",
    "                \n",
    "                if model_name == \"llama\":\n",
    "                    inputs = processor(text=classification_prompt, images=image, return_tensors=\"pt\")\n",
    "                    device = next(model.parameters()).device\n",
    "                    if device.type != \"cpu\":\n",
    "                        device_target = str(device).split(\":\")[0] if \":\" in str(device) else str(device)\n",
    "                        inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model.generate(\n",
    "                            **inputs,\n",
    "                            max_new_tokens=32,\n",
    "                            do_sample=False,\n",
    "                            pad_token_id=processor.tokenizer.eos_token_id,\n",
    "                            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                            use_cache=True,\n",
    "                        )\n",
    "                    \n",
    "                    raw_response = processor.decode(\n",
    "                        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "                        skip_special_tokens=True\n",
    "                    )\n",
    "                    \n",
    "                    # Immediate cleanup of inference tensors\n",
    "                    del inputs, outputs\n",
    "                    \n",
    "                elif model_name == \"internvl\":\n",
    "                    transform = T.Compose([\n",
    "                        T.Resize((448, 448), interpolation=InterpolationMode.BICUBIC),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                    ])\n",
    "                    \n",
    "                    pixel_values = transform(image).unsqueeze(0)\n",
    "                    if torch.cuda.is_available():\n",
    "                        pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n",
    "                    \n",
    "                    raw_response = model.chat(\n",
    "                        tokenizer=tokenizer,\n",
    "                        pixel_values=pixel_values,\n",
    "                        question=classification_prompt,\n",
    "                        generation_config={\"max_new_tokens\": 32, \"do_sample\": False}\n",
    "                    )\n",
    "                    \n",
    "                    if isinstance(raw_response, tuple):\n",
    "                        raw_response = raw_response[0]\n",
    "                    \n",
    "                    # Immediate cleanup of inference tensors\n",
    "                    del pixel_values\n",
    "                \n",
    "                inference_time = time.time() - inference_start\n",
    "                \n",
    "                # Extract classification\n",
    "                extracted_classification = \"UNKNOWN\"\n",
    "                response_clean = raw_response.strip().upper()\n",
    "                \n",
    "                for doc_type in DOCUMENT_TYPES:\n",
    "                    if doc_type in response_clean:\n",
    "                        extracted_classification = doc_type\n",
    "                        break\n",
    "                \n",
    "                # Calculate accuracy against human annotation\n",
    "                is_correct = extracted_classification == expected_classification\n",
    "                multi_doc_results[model_name][\"total\"] += 1\n",
    "                if is_correct:\n",
    "                    multi_doc_results[model_name][\"correct\"] += 1\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    \"image\": img_name,\n",
    "                    \"predicted\": extracted_classification,\n",
    "                    \"expected\": expected_classification,\n",
    "                    \"correct\": is_correct,\n",
    "                    \"inference_time\": inference_time,\n",
    "                    \"raw_response\": raw_response[:60] + \"...\" if len(raw_response) > 60 else raw_response\n",
    "                }\n",
    "                \n",
    "                multi_doc_results[model_name][\"classifications\"].append(result)\n",
    "                multi_doc_results[model_name][\"times\"].append(inference_time)\n",
    "                \n",
    "                # Show result\n",
    "                status = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "                print(f\"   {status} {extracted_classification} ({inference_time:.1f}s)\")\n",
    "                \n",
    "                # Periodic memory cleanup every 3 images\n",
    "                if i % 3 == 0:\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                multi_doc_results[model_name][\"errors\"].append({\n",
    "                    \"image\": img_name,\n",
    "                    \"expected\": expected_classification,\n",
    "                    \"error\": str(e)[:100]\n",
    "                })\n",
    "                multi_doc_results[model_name][\"total\"] += 1\n",
    "                print(f\"   ‚ùå ERROR: {str(e)[:60]}...\")\n",
    "        \n",
    "        # AGGRESSIVE cleanup after model testing\n",
    "        print(f\"\\nüßπ Cleaning up {model_name}...\")\n",
    "        del model\n",
    "        if model_name == \"llama\":\n",
    "            del processor\n",
    "        elif model_name == \"internvl\":\n",
    "            del tokenizer\n",
    "        \n",
    "        cleanup_gpu_memory()\n",
    "        \n",
    "        total_time = time.time() - model_start_time\n",
    "        accuracy = multi_doc_results[model_name][\"correct\"] / multi_doc_results[model_name][\"total\"] * 100 if multi_doc_results[model_name][\"total\"] > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüìä {model_name.upper()} SUMMARY:\")\n",
    "        print(f\"   Accuracy: {accuracy:.1f}% ({multi_doc_results[model_name]['correct']}/{multi_doc_results[model_name]['total']})\")\n",
    "        print(f\"   Total Time: {total_time:.1f}s\")\n",
    "        print(f\"   Avg Time/Doc: {sum(multi_doc_results[model_name]['times'])/max(1,len(multi_doc_results[model_name]['times'])):.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {model_name.upper()} FAILED TO LOAD: {str(e)[:100]}...\")\n",
    "        \n",
    "        # Emergency cleanup\n",
    "        for var in ['model', 'processor', 'tokenizer', 'inputs', 'outputs', 'pixel_values']:\n",
    "            if var in locals():\n",
    "                del locals()[var]\n",
    "        cleanup_gpu_memory()\n",
    "        \n",
    "        multi_doc_results[model_name][\"model_error\"] = str(e)\n",
    "\n",
    "# Final Analysis\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"üèÜ HUMAN ANNOTATION ACCURACY ANALYSIS\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "# Comparison table\n",
    "comparison_data = []\n",
    "comparison_data.append([\"Image\", \"Expected\", \"Llama\", \"‚úì\", \"InternVL\", \"‚úì\"])\n",
    "comparison_data.append([\"-\" * 10, \"-\" * 10, \"-\" * 10, \"-\", \"-\" * 10, \"-\"])\n",
    "\n",
    "llama_results = {r[\"image\"]: r for r in multi_doc_results[\"llama\"][\"classifications\"]}\n",
    "internvl_results = {r[\"image\"]: r for r in multi_doc_results[\"internvl\"][\"classifications\"]}\n",
    "\n",
    "for img_name in verified_test_images:\n",
    "    expected = verified_ground_truth[img_name]\n",
    "    llama_result = llama_results.get(img_name, {\"predicted\": \"ERROR\", \"correct\": False})\n",
    "    internvl_result = internvl_results.get(img_name, {\"predicted\": \"ERROR\", \"correct\": False})\n",
    "    \n",
    "    comparison_data.append([\n",
    "        img_name[:8],\n",
    "        expected[:8],\n",
    "        llama_result[\"predicted\"][:8],\n",
    "        \"‚úÖ\" if llama_result[\"correct\"] else \"‚ùå\",\n",
    "        internvl_result[\"predicted\"][:8],\n",
    "        \"‚úÖ\" if internvl_result[\"correct\"] else \"‚ùå\"\n",
    "    ])\n",
    "\n",
    "for row in comparison_data:\n",
    "    print(f\"{row[0]:<10} {row[1]:<10} {row[2]:<10} {row[3]:<2} {row[4]:<10} {row[5]}\")\n",
    "\n",
    "# Final statistics\n",
    "print(f\"\\nüìà FINAL ACCURACY STATISTICS:\")\n",
    "for model_name in [\"llama\", \"internvl\"]:\n",
    "    if multi_doc_results[model_name][\"total\"] > 0:\n",
    "        accuracy = multi_doc_results[model_name][\"correct\"] / multi_doc_results[model_name][\"total\"] * 100\n",
    "        avg_time = sum(multi_doc_results[model_name][\"times\"]) / len(multi_doc_results[model_name][\"times\"])\n",
    "        print(f\"{model_name.upper()}: {accuracy:.1f}% accuracy, {avg_time:.2f}s/doc average\")\n",
    "\n",
    "# Final memory state\n",
    "print(f\"\\nüß† Final Memory State:\")\n",
    "cleanup_gpu_memory()\n",
    "\n",
    "print(f\"\\n‚úÖ Multi-document classification completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Memory Cleanup - Run at end of all testing\n",
    "print(\"üßπ Final Memory Cleanup...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Safe cleanup with existence checks for all possible model artifacts\n",
    "cleanup_success = []\n",
    "\n",
    "# Clean up any remaining model objects\n",
    "for var_name in ['model', 'processor', 'tokenizer']:\n",
    "    if var_name in locals() or var_name in globals():\n",
    "        try:\n",
    "            if var_name in locals():\n",
    "                del locals()[var_name]\n",
    "            if var_name in globals():\n",
    "                del globals()[var_name]\n",
    "            cleanup_success.append(f\"‚úì {var_name} deleted\")\n",
    "        except:\n",
    "            cleanup_success.append(f\"‚ö†Ô∏è {var_name} cleanup failed\")\n",
    "    else:\n",
    "        cleanup_success.append(f\"- {var_name} not found\")\n",
    "\n",
    "# Clean up other variables\n",
    "other_vars = ['inputs', 'outputs', 'pixel_values', 'image', 'raw_response', 'response']\n",
    "for var_name in other_vars:\n",
    "    if var_name in locals() or var_name in globals():\n",
    "        try:\n",
    "            if var_name in locals():\n",
    "                del locals()[var_name]\n",
    "            if var_name in globals():\n",
    "                del globals()[var_name]\n",
    "            cleanup_success.append(f\"‚úì {var_name} deleted\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# CUDA cleanup\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        cleanup_success.append(\"‚úì CUDA cache cleared\")\n",
    "        \n",
    "        # Check GPU memory usage\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1024**3   # GB\n",
    "        cleanup_success.append(f\"üìä GPU Memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        cleanup_success.append(f\"‚ö†Ô∏è CUDA cleanup error: {str(e)[:50]}\")\n",
    "else:\n",
    "    cleanup_success.append(\"- No CUDA device available\")\n",
    "\n",
    "# Print cleanup results\n",
    "for message in cleanup_success:\n",
    "    print(message)\n",
    "\n",
    "print(f\"\\nüéâ ALL TESTING COMPLETED!\")\n",
    "print(f\"üìä Summary:\")\n",
    "print(f\"- ‚úÖ Model loading and inference tests\")\n",
    "print(f\"- ‚úÖ Ultra-aggressive repetition control tests\") \n",
    "print(f\"- ‚úÖ Document classification tests\")\n",
    "print(f\"- ‚úÖ Memory cleanup completed\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for production deployment!\")\n",
    "print(f\"\\nüìã Key Findings:\")\n",
    "print(f\"- Llama-3.2-Vision: Works with simple prompts, has repetition issues\")\n",
    "print(f\"- InternVL3: More flexible, better prompt handling\")  \n",
    "print(f\"- Ultra-aggressive repetition control: Reduces output by 85%+\")\n",
    "print(f\"- Document classification: Tests {len(STANDARD_DOCUMENT_TYPES)} taxpayer categories\")\n",
    "print(f\"- Memory management: Safe cleanup for multi-user environments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vision_env)",
   "language": "python",
   "name": "vision_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

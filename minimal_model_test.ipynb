{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Vision Model Test\n",
    "\n",
    "Direct model loading and testing without using the unified_vision_processor package.\n",
    "\n",
    "All configuration is embedded in the notebook for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Model: llama (using WORKING vision_processor patterns)\n",
      "Image: datasets/image14.png\n",
      "Prompt: <|image|>Extract data from this receipt in KEY-VALUE format.\n",
      "\n",
      "Output format:\n",
      "DATE: [date from receip...\n",
      "\n",
      "‚úÖ Using PROVEN working patterns from vision_processor/models/llama_model.py\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Modify as needed\n",
    "CONFIG = {\n",
    "    # Model selection: \"llama\" or \"internvl\"\n",
    "    \"model_type\": \"llama\",  # BACK TO LLAMA with working code patterns\n",
    "    \n",
    "    # Model paths\n",
    "    \"model_paths\": {\n",
    "        \"llama\": \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision\",\n",
    "        \"internvl\": \"/home/jovyan/nfs_share/models/InternVL3-8B\"\n",
    "    },\n",
    "    \n",
    "    # Test image path\n",
    "    \"test_image\": \"datasets/image14.png\",\n",
    "    \n",
    "    # WORKING prompt pattern from vision_processor (KEY-VALUE format)\n",
    "    \"prompt\": \"<|image|>Extract data from this receipt in KEY-VALUE format.\\n\\nOutput format:\\nDATE: [date from receipt]\\nSTORE: [store name]\\nTOTAL: [total amount]\\n\\nExtract all visible text and format as KEY: VALUE pairs only.\",\n",
    "    \n",
    "    # EXACT working generation parameters from LlamaVisionModel\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"enable_quantization\": True\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"Model: {CONFIG['model_type']} (using WORKING vision_processor patterns)\")\n",
    "print(f\"Image: {CONFIG['test_image']}\")\n",
    "print(f\"Prompt: {CONFIG['prompt'][:100]}...\")\n",
    "print(\"\\n‚úÖ Using PROVEN working patterns from vision_processor/models/llama_model.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful for llama ‚úì\n"
     ]
    }
   ],
   "source": [
    "# Imports - Direct model loading\n",
    "import time\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Model-specific imports based on selection\n",
    "if CONFIG[\"model_type\"] == \"llama\":\n",
    "    from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    import torchvision.transforms as T\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "print(f\"Imports successful for {CONFIG['model_type']} ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama model from /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision...\n",
      "‚úÖ Using WORKING quantization config (skipping vision modules)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02555de0eb184260af15a0e472c65039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Applied WORKING generation config (no sampling parameters)\n",
      "‚úÖ Model loaded successfully in 5.59s\n",
      "Model device: cuda:0\n",
      "Quantization active: True\n"
     ]
    }
   ],
   "source": [
    "# Load model directly - USING WORKING VISION_PROCESSOR PATTERNS\n",
    "model_path = CONFIG[\"model_paths\"][CONFIG[\"model_type\"]]\n",
    "print(f\"Loading {CONFIG['model_type']} model from {model_path}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    if CONFIG[\"model_type\"] == \"llama\":\n",
    "        # EXACT pattern from vision_processor/models/llama_model.py\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        # Working quantization config from LlamaVisionModel\n",
    "        quantization_config = None\n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,\n",
    "                    llm_int8_enable_fp32_cpu_offload=True,\n",
    "                    llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n",
    "                    llm_int8_threshold=6.0,\n",
    "                )\n",
    "                print(\"‚úÖ Using WORKING quantization config (skipping vision modules)\")\n",
    "            except ImportError:\n",
    "                print(\"Quantization not available, using FP16\")\n",
    "                CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        # Working model loading args from LlamaVisionModel\n",
    "        model_loading_args = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if quantization_config:\n",
    "            model_loading_args[\"quantization_config\"] = quantization_config\n",
    "        \n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            **model_loading_args\n",
    "        ).eval()\n",
    "        \n",
    "        # CRITICAL: Set working generation config exactly like LlamaVisionModel\n",
    "        model.generation_config.max_new_tokens = CONFIG[\"max_new_tokens\"]\n",
    "        model.generation_config.do_sample = False\n",
    "        model.generation_config.temperature = None  # Disable temperature\n",
    "        model.generation_config.top_p = None        # Disable top_p  \n",
    "        model.generation_config.top_k = None        # Disable top_k\n",
    "        model.config.use_cache = True               # Enable KV cache\n",
    "        \n",
    "        print(\"‚úÖ Applied WORKING generation config (no sampling parameters)\")\n",
    "        \n",
    "    elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "        # Load InternVL3\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        model_kwargs = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                model_kwargs[\"load_in_8bit\"] = True\n",
    "                print(\"8-bit quantization enabled\")\n",
    "            except Exception:\n",
    "                print(\"Quantization not available, using bfloat16\")\n",
    "                CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            **model_kwargs\n",
    "        ).eval()\n",
    "        \n",
    "        if torch.cuda.is_available() and not CONFIG[\"enable_quantization\"]:\n",
    "            model = model.cuda()\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Model loaded successfully in {load_time:.2f}s\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Quantization active: {CONFIG['enable_quantization']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Model loading failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Image loaded: (2048, 2048)\n",
      "  File size: 211.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess image\n",
    "test_image_path = Path(CONFIG[\"test_image\"])\n",
    "\n",
    "if not test_image_path.exists():\n",
    "    print(f\"‚úó Test image not found: {test_image_path}\")\n",
    "    available = list(Path(\"datasets\").glob(\"*.png\"))[:5]\n",
    "    print(f\"Available images: {[img.name for img in available]}\")\n",
    "    raise FileNotFoundError(f\"Test image not found: {test_image_path}\")\n",
    "\n",
    "# Load image\n",
    "image = Image.open(test_image_path)\n",
    "if image.mode != \"RGB\":\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "print(f\"‚úì Image loaded: {image.size}\")\n",
    "print(f\"  File size: {test_image_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run inference - ULTRA-AGGRESSIVE REPETITION CONTROL\nprompt = CONFIG[\"prompt\"]\nprint(f\"Running inference with {CONFIG['model_type']}...\")\nprint(f\"Prompt: {prompt[:100]}...\")\nprint(\"-\" * 50)\n\nstart_time = time.time()\n\nclass UltraAggressiveRepetitionController:\n    \"\"\"Ultra-aggressive repetition detection and control specifically for Llama-3.2-Vision.\"\"\"\n    \n    def __init__(self, word_threshold: float = 0.15, phrase_threshold: int = 2):\n        \"\"\"\n        Initialize ultra-aggressive repetition controller.\n        \n        Args:\n            word_threshold: If any word appears more than this % of total words, it's repetitive (15% vs 30%)\n            phrase_threshold: Minimum repetitions to trigger cleaning (2 vs 3)\n        \"\"\"\n        self.word_threshold = word_threshold\n        self.phrase_threshold = phrase_threshold\n        \n        # Known problematic patterns from Llama-3.2-Vision\n        self.toxic_patterns = [\n            r\"THANK YOU FOR SHOPPING WITH US[^.]*\",\n            r\"All prices include GST where applicable[^.]*\",\n            r\"\\\\+[a-zA-Z]*\\{[^}]*\\}\",  # LaTeX artifacts\n            r\"\\(\\s*\\)\",  # Empty parentheses\n            r\"[.-]\\s*THANK YOU\",  # Dash/period before thank you\n        ]\n    \n    def detect_repetitive_generation(self, text: str, min_words: int = 3) -> bool:\n        \"\"\"Ultra-sensitive repetition detection.\"\"\"\n        words = text.split()\n        \n        # Much stricter minimum content requirement\n        if len(words) < min_words:\n            return True\n        \n        # Check for known toxic patterns first\n        if self._has_toxic_patterns(text):\n            return True\n            \n        # Ultra-aggressive word repetition check (15% threshold vs 30%)\n        word_counts = {}\n        for word in words:\n            word_lower = word.lower().strip('.,!?()[]{}')\n            if len(word_lower) > 2:  # Ignore very short words\n                word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n        \n        total_words = len([w for w in words if len(w.strip('.,!?()[]{}')) > 2])\n        if total_words > 0:\n            for word, count in word_counts.items():\n                if count > total_words * self.word_threshold:  # 15% threshold\n                    return True\n        \n        # Ultra-aggressive phrase repetition\n        if self._detect_aggressive_phrase_repetition(text):\n            return True\n            \n        return False\n    \n    def _has_toxic_patterns(self, text: str) -> bool:\n        \"\"\"Check for known problematic patterns.\"\"\"\n        import re\n        \n        for pattern in self.toxic_patterns:\n            matches = re.findall(pattern, text, flags=re.IGNORECASE)\n            if len(matches) >= 2:  # Even 2 occurrences is too many\n                return True\n        \n        return False\n    \n    def _detect_aggressive_phrase_repetition(self, text: str) -> bool:\n        \"\"\"Ultra-aggressive phrase repetition detection.\"\"\"\n        import re\n        \n        # Check for 3+ word phrases repeated even twice\n        words = text.split()\n        for i in range(len(words) - 6):  # Need at least 6 words for 3+3\n            phrase = ' '.join(words[i:i+3]).lower()\n            remainder = ' '.join(words[i+3:]).lower()\n            if phrase in remainder:\n                return True\n        \n        # Check sentences/segments\n        segments = re.split(r'[.!?]+', text)\n        segment_counts = {}\n        \n        for segment in segments:\n            segment_clean = re.sub(r'\\s+', ' ', segment.strip().lower())\n            # Much shorter minimum segment length\n            if len(segment_clean) > 5:  # Was 10, now 5\n                segment_counts[segment_clean] = segment_counts.get(segment_clean, 0) + 1\n        \n        # Any segment appearing twice is problematic\n        for count in segment_counts.values():\n            if count >= self.phrase_threshold:  # Now 2 instead of 3\n                return True\n                \n        return False\n    \n    def clean_response(self, response: str) -> str:\n        \"\"\"Ultra-aggressive cleaning with early truncation.\"\"\"\n        import re\n        \n        if not response or len(response.strip()) == 0:\n            return \"\"\n        \n        original_length = len(response)\n        \n        # Step 1: Early truncation at first major repetition\n        response = self._early_truncate_at_repetition(response)\n        \n        # Step 2: Remove toxic patterns aggressively\n        response = self._remove_toxic_patterns(response)\n        \n        # Step 3: Remove safety warnings\n        response = self._remove_safety_warnings(response)\n        \n        # Step 4: Ultra-aggressive repetition removal\n        response = self._ultra_aggressive_word_removal(response)\n        response = self._ultra_aggressive_phrase_removal(response)\n        response = self._ultra_aggressive_sentence_removal(response)\n        \n        # Step 5: Clean artifacts\n        response = self._clean_artifacts(response)\n        \n        # Step 6: Final validation and truncation\n        response = self._final_validation_truncate(response)\n        \n        final_length = len(response)\n        reduction = ((original_length - final_length) / original_length * 100) if original_length > 0 else 0\n        \n        print(f\"üßπ Cleaning: {original_length} ‚Üí {final_length} chars ({reduction:.1f}% reduction)\")\n        \n        return response.strip()\n    \n    def _early_truncate_at_repetition(self, text: str) -> str:\n        \"\"\"Truncate immediately when repetition starts.\"\"\"\n        import re\n        \n        # Find first occurrence of toxic patterns and truncate there\n        for pattern in self.toxic_patterns:\n            match = re.search(pattern, text, flags=re.IGNORECASE)\n            if match:\n                # Find the SECOND occurrence and truncate before it\n                remaining = text[match.end():]\n                second_match = re.search(pattern, remaining, flags=re.IGNORECASE)\n                if second_match:\n                    truncate_point = match.end() + second_match.start()\n                    print(f\"üî™ Early truncation at repetition: {len(text)} ‚Üí {truncate_point} chars\")\n                    return text[:truncate_point]\n        \n        return text\n    \n    def _remove_toxic_patterns(self, text: str) -> str:\n        \"\"\"Aggressively remove known toxic patterns.\"\"\"\n        import re\n        \n        for pattern in self.toxic_patterns:\n            # Remove ALL occurrences, not just duplicates\n            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n        \n        return text\n    \n    def _remove_safety_warnings(self, text: str) -> str:\n        \"\"\"Remove safety warnings.\"\"\"\n        import re\n        \n        safety_patterns = [\n            r\"I'm not able to provide.*?information\\.?\",\n            r\"I cannot provide.*?information\\.?\", \n            r\"I'm unable to.*?\\.?\",\n            r\"I can't.*?\\.?\",\n            r\"Sorry, I cannot.*?\\.?\",\n            r\".*could compromise.*privacy.*\",\n        ]\n        \n        for pattern in safety_patterns:\n            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE | re.DOTALL)\n        \n        return text\n    \n    def _ultra_aggressive_word_removal(self, text: str) -> str:\n        \"\"\"Ultra-aggressive word repetition removal.\"\"\"\n        import re\n        \n        # Remove 2+ consecutive identical words (was 3+)\n        text = re.sub(r'\\b(\\w+)(\\s+\\1){1,}', r'\\1', text, flags=re.IGNORECASE)\n        \n        # Remove any word appearing more than 3 times total\n        words = text.split()\n        word_counts = {}\n        for word in words:\n            word_lower = word.lower().strip('.,!?()[]{}')\n            word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n        \n        # Rebuild text, limiting each word to max 3 occurrences\n        result_words = []\n        word_usage = {}\n        \n        for word in words:\n            word_lower = word.lower().strip('.,!?()[]{}')\n            current_count = word_usage.get(word_lower, 0)\n            \n            if current_count < 3:  # Allow max 3 occurrences\n                result_words.append(word)\n                word_usage[word_lower] = current_count + 1\n        \n        return ' '.join(result_words)\n    \n    def _ultra_aggressive_phrase_removal(self, text: str) -> str:\n        \"\"\"Ultra-aggressive phrase removal.\"\"\"\n        import re\n        \n        # Remove repeated 2-6 word phrases (expanded range)\n        for phrase_length in range(2, 7):\n            pattern = r'\\b((?:\\w+\\s+){' + str(phrase_length-1) + r'}\\w+)(\\s+\\1){1,}'  # 1+ repetitions vs 2+\n            text = re.sub(pattern, r'\\1', text, flags=re.IGNORECASE)\n        \n        return text\n    \n    def _ultra_aggressive_sentence_removal(self, text: str) -> str:\n        \"\"\"Ultra-aggressive sentence removal.\"\"\"\n        import re\n        \n        sentences = re.split(r'[.!?]+', text)\n        \n        # Keep only first occurrence of any sentence\n        seen = set()\n        unique_sentences = []\n        \n        for sentence in sentences:\n            sentence_clean = re.sub(r'\\s+', ' ', sentence.strip().lower())\n            sentence_clean = re.sub(r'[^\\w\\s]', '', sentence_clean)  # Remove all punctuation for comparison\n            \n            if sentence_clean and len(sentence_clean) > 3:  # Very short minimum\n                if sentence_clean not in seen:\n                    seen.add(sentence_clean)\n                    unique_sentences.append(sentence.strip())\n        \n        return '. '.join(unique_sentences)\n    \n    def _clean_artifacts(self, text: str) -> str:\n        \"\"\"Aggressive artifact cleaning.\"\"\"\n        import re\n        \n        # Remove whitespace\n        text = re.sub(r'\\s+', ' ', text)\n        \n        # Remove LaTeX/markdown aggressively\n        text = re.sub(r'\\\\+[a-zA-Z]*\\{[^}]*\\}', '', text)\n        text = re.sub(r'\\\\+[a-zA-Z]+', '', text)\n        text = re.sub(r'```+[^`]*```+', '', text)\n        text = re.sub(r'[{}]+', '', text)\n        \n        # Remove excessive punctuation\n        text = re.sub(r'[.]{2,}', '.', text)\n        text = re.sub(r'[!]{2,}', '!', text)\n        text = re.sub(r'[?]{2,}', '?', text)\n        text = re.sub(r'[,]{2,}', ',', text)\n        \n        # Remove empty parentheses and brackets\n        text = re.sub(r'\\(\\s*\\)', '', text)\n        text = re.sub(r'\\[\\s*\\]', '', text)\n        \n        # Remove standalone punctuation\n        text = re.sub(r'\\s+[.,!?;:]\\s+', ' ', text)\n        \n        return text\n    \n    def _final_validation_truncate(self, text: str, max_length: int = 800) -> str:\n        \"\"\"Final validation with aggressive truncation.\"\"\"\n        # If still repetitive after all cleaning, something is very wrong\n        if self.detect_repetitive_generation(text):\n            print(\"‚ö†Ô∏è Still repetitive after ultra-aggressive cleaning - truncating heavily\")\n            # Find last good sentence in first half\n            half_point = len(text) // 2\n            truncated = text[:half_point]\n            last_period = truncated.rfind('.')\n            if last_period > half_point * 0.5:\n                return truncated[:last_period + 1]\n            else:\n                return truncated[:half_point] + \"...\"\n        \n        # Aggressive length limit\n        if len(text) > max_length:\n            truncated = text[:max_length]\n            last_period = truncated.rfind('.')\n            if last_period > max_length * 0.7:\n                return truncated[:last_period + 1]\n            else:\n                return truncated + \"...\"\n        \n        return text\n\n# Initialize ultra-aggressive repetition controller\nrepetition_controller = UltraAggressiveRepetitionController(\n    word_threshold=0.15,  # Much stricter: 15% vs 30%\n    phrase_threshold=2    # Much stricter: 2 vs 3 repetitions\n)\n\ntry:\n    if CONFIG[\"model_type\"] == \"llama\":\n        # EXACT input preparation from LlamaVisionModel._prepare_inputs()\n        prompt_with_image = prompt if prompt.startswith(\"<|image|>\") else f\"<|image|>{prompt}\"\n        \n        inputs = processor(text=prompt_with_image, images=image, return_tensors=\"pt\")\n        \n        # WORKING device handling from LlamaVisionModel\n        device = next(model.parameters()).device\n        if device.type != \"cpu\":\n            device_target = str(device).split(\":\")[0] if \":\" in str(device) else str(device)\n            inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n        \n        print(f\"Input tensor shapes: {[(k, v.shape) for k, v in inputs.items() if hasattr(v, 'shape')]}\")\n        print(f\"Device target: {device}\")\n        \n        # ULTRA-AGGRESSIVE: Even shorter token limit\n        effective_max_tokens = min(CONFIG[\"max_new_tokens\"], 384)  # Further reduced: 384 vs 512\n        print(f\"Using ultra-short max_new_tokens: {effective_max_tokens} (was {CONFIG['max_new_tokens']})\")\n        \n        # EXACT generation kwargs from LlamaVisionModel.generate()\n        generation_kwargs = {\n            **inputs,\n            \"max_new_tokens\": effective_max_tokens,\n            \"do_sample\": False,  # Deterministic generation\n            \"pad_token_id\": processor.tokenizer.eos_token_id,\n            \"eos_token_id\": processor.tokenizer.eos_token_id,\n            \"use_cache\": True,\n        }\n        \n        print(\"‚úÖ Using ULTRA-AGGRESSIVE repetition control + shorter generation\")\n        \n        with torch.no_grad():\n            outputs = model.generate(**generation_kwargs)\n        \n        raw_response = processor.decode(\n            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n            skip_special_tokens=True\n        )\n        \n        print(f\"Raw response (first 200 chars): {raw_response[:200]}...\")\n        print(f\"Raw response length: {len(raw_response)} characters\")\n        \n        # ULTRA-AGGRESSIVE: Enhanced repetition control\n        response = repetition_controller.clean_response(raw_response)\n        \n        # Final check with stricter detection\n        if repetition_controller.detect_repetitive_generation(response):\n            print(\"‚ùå STILL REPETITIVE after ultra-aggressive cleaning!\")\n            print(\"   This indicates a fundamental issue with the model's generation pattern\")\n        else:\n            print(\"‚úÖ Ultra-aggressive cleaning successful - repetition eliminated\")\n        \n    elif CONFIG[\"model_type\"] == \"internvl\":\n        # InternVL inference with ultra-aggressive repetition control\n        image_size = 448\n        transform = T.Compose([\n            T.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n            T.ToTensor(),\n            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n        ])\n        \n        pixel_values = transform(image).unsqueeze(0)\n        \n        if torch.cuda.is_available():\n            pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n        else:\n            pixel_values = pixel_values.contiguous()\n        \n        generation_config = {\n            \"max_new_tokens\": min(CONFIG[\"max_new_tokens\"], 384),\n            \"do_sample\": False,\n            \"pad_token_id\": tokenizer.eos_token_id\n        }\n        \n        raw_response = model.chat(\n            tokenizer=tokenizer,\n            pixel_values=pixel_values,\n            question=prompt,\n            generation_config=generation_config\n        )\n        \n        if isinstance(raw_response, tuple):\n            raw_response = raw_response[0]\n        \n        # Apply ultra-aggressive repetition control\n        response = repetition_controller.clean_response(raw_response)\n    \n    inference_time = time.time() - start_time\n    print(f\"‚úÖ Inference completed in {inference_time:.2f}s\")\n    print(f\"Final response length: {len(response)} characters\")\n    \nexcept Exception as e:\n    print(f\"‚úó Inference failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    \n    response = f\"Error: Inference failed - {str(e)}\"\n    inference_time = time.time() - start_time\n\nprint(f\"Final response ready for display (length: {len(response) if 'response' in locals() else 0} characters)\")"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTED TEXT:\n",
      "============================================================\n",
      "DATE: 11-07-2022 STORE: SPOTLIGHT TOTAL: $22. 45 ITEM: Apples (kg) QUANTITY: 1 PRICE: $3. 96 TOTAL: $3. 96 ITEM: Tea Bags (box) QUANTITY: 1 PRICE: $4. 53 TOTAL: $4. 53 ITEM: Free Range Eggs (d) QUANTITY: 1 PRICE: $4. 71 TOTAL: $4. 71 ITEM: Dishwashing Liquid ( QUANTITY: 1 PRICE: $3. 79 TOTAL: $3. 79 ITEM: Bananas QUANTITY: 1 PRICE: $3. 42 TOTAL: $3. 42 Subtotal: $20. 41 GST (10%): $2. 04 TOTAL: $22. 45 PAYMENT DETAILS Method: VISA Amount: $22. 45 XXXX-XXXX-XXXX-4978 Authorization: 206851 APPROVED THANK YOU FOR SHOPPING WITH US All prices include GST where applicable  THANK YOU FOR SHOPPING WITH US All prices include GST where applicable () THANK YOU FOR SHOPPING WITH US All prices include GST where applicable () - THANK YOU FOR SHOPPING WITH US All prices include GST where applicable () THANK YOU FOR SHOPPING WITH US All prices include GST where applicable () THANK YOU FOR SHOPPING WITH US All prices include GST where applicable () THANK YOU FOR SHOPPING WITH US All prices include GST ...\n",
      "============================================================\n",
      "\n",
      "SUMMARY:\n",
      "Model: llama\n",
      "Response length: 1003 characters\n",
      "Processing time: 43.94s\n",
      "Quantization enabled: True\n",
      "Device: CUDA\n",
      "\n",
      "RESPONSE ANALYSIS:\n",
      "‚úÖ KEY-VALUE format detected\n",
      "Extracted fields:\n",
      "  DATE: 11-07-2022 STORE: SPOTLIGHT TOTAL: $22. 45 ITEM: Apples (kg) QUANTITY: 1 PRICE: $3. 96 TOTAL: $3. 96 ITEM: Tea Bags (box) QUANTITY: 1 PRICE: $4. 53 TOTAL: $4. 53 ITEM: Free Range Eggs (d) QUANTITY: 1 PRICE: $4. 71 TOTAL: $4. 71 ITEM: Dishwashing Liquid ( QUANTITY: 1 PRICE: $3. 79 TOTAL: $3. 79 ITEM: Bananas QUANTITY: 1 PRICE: $3. 42 TOTAL: $3. 42 Subtotal: $20. 41 GST (10%): $2. 04 TOTAL: $22. 45 PAYMENT DETAILS Method: VISA Amount: $22. 45 XXXX-XXXX-XXXX-4978 Authorization: 206851 APPROVED THANK YOU FOR SHOPPING WITH US All prices include GST where applicable  THANK YOU FOR SHOPPING WITH US All prices include GST where applicable () THANK YOU FOR SHOPPING WITH US All prices include GST where applicable () - THANK YOU FOR SHOPPING WITH US All prices include GST where applicable () THANK YOU FOR SHOPPING WITH US All prices include GST where applicable () THANK YOU FOR SHOPPING WITH US All prices include GST where applicable () THANK YOU FOR SHOPPING WITH US All prices include GST ...\n",
      "\n",
      "‚ö†Ô∏è ACCEPTABLE performance: 43.9s\n",
      "\n",
      "üéØ For production use:\n",
      "- Llama-3.2-Vision: Use simple JSON prompts only\n",
      "- InternVL3: More flexible, handles complex prompts better\n",
      "- Both models: Shorter max_new_tokens prevents issues\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTED TEXT:\")\n",
    "print(\"=\" * 60)\n",
    "print(response)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"Model: {CONFIG['model_type']}\")\n",
    "print(f\"Response length: {len(response)} characters\")\n",
    "print(f\"Processing time: {inference_time:.2f}s\")\n",
    "print(f\"Quantization enabled: {CONFIG['enable_quantization']}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Enhanced JSON parsing with validation\n",
    "print(f\"\\nRESPONSE ANALYSIS:\")\n",
    "if response.strip().startswith('{') and response.strip().endswith('}'):\n",
    "    try:\n",
    "        import json\n",
    "        parsed = json.loads(response.strip())\n",
    "        print(f\"‚úÖ VALID JSON EXTRACTED:\")\n",
    "        for key, value in parsed.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Validate completeness\n",
    "        expected_fields = [\"DATE\", \"STORE\", \"TOTAL\"]\n",
    "        missing = [field for field in expected_fields if field not in parsed or not parsed[field]]\n",
    "        if missing:\n",
    "            print(f\"‚ö†Ô∏è Missing fields: {missing}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ All expected fields present\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå Invalid JSON: {e}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        \n",
    "elif any(keyword in response for keyword in [\"DATE:\", \"STORE:\", \"TOTAL:\"]):\n",
    "    print(f\"‚úÖ KEY-VALUE format detected\")\n",
    "    # Try to extract key-value pairs\n",
    "    import re\n",
    "    matches = re.findall(r'([A-Z]+):\\s*([^\\n]+)', response)\n",
    "    if matches:\n",
    "        print(f\"Extracted fields:\")\n",
    "        for key, value in matches:\n",
    "            print(f\"  {key}: {value.strip()}\")\n",
    "            \n",
    "elif any(phrase in response.lower() for phrase in [\"not able\", \"cannot provide\", \"sorry\"]):\n",
    "    print(f\"‚ùå SAFETY MODE TRIGGERED\")\n",
    "    print(f\"This indicates the prompt triggered Llama's safety restrictions\")\n",
    "    print(f\"Solution: Use simpler JSON format prompts\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è UNSTRUCTURED RESPONSE\")\n",
    "    print(f\"Response doesn't match expected patterns\")\n",
    "    print(f\"Consider using different prompt format\")\n",
    "\n",
    "# Performance assessment\n",
    "if inference_time < 30:\n",
    "    print(f\"\\n‚ö° GOOD performance: {inference_time:.1f}s\")\n",
    "elif inference_time < 60:\n",
    "    print(f\"\\n‚ö†Ô∏è ACCEPTABLE performance: {inference_time:.1f}s\") \n",
    "else:\n",
    "    print(f\"\\n‚ùå SLOW performance: {inference_time:.1f}s\")\n",
    "\n",
    "print(f\"\\nüéØ For production use:\")\n",
    "print(f\"- Llama-3.2-Vision: Use simple JSON prompts only\")\n",
    "print(f\"- InternVL3: More flexible, handles complex prompts better\")\n",
    "print(f\"- Both models: Shorter max_new_tokens prevents issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test additional prompts - WITH ULTRA-AGGRESSIVE REPETITION CONTROL\nworking_test_prompts = [\n    \"<|image|>Extract store name and total amount in KEY-VALUE format.\\n\\nOutput format:\\nSTORE: [store name]\\nTOTAL: [total amount]\",\n    \"<|image|>What type of business document is this? Answer: receipt, invoice, or statement.\",\n    \"<|image|>Extract the date from this document in format DD/MM/YYYY.\"\n]\n\nprint(\"Testing additional prompts with ULTRA-AGGRESSIVE REPETITION CONTROL...\\n\")\n\nfor i, test_prompt in enumerate(working_test_prompts, 1):\n    print(f\"Test {i}: {test_prompt[:60]}...\")\n    try:\n        start = time.time()\n        \n        if CONFIG[\"model_type\"] == \"llama\":\n            # Use EXACT same pattern as main inference\n            prompt_with_image = test_prompt if test_prompt.startswith(\"<|image|>\") else f\"<|image|>{test_prompt}\"\n            \n            inputs = processor(text=prompt_with_image, images=image, return_tensors=\"pt\")\n            \n            # Same device handling\n            device = next(model.parameters()).device\n            if device.type != \"cpu\":\n                device_target = str(device).split(\":\")[0] if \":\" in str(device) else str(device)\n                inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n            \n            # ULTRA-AGGRESSIVE: Extremely short tokens for tests\n            generation_kwargs = {\n                **inputs,\n                \"max_new_tokens\": 96,  # Even shorter: 96 vs 128\n                \"do_sample\": False,\n                \"pad_token_id\": processor.tokenizer.eos_token_id,\n                \"eos_token_id\": processor.tokenizer.eos_token_id,\n                \"use_cache\": True,\n            }\n            \n            with torch.no_grad():\n                outputs = model.generate(**generation_kwargs)\n            \n            raw_result = processor.decode(\n                outputs[0][inputs[\"input_ids\"].shape[-1]:],\n                skip_special_tokens=True\n            )\n            \n            # Apply ultra-aggressive repetition control\n            result = repetition_controller.clean_response(raw_result)\n            \n        elif CONFIG[\"model_type\"] == \"internvl\":\n            result = model.chat(\n                tokenizer=tokenizer,\n                pixel_values=pixel_values,\n                question=test_prompt,\n                generation_config={\n                    \"max_new_tokens\": 96, \n                    \"do_sample\": False\n                }\n            )\n            if isinstance(result, tuple):\n                result = result[0]\n            result = repetition_controller.clean_response(result)\n        \n        elapsed = time.time() - start\n        \n        # Ultra-strict analysis of results\n        if repetition_controller.detect_repetitive_generation(result):\n            print(f\"‚ùå STILL REPETITIVE ({elapsed:.1f}s): {result[:60]}...\")\n            print(f\"   Even ultra-aggressive cleaning failed - model has fundamental repetition issue\")\n        elif any(phrase in result.lower() for phrase in [\"not able\", \"cannot provide\", \"sorry\"]):\n            print(f\"‚ö†Ô∏è Safety mode triggered ({elapsed:.1f}s): {result[:60]}...\")\n        elif len(result.strip()) < 3:\n            print(f\"‚ö†Ô∏è Over-cleaned ({elapsed:.1f}s): '{result}' - may be too aggressive\")\n        else:\n            print(f\"‚úÖ SUCCESS ({elapsed:.1f}s): {result[:80]}...\")\n            print(f\"   Length: {len(result)} chars - repetition eliminated\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error: {str(e)[:100]}...\")\n    print(\"-\" * 50)\n\nprint(\"\\nüéØ ULTRA-AGGRESSIVE REPETITION CONTROL FEATURES:\")\nprint(\"üî• UltraAggressiveRepetitionController - Nuclear option for repetition\")\nprint(\"üî• Stricter thresholds:\")\nprint(\"   - Word repetition: 15% threshold (was 30%)\")  \nprint(\"   - Phrase repetition: 2 occurrences trigger (was 3)\")\nprint(\"   - Sentence repetition: Any duplicate removed\")\nprint(\"üî• Toxic pattern targeting:\")\nprint(\"   - 'THANK YOU FOR SHOPPING...' pattern recognition\")\nprint(\"   - 'All prices include GST...' pattern recognition\")\nprint(\"   - LaTeX artifact removal\")\nprint(\"üî• Early truncation at first repetition detection\")\nprint(\"üî• Max 3 occurrences per word across entire text\")\nprint(\"üî• Ultra-short token limits (384 main, 96 tests)\")\nprint(\"üî• Aggressive artifact cleaning (punctuation, parentheses, etc.)\")\nprint(\"\\nüí° If this still shows repetition, the issue is in the model's generation\")\nprint(\"   pattern itself, not the post-processing cleaning.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up memory...\n",
      "‚úì Model deleted\n",
      "‚úì Processor deleted\n",
      "‚úì CUDA cache cleared\n",
      "‚úì Memory cleanup completed\n",
      "\n",
      "üéâ Test completed!\n",
      "\n",
      "üìã SUMMARY OF FIXES APPLIED:\n",
      "1. ‚ùå FIXED: Removed repetition_penalty (causes CUDA assert errors)\n",
      "2. ‚úÖ SAFE: Using minimal generation parameters\n",
      "3. üîß ROBUST: Added proper error handling\n",
      "4. üßπ CLEAN: Safe memory cleanup with existence checks\n",
      "\n",
      "üöÄ Ready for testing on remote machine!\n"
     ]
    }
   ],
   "source": [
    "# Memory cleanup\n",
    "print(\"Cleaning up memory...\")\n",
    "\n",
    "# Safe cleanup with existence checks\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    try:\n",
    "        del model\n",
    "        print(\"‚úì Model deleted\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if CONFIG[\"model_type\"] == \"llama\":\n",
    "    if 'processor' in locals() or 'processor' in globals():\n",
    "        try:\n",
    "            del processor\n",
    "            print(\"‚úì Processor deleted\")\n",
    "        except:\n",
    "            pass\n",
    "elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "    if 'tokenizer' in locals() or 'tokenizer' in globals():\n",
    "        try:\n",
    "            del tokenizer\n",
    "            print(\"‚úì Tokenizer deleted\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"‚úì CUDA cache cleared\")\n",
    "\n",
    "print(\"‚úì Memory cleanup completed\")\n",
    "print(\"\\nüéâ Test completed!\")\n",
    "print(\"\\nüìã SUMMARY OF FIXES APPLIED:\")\n",
    "print(\"1. ‚ùå FIXED: Removed repetition_penalty (causes CUDA assert errors)\")\n",
    "print(\"2. ‚úÖ SAFE: Using minimal generation parameters\")\n",
    "print(\"3. üîß ROBUST: Added proper error handling\")\n",
    "print(\"4. üßπ CLEAN: Safe memory cleanup with existence checks\")\n",
    "print(\"\\nüöÄ Ready for testing on remote machine!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vision_env)",
   "language": "python",
   "name": "vision_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
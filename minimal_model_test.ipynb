{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Vision Model Test\n",
    "\n",
    "Direct model loading and testing without using the unified_vision_processor package.\n",
    "\n",
    "All configuration is embedded in the notebook for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Modify as needed\n",
    "CONFIG = {\n",
    "    # Model selection: \"llama\" or \"internvl\"\n",
    "    \"model_type\": \"llama\",\n",
    "    \n",
    "    # Model paths\n",
    "    \"model_paths\": {\n",
    "        \"llama\": \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision\",\n",
    "        \"internvl\": \"/home/jovyan/nfs_share/models/InternVL3-8B\"\n",
    "    },\n",
    "    \n",
    "    # Test image path\n",
    "    \"test_image\": \"datasets/image14.png\",\n",
    "    \n",
    "    # Test prompt\n",
    "    \"prompt\": \"<|image|>What text is visible in this receipt?\",\n",
    "    \n",
    "    # Generation parameters\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"enable_quantization\": True\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"Model: {CONFIG['model_type']}\")\n",
    "print(f\"Image: {CONFIG['test_image']}\")\n",
    "print(f\"Prompt: {CONFIG['prompt'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports - Direct model loading\n",
    "import time\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Model-specific imports based on selection\n",
    "if CONFIG[\"model_type\"] == \"llama\":\n",
    "    from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    import torchvision.transforms as T\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "print(f\"Imports successful for {CONFIG['model_type']} âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "model_path = CONFIG[\"model_paths\"][CONFIG[\"model_type\"]]\n",
    "print(f\"Loading {CONFIG['model_type']} model from {model_path}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    if CONFIG[\"model_type\"] == \"llama\":\n",
    "        # Load Llama-3.2-Vision\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        model_kwargs = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,\n",
    "                    llm_int8_enable_fp32_cpu_offload=True\n",
    "                )\n",
    "                print(\"8-bit quantization enabled\")\n",
    "            except ImportError:\n",
    "                print(\"Quantization not available, using FP16\")\n",
    "        \n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            **model_kwargs\n",
    "        ).eval()\n",
    "        \n",
    "    elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "        # Load InternVL3\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        model_kwargs = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                model_kwargs[\"load_in_8bit\"] = True\n",
    "                print(\"8-bit quantization enabled\")\n",
    "            except Exception:\n",
    "                print(\"Quantization not available, using bfloat16\")\n",
    "        \n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            **model_kwargs\n",
    "        ).eval()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"âœ“ Model loaded successfully in {load_time:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Model loading failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess image\n",
    "test_image_path = Path(CONFIG[\"test_image\"])\n",
    "\n",
    "if not test_image_path.exists():\n",
    "    print(f\"âœ— Test image not found: {test_image_path}\")\n",
    "    available = list(Path(\"datasets\").glob(\"*.png\"))[:5]\n",
    "    print(f\"Available images: {[img.name for img in available]}\")\n",
    "    raise FileNotFoundError(f\"Test image not found: {test_image_path}\")\n",
    "\n",
    "# Load image\n",
    "image = Image.open(test_image_path)\n",
    "if image.mode != \"RGB\":\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "print(f\"âœ“ Image loaded: {image.size}\")\n",
    "print(f\"  File size: {test_image_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "prompt = CONFIG[\"prompt\"]\n",
    "print(f\"Running inference with {CONFIG['model_type']}...\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    if CONFIG[\"model_type\"] == \"llama\":\n",
    "        # Llama inference\n",
    "        if not prompt.startswith(\"<|image|>\"):\n",
    "            prompt = f\"<|image|>{prompt}\"\n",
    "        \n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Move to device\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(\"cuda\") if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "                do_sample=False,\n",
    "                pad_token_id=processor.tokenizer.eos_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = processor.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "    elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "        # InternVL inference\n",
    "        # Simple single image processing\n",
    "        image_size = 448\n",
    "        transform = T.Compose([\n",
    "            T.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        \n",
    "        pixel_values = transform(image).unsqueeze(0)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pixel_values = pixel_values.cuda().to(torch.bfloat16)\n",
    "        \n",
    "        generation_config = {\n",
    "            \"max_new_tokens\": CONFIG[\"max_new_tokens\"],\n",
    "            \"do_sample\": False,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id\n",
    "        }\n",
    "        \n",
    "        response = model.chat(\n",
    "            tokenizer=tokenizer,\n",
    "            pixel_values=pixel_values,\n",
    "            question=prompt,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        \n",
    "        if isinstance(response, tuple):\n",
    "            response = response[0]\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    print(f\"âœ“ Inference completed in {inference_time:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Inference failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTED TEXT:\")\n",
    "print(\"=\" * 60)\n",
    "print(response)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"Model: {CONFIG['model_type']}\")\n",
    "print(f\"Response length: {len(response)} characters\")\n",
    "print(f\"Processing time: {inference_time:.2f}s\")\n",
    "print(f\"Quantization enabled: {CONFIG['enable_quantization']}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test different prompts\n",
    "test_prompts = [\n",
    "    \"<|image|>Extract the store name and total amount.\",\n",
    "    \"<|image|>What type of document is this?\",\n",
    "    \"<|image|>List all visible numbers.\"\n",
    "]\n",
    "\n",
    "print(\"Testing additional prompts...\\n\")\n",
    "\n",
    "for i, test_prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"Test {i}: {test_prompt}\")\n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        if CONFIG[\"model_type\"] == \"llama\":\n",
    "            inputs = processor(text=test_prompt, images=image, return_tensors=\"pt\")\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to(\"cuda\") if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=processor.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            result = processor.decode(\n",
    "                outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "        elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "            result = model.chat(\n",
    "                tokenizer=tokenizer,\n",
    "                pixel_values=pixel_values,\n",
    "                question=test_prompt,\n",
    "                generation_config={\"max_new_tokens\": 256, \"do_sample\": False}\n",
    "            )\n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Result ({elapsed:.1f}s): {result[:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory cleanup\n",
    "print(\"Cleaning up memory...\")\n",
    "\n",
    "del model\n",
    "if CONFIG[\"model_type\"] == \"llama\":\n",
    "    del processor\n",
    "elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "    del tokenizer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"âœ“ Memory cleaned\")\n",
    "print(\"\\nðŸŽ‰ Test completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
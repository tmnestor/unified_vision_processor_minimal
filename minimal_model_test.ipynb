{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Vision Model Test\n",
    "\n",
    "Direct model loading and testing without using the unified_vision_processor package.\n",
    "\n",
    "All configuration is embedded in the notebook for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Model: llama\n",
      "Image: datasets/image14.png\n",
      "Prompt: <|image|>Extract receipt details in JSON: {\"DATE\": \"\", \"STORE\": \"\", \"TOTAL\": \"\"}\n",
      "Safety bypass: True\n",
      "\n",
      "‚úì Using PROVEN JSON prompt pattern that bypasses Llama safety mode\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Modify as needed\n",
    "CONFIG = {\n",
    "    # Model selection: \"llama\" or \"internvl\"\n",
    "    \"model_type\": \"llama\",\n",
    "    \n",
    "    # Model paths\n",
    "    \"model_paths\": {\n",
    "        \"llama\": \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision\",\n",
    "        \"internvl\": \"/home/jovyan/nfs_share/models/InternVL3-8B\"\n",
    "    },\n",
    "    \n",
    "    # Test image path\n",
    "    \"test_image\": \"datasets/image14.png\",\n",
    "    \n",
    "    # PROVEN WORKING prompt pattern for Llama safety bypass\n",
    "    \"prompt\": \"<|image|>Extract receipt details in JSON: {\\\"DATE\\\": \\\"\\\", \\\"STORE\\\": \\\"\\\", \\\"TOTAL\\\": \\\"\\\"}\",\n",
    "    \n",
    "    # Generation parameters - optimized for reliable output\n",
    "    \"max_new_tokens\": 128,  # Shorter to prevent repetition\n",
    "    \"enable_quantization\": True,\n",
    "    \n",
    "    # Safety mode bypass settings\n",
    "    \"bypass_safety\": True,\n",
    "    \"deterministic_generation\": True\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"Model: {CONFIG['model_type']}\")\n",
    "print(f\"Image: {CONFIG['test_image']}\")\n",
    "print(f\"Prompt: {CONFIG['prompt']}\")\n",
    "print(f\"Safety bypass: {CONFIG['bypass_safety']}\")\n",
    "print(\"\\n‚úì Using PROVEN JSON prompt pattern that bypasses Llama safety mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful for llama ‚úì\n"
     ]
    }
   ],
   "source": [
    "# Imports - Direct model loading\n",
    "import time\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Model-specific imports based on selection\n",
    "if CONFIG[\"model_type\"] == \"llama\":\n",
    "    from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    import torchvision.transforms as T\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "print(f\"Imports successful for {CONFIG['model_type']} ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama model from /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision...\n",
      "8-bit quantization enabled (skipping vision modules)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f6428cf8fe4f809b0d017dc66b5cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded successfully in 5.75s\n",
      "Model device: cuda:0\n",
      "Quantization active: True\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "model_path = CONFIG[\"model_paths\"][CONFIG[\"model_type\"]]\n",
    "print(f\"Loading {CONFIG['model_type']} model from {model_path}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    if CONFIG[\"model_type\"] == \"llama\":\n",
    "        # Load Llama-3.2-Vision\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        model_kwargs = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,\n",
    "                    llm_int8_enable_fp32_cpu_offload=True,\n",
    "                    llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n",
    "                    llm_int8_threshold=6.0\n",
    "                )\n",
    "                model_kwargs[\"quantization_config\"] = quantization_config\n",
    "                print(\"8-bit quantization enabled (skipping vision modules)\")\n",
    "            except ImportError:\n",
    "                print(\"Quantization not available, using FP16\")\n",
    "                CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            **model_kwargs\n",
    "        ).eval()\n",
    "        \n",
    "        # Configure generation settings\n",
    "        model.generation_config.max_new_tokens = 1024\n",
    "        model.generation_config.do_sample = False\n",
    "        model.generation_config.use_cache = True\n",
    "        \n",
    "    elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "        # Load InternVL3\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        model_kwargs = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                model_kwargs[\"load_in_8bit\"] = True\n",
    "                print(\"8-bit quantization enabled\")\n",
    "            except Exception:\n",
    "                print(\"Quantization not available, using bfloat16\")\n",
    "                CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            **model_kwargs\n",
    "        ).eval()\n",
    "        \n",
    "        if torch.cuda.is_available() and not CONFIG[\"enable_quantization\"]:\n",
    "            model = model.cuda()\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"‚úì Model loaded successfully in {load_time:.2f}s\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Quantization active: {CONFIG['enable_quantization']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Model loading failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Try loading without quantization as fallback\n",
    "    if CONFIG[\"enable_quantization\"]:\n",
    "        print(\"\\nRetrying without quantization...\")\n",
    "        CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        if CONFIG[\"model_type\"] == \"llama\":\n",
    "            model = MllamaForConditionalGeneration.from_pretrained(\n",
    "                model_path,\n",
    "                low_cpu_mem_usage=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "                local_files_only=True\n",
    "            ).eval()\n",
    "            \n",
    "            # Configure generation settings\n",
    "            model.generation_config.max_new_tokens = 1024\n",
    "            model.generation_config.do_sample = False\n",
    "            model.generation_config.use_cache = True\n",
    "            \n",
    "        elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_path,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                local_files_only=True\n",
    "            ).eval()\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"‚úì Model loaded without quantization in {load_time:.2f}s\")\n",
    "        print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    else:\n",
    "        print(\"Cannot proceed without model - please check configuration\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Image loaded: (2048, 2048)\n",
      "  File size: 211.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess image\n",
    "test_image_path = Path(CONFIG[\"test_image\"])\n",
    "\n",
    "if not test_image_path.exists():\n",
    "    print(f\"‚úó Test image not found: {test_image_path}\")\n",
    "    available = list(Path(\"datasets\").glob(\"*.png\"))[:5]\n",
    "    print(f\"Available images: {[img.name for img in available]}\")\n",
    "    raise FileNotFoundError(f\"Test image not found: {test_image_path}\")\n",
    "\n",
    "# Load image\n",
    "image = Image.open(test_image_path)\n",
    "if image.mode != \"RGB\":\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "print(f\"‚úì Image loaded: {image.size}\")\n",
    "print(f\"  File size: {test_image_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference with llama...\n",
      "Prompt: <|image|>Extract receipt details in JSON: {\"DATE\": \"\", \"STORE\": \"\", \"TOTAL\": \"\"}\n",
      "--------------------------------------------------\n",
      "Input tensor shapes: [('input_ids', torch.Size([1, 22])), ('attention_mask', torch.Size([1, 22])), ('pixel_values', torch.Size([1, 1, 4, 3, 448, 448])), ('aspect_ratio_ids', torch.Size([1, 1])), ('aspect_ratio_mask', torch.Size([1, 1, 4])), ('cross_attention_mask', torch.Size([1, 22, 1, 4]))]\n",
      "‚úó Inference failed: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "Trying fallback without quantization...\n",
      "Reloading model without quantization...\n",
      "‚úó Fallback also failed: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Final response ready for display (length: 638 characters)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1729647348947/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [1,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2701/192759486.py\", line 76, in <module>\n",
      "    outputs = model.generate(**generation_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/vision_env/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/vision_env/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2047, in generate\n",
      "    result = self._sample(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/vision_env/lib/python3.11/site-packages/transformers/generation/utils.py\", line 3017, in _sample\n",
      "    next_token_scores = logits_processor(input_ids, next_token_logits)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/vision_env/lib/python3.11/site-packages/transformers/generation/logits_process.py\", line 104, in __call__\n",
      "    scores = processor(input_ids, scores)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/vision_env/lib/python3.11/site-packages/transformers/generation/logits_process.py\", line 356, in __call__\n",
      "    score = torch.where(score < 0, score * self.penalty, score / self.penalty)\n",
      "                                                         ~~~~~~^~~~~~~~~~~~~~\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "prompt = CONFIG[\"prompt\"]\n",
    "print(f\"Running inference with {CONFIG['model_type']}...\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def clean_response(response: str) -> str:\n",
    "    \"\"\"Clean response from repetitive text and artifacts.\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Remove excessive repetition of ANY word repeated 3+ times consecutively\n",
    "    response = re.sub(r'\\b(\\w+)(\\s+\\1){2,}', r'\\1', response, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove excessive repetition of longer phrases\n",
    "    response = re.sub(r'\\b((?:\\w+\\s+){1,3})(?:\\1){2,}', r'\\1', response, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove safety warnings and repetitive content\n",
    "    safety_patterns = [\n",
    "        r\"I'm not able to provide.*?information\\.\",\n",
    "        r\"I cannot provide.*?information\\.\",\n",
    "        r\"I'm unable to.*?\\.\",\n",
    "        r\"I can't.*?\\.\",\n",
    "        r\"Sorry, I cannot.*?\\.\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in safety_patterns:\n",
    "        response = re.sub(pattern, \"\", response, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Clean up excessive whitespace and artifacts\n",
    "    response = re.sub(r'\\s+', ' ', response)\n",
    "    response = re.sub(r'[{}]+', '', response)  # Remove extra braces\n",
    "    response = re.sub(r'\\\\+', '', response)    # Remove backslashes\n",
    "    \n",
    "    # Extract JSON if present\n",
    "    json_match = re.search(r'\\{[^{}]*\\}', response)\n",
    "    if json_match:\n",
    "        response = json_match.group(0)\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "try:\n",
    "    if CONFIG[\"model_type\"] == \"llama\":\n",
    "        # Enhanced Llama inference with safety bypass\n",
    "        if not prompt.startswith(\"<|image|>\"):\n",
    "            prompt = f\"<|image|>{prompt}\"\n",
    "        \n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Move to device and ensure contiguity for quantized models\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "            inputs = {k: v.to(device).contiguous() if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "            inputs = {k: v.contiguous() if hasattr(v, \"contiguous\") else v for k, v in inputs.items()}\n",
    "        \n",
    "        print(f\"Input tensor shapes: {[(k, v.shape) for k, v in inputs.items() if hasattr(v, 'shape')]}\")\n",
    "        \n",
    "        # Generation settings optimized for safety bypass\n",
    "        generation_kwargs = {\n",
    "            **inputs,\n",
    "            \"max_new_tokens\": CONFIG[\"max_new_tokens\"],\n",
    "            \"do_sample\": False,  # Critical for safety bypass\n",
    "            \"pad_token_id\": processor.tokenizer.eos_token_id,\n",
    "            \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "            \"use_cache\": True,\n",
    "            \"repetition_penalty\": 1.1,  # Reduce repetition\n",
    "            \"temperature\": None,  # Force deterministic\n",
    "            \"top_p\": None,        # Force deterministic\n",
    "            \"top_k\": None         # Force deterministic\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**generation_kwargs)\n",
    "        \n",
    "        raw_response = processor.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Raw response (first 200 chars): {raw_response[:200]}...\")\n",
    "        \n",
    "        # Clean the response\n",
    "        response = clean_response(raw_response)\n",
    "        \n",
    "    elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "        # InternVL inference - generally more stable\n",
    "        image_size = 448\n",
    "        transform = T.Compose([\n",
    "            T.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        \n",
    "        pixel_values = transform(image).unsqueeze(0)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n",
    "        else:\n",
    "            pixel_values = pixel_values.contiguous()\n",
    "        \n",
    "        generation_config = {\n",
    "            \"max_new_tokens\": CONFIG[\"max_new_tokens\"],\n",
    "            \"do_sample\": False,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            \"repetition_penalty\": 1.1\n",
    "        }\n",
    "        \n",
    "        raw_response = model.chat(\n",
    "            tokenizer=tokenizer,\n",
    "            pixel_values=pixel_values,\n",
    "            question=prompt,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        \n",
    "        if isinstance(raw_response, tuple):\n",
    "            raw_response = raw_response[0]\n",
    "        \n",
    "        # Clean the response\n",
    "        response = clean_response(raw_response)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    print(f\"‚úì Inference completed in {inference_time:.2f}s\")\n",
    "    print(f\"Cleaned response: {response}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Inference failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Fallback: Try without quantization\n",
    "    print(\"\\nTrying fallback without quantization...\")\n",
    "    if CONFIG[\"model_type\"] == \"llama\":\n",
    "        try:\n",
    "            # Reload model without quantization\n",
    "            print(\"Reloading model without quantization...\")\n",
    "            del model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            model = MllamaForConditionalGeneration.from_pretrained(\n",
    "                CONFIG[\"model_paths\"][\"llama\"],\n",
    "                low_cpu_mem_usage=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "                local_files_only=True\n",
    "            ).eval()\n",
    "            \n",
    "            # Retry inference with strict safety bypass\n",
    "            inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to(\"cuda\").contiguous() if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=processor.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "            \n",
    "            raw_response = processor.decode(\n",
    "                outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            response = clean_response(raw_response)\n",
    "            inference_time = time.time() - start_time\n",
    "            print(f\"‚úì Fallback inference completed in {inference_time:.2f}s\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚úó Fallback also failed: {e2}\")\n",
    "            response = f\"Error: Both primary and fallback inference failed. Primary: {str(e)}, Fallback: {str(e2)}\"\n",
    "            inference_time = time.time() - start_time\n",
    "    else:\n",
    "        response = f\"Error: Inference failed - {str(e)}\"\n",
    "        inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"Final response ready for display (length: {len(response) if 'response' in locals() else 0} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTED TEXT:\n",
      "============================================================\n",
      "Error: Both primary and fallback inference failed. Primary: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      ", Fallback: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "============================================================\n",
      "\n",
      "SUMMARY:\n",
      "Model: llama\n",
      "Response length: 638 characters\n",
      "Processing time: 1.05s\n",
      "Quantization enabled: True\n",
      "Device: CUDA\n",
      "\n",
      "RESPONSE ANALYSIS:\n",
      "‚ö†Ô∏è UNSTRUCTURED RESPONSE\n",
      "Response doesn't match expected patterns\n",
      "Consider using different prompt format\n",
      "\n",
      "‚ö° GOOD performance: 1.1s\n",
      "\n",
      "üéØ For production use:\n",
      "- Llama-3.2-Vision: Use simple JSON prompts only\n",
      "- InternVL3: More flexible, handles complex prompts better\n",
      "- Both models: Shorter max_new_tokens prevents issues\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTED TEXT:\")\n",
    "print(\"=\" * 60)\n",
    "print(response)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"Model: {CONFIG['model_type']}\")\n",
    "print(f\"Response length: {len(response)} characters\")\n",
    "print(f\"Processing time: {inference_time:.2f}s\")\n",
    "print(f\"Quantization enabled: {CONFIG['enable_quantization']}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Enhanced JSON parsing with validation\n",
    "print(f\"\\nRESPONSE ANALYSIS:\")\n",
    "if response.strip().startswith('{') and response.strip().endswith('}'):\n",
    "    try:\n",
    "        import json\n",
    "        parsed = json.loads(response.strip())\n",
    "        print(f\"‚úÖ VALID JSON EXTRACTED:\")\n",
    "        for key, value in parsed.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Validate completeness\n",
    "        expected_fields = [\"DATE\", \"STORE\", \"TOTAL\"]\n",
    "        missing = [field for field in expected_fields if field not in parsed or not parsed[field]]\n",
    "        if missing:\n",
    "            print(f\"‚ö†Ô∏è Missing fields: {missing}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ All expected fields present\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå Invalid JSON: {e}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        \n",
    "elif any(keyword in response for keyword in [\"DATE:\", \"STORE:\", \"TOTAL:\"]):\n",
    "    print(f\"‚úÖ KEY-VALUE format detected\")\n",
    "    # Try to extract key-value pairs\n",
    "    import re\n",
    "    matches = re.findall(r'([A-Z]+):\\s*([^\\n]+)', response)\n",
    "    if matches:\n",
    "        print(f\"Extracted fields:\")\n",
    "        for key, value in matches:\n",
    "            print(f\"  {key}: {value.strip()}\")\n",
    "            \n",
    "elif any(phrase in response.lower() for phrase in [\"not able\", \"cannot provide\", \"sorry\"]):\n",
    "    print(f\"‚ùå SAFETY MODE TRIGGERED\")\n",
    "    print(f\"This indicates the prompt triggered Llama's safety restrictions\")\n",
    "    print(f\"Solution: Use simpler JSON format prompts\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è UNSTRUCTURED RESPONSE\")\n",
    "    print(f\"Response doesn't match expected patterns\")\n",
    "    print(f\"Consider using different prompt format\")\n",
    "\n",
    "# Performance assessment\n",
    "if inference_time < 30:\n",
    "    print(f\"\\n‚ö° GOOD performance: {inference_time:.1f}s\")\n",
    "elif inference_time < 60:\n",
    "    print(f\"\\n‚ö†Ô∏è ACCEPTABLE performance: {inference_time:.1f}s\") \n",
    "else:\n",
    "    print(f\"\\n‚ùå SLOW performance: {inference_time:.1f}s\")\n",
    "\n",
    "print(f\"\\nüéØ For production use:\")\n",
    "print(f\"- Llama-3.2-Vision: Use simple JSON prompts only\")\n",
    "print(f\"- InternVL3: More flexible, handles complex prompts better\")\n",
    "print(f\"- Both models: Shorter max_new_tokens prevents issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing additional prompts with SAFE JSON patterns only...\n",
      "\n",
      "Test 1: <|image|>Extract store and total in JSON: {\"STORE\": \"\", \"TOT...\n",
      "‚ùå Error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "----------------------------------------\n",
      "Test 2: <|image|>Document type in JSON: {\"TYPE\": \"\"}...\n",
      "‚ùå Error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "----------------------------------------\n",
      "Test 3: <|image|>Extract numbers in JSON: {\"NUMBERS\": \"\"}...\n",
      "‚ùå Error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "üí° TIP: For Llama-3.2-Vision, use ONLY simple JSON format prompts to avoid safety mode.\n",
      "‚úÖ Pattern: '<|image|>Extract [field] in JSON: {\"FIELD\": \"\"}'\n",
      "‚ùå Avoid: Complex instructions, examples, or 'read all text' requests\n"
     ]
    }
   ],
   "source": [
    "# Optional: Test different prompts - Using SAFE patterns only\n",
    "safe_test_prompts = [\n",
    "    \"<|image|>Extract store and total in JSON: {\\\"STORE\\\": \\\"\\\", \\\"TOTAL\\\": \\\"\\\"}\",\n",
    "    \"<|image|>Document type in JSON: {\\\"TYPE\\\": \\\"\\\"}\",\n",
    "    \"<|image|>Extract numbers in JSON: {\\\"NUMBERS\\\": \\\"\\\"}\"\n",
    "]\n",
    "\n",
    "print(\"Testing additional prompts with SAFE JSON patterns only...\\n\")\n",
    "\n",
    "for i, test_prompt in enumerate(safe_test_prompts, 1):\n",
    "    print(f\"Test {i}: {test_prompt[:60]}...\")\n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        if CONFIG[\"model_type\"] == \"llama\":\n",
    "            inputs = processor(text=test_prompt, images=image, return_tensors=\"pt\")\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to(\"cuda\").contiguous() if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=64,  # Very short to prevent safety triggers\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=processor.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.2\n",
    "                )\n",
    "            \n",
    "            result = processor.decode(\n",
    "                outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Clean result\n",
    "            result = clean_response(result)\n",
    "            \n",
    "        elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "            result = model.chat(\n",
    "                tokenizer=tokenizer,\n",
    "                pixel_values=pixel_values,\n",
    "                question=test_prompt,\n",
    "                generation_config={\n",
    "                    \"max_new_tokens\": 64, \n",
    "                    \"do_sample\": False,\n",
    "                    \"repetition_penalty\": 1.2\n",
    "                }\n",
    "            )\n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "            result = clean_response(result)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Check if result is a safety response\n",
    "        if any(phrase in result.lower() for phrase in [\"not able\", \"cannot provide\", \"sorry\"]):\n",
    "            print(f\"‚ùå Safety mode triggered ({elapsed:.1f}s): {result[:80]}...\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Success ({elapsed:.1f}s): {result}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nüí° TIP: For Llama-3.2-Vision, use ONLY simple JSON format prompts to avoid safety mode.\")\n",
    "print(\"‚úÖ Pattern: '<|image|>Extract [field] in JSON: {\\\"FIELD\\\": \\\"\\\"}'\")\n",
    "print(\"‚ùå Avoid: Complex instructions, examples, or 'read all text' requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up memory...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Memory cleanup\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCleaning up memory...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m CONFIG[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mllama\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m processor\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Memory cleanup\n",
    "print(\"Cleaning up memory...\")\n",
    "\n",
    "del model\n",
    "if CONFIG[\"model_type\"] == \"llama\":\n",
    "    del processor\n",
    "elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "    del tokenizer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"‚úì Memory cleaned\")\n",
    "print(\"\\nüéâ Test completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vision_env)",
   "language": "python",
   "name": "vision_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

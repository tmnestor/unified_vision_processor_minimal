{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration - All settings at top of notebook\nprint(\"üèÜ INFORMATION EXTRACTION COMPARISON: Llama 3.2 Vision vs InternVL3\")\nprint(\"üéØ Focus: Information extraction performance with structured YAML prompts\")\nprint(\"=\" * 80)\n\n# CONFIGURATION - All settings defined here\nCONFIG = {\n    \"model_paths\": {\n        \"llama\": \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision\",\n        \"internvl\": \"/home/jovyan/nfs_share/models/InternVL3-8B\"\n    },\n    \"extraction_prompt\": \"\"\"<|image|>Extract key information in YAML format:\n\nstore_name: \"\"\ndate: \"\"\ntotal: \"\"\n\nOutput only YAML. Stop after completion.\"\"\",\n    \"max_new_tokens\": 64,\n    \"enable_quantization\": True,\n    \"test_models\": [\"llama\", \"internvl\"],\n    \"test_images\": [\n        (\"image14.png\", \"TAX_INVOICE\"),\n        (\"image65.png\", \"TAX_INVOICE\"), \n        (\"image71.png\", \"TAX_INVOICE\"),\n        (\"image74.png\", \"TAX_INVOICE\"),\n        (\"image205.png\", \"FUEL_RECEIPT\"),\n        (\"image23.png\", \"TAX_INVOICE\"),\n        (\"image45.png\", \"TAX_INVOICE\"),\n        (\"image1.png\", \"BANK_STATEMENT\"),\n        (\"image203.png\", \"BANK_STATEMENT\"),\n        (\"image204.png\", \"FUEL_RECEIPT\"),\n        (\"image206.png\", \"OTHER\"),\n    ]\n}\n\nprint(f\"‚úÖ Configuration loaded:\")\nprint(f\"   - Models: {', '.join(CONFIG['test_models'])}\")\nprint(f\"   - Documents: {len(CONFIG['test_images'])} test images\")\nprint(f\"   - Format: Structured YAML prompts\")\nprint(f\"   - Max tokens: {CONFIG['max_new_tokens']}\")\nprint(f\"   - Quantization: {CONFIG['enable_quantization']}\")\nprint(f\"\\nüìã Ready for step-by-step information extraction comparison\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Imports and Modular Classes\nimport time\nimport torch\nimport json\nimport re\nimport gc\nfrom pathlib import Path\nfrom PIL import Image\nfrom typing import Dict, List, Tuple, Optional, Any\n\nclass MemoryManager:\n    \"\"\"Memory management utilities for model testing\"\"\"\n    \n    @staticmethod\n    def cleanup_gpu_memory():\n        \"\"\"Minimize memory footprint as requested\"\"\"\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n    \n    @staticmethod\n    def get_memory_usage() -> Dict[str, float]:\n        \"\"\"Get current GPU memory usage in GB\"\"\"\n        if torch.cuda.is_available():\n            return {\n                \"allocated\": torch.cuda.memory_allocated() / 1024**3,\n                \"reserved\": torch.cuda.memory_reserved() / 1024**3\n            }\n        return {\"allocated\": 0.0, \"reserved\": 0.0}\n\nclass UltraAggressiveRepetitionController:\n    \"\"\"Business document repetition detection and cleanup\"\"\"\n    \n    def __init__(self, word_threshold: float = 0.15, phrase_threshold: int = 2):\n        self.word_threshold = word_threshold\n        self.phrase_threshold = phrase_threshold\n        \n        # Business document specific repetition patterns\n        self.toxic_patterns = [\n            r\"THANK YOU FOR SHOPPING WITH US[^.]*\",\n            r\"All prices include GST where applicable[^.]*\",\n            r\"applicable\\.\\s*applicable\\.\",\n            r\"GST where applicable[^.]*applicable\",\n            r\"\\\\+[a-zA-Z]*\\{[^}]*\\}\",  # LaTeX artifacts\n            r\"\\(\\s*\\)\",  # Empty parentheses\n            r\"[.-]\\s*THANK YOU\",\n        ]\n    \n    def clean_response(self, response: str) -> str:\n        \"\"\"Clean business document extraction response\"\"\"\n        if not response or len(response.strip()) == 0:\n            return \"\"\n        \n        # Remove toxic business document patterns\n        response = self._remove_business_patterns(response)\n        \n        # Remove repetitive words and phrases\n        response = self._remove_word_repetition(response)\n        response = self._remove_phrase_repetition(response)\n        \n        # Clean artifacts\n        response = re.sub(r'\\s+', ' ', response)\n        response = re.sub(r'[.]{2,}', '.', response)\n        response = re.sub(r'[!]{2,}', '!', response)\n        \n        return response.strip()\n    \n    def _remove_business_patterns(self, text: str) -> str:\n        \"\"\"Remove business document specific repetitive patterns\"\"\"\n        for pattern in self.toxic_patterns:\n            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n        \n        # Remove excessive \"applicable\" repetition\n        text = re.sub(r'(applicable\\.\\s*){2,}', 'applicable. ', text, flags=re.IGNORECASE)\n        \n        return text\n    \n    def _remove_word_repetition(self, text: str) -> str:\n        \"\"\"Remove word repetition in business documents\"\"\"\n        # Remove consecutive identical words\n        text = re.sub(r'\\b(\\w+)(\\s+\\1){1,}', r'\\1', text, flags=re.IGNORECASE)\n        \n        return text\n    \n    def _remove_phrase_repetition(self, text: str) -> str:\n        \"\"\"Remove phrase repetition\"\"\"\n        for phrase_length in range(2, 7):\n            pattern = r'\\b((?:\\w+\\s+){' + str(phrase_length-1) + r'}\\w+)(\\s+\\1){1,}'\n            text = re.sub(pattern, r'\\1', text, flags=re.IGNORECASE)\n        \n        return text\n\nclass YAMLExtractionAnalyzer:\n    \"\"\"Analyzer for YAML extraction results\"\"\"\n    \n    @staticmethod\n    def analyze(response: str, img_name: str) -> Dict[str, Any]:\n        \"\"\"Analyze YAML extraction results with consistent format\"\"\"\n        response_clean = response.strip()\n        \n        # Detect YAML format\n        is_yaml = bool(re.search(r'(store_name:|date:|total:)', response_clean, re.IGNORECASE))\n        \n        # Extract data from YAML or text\n        store_match = re.search(r'store_name:\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)\n        date_match = re.search(r'date:\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)  \n        total_match = re.search(r'total:\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)\n        \n        # Fallback detection for non-YAML responses\n        if not store_match:\n            store_match = re.search(r'(spotlight|store)', response_clean, re.IGNORECASE)\n        if not date_match:\n            date_match = re.search(r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}', response_clean)\n        if not total_match:\n            total_match = re.search(r'(\\$\\d+\\.\\d{2}|\\$\\d+)', response_clean)\n        \n        has_store = bool(store_match)\n        has_date = bool(date_match)\n        has_total = bool(total_match)\n        \n        extraction_score = sum([has_store, has_date, has_total])\n        \n        return {\n            \"img_name\": img_name,\n            \"response\": response_clean,\n            \"is_yaml\": is_yaml,\n            \"has_store\": has_store,\n            \"has_date\": has_date,\n            \"has_total\": has_total,\n            \"extraction_score\": extraction_score,\n            \"successful\": extraction_score >= 2  # At least 2/3 fields\n        }\n\nclass DatasetManager:\n    \"\"\"Dataset verification and management\"\"\"\n    \n    def __init__(self, datasets_path: str = \"datasets\"):\n        self.datasets_path = Path(datasets_path)\n    \n    def verify_images(self, test_images: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n        \"\"\"Verify that test images exist and return verified list\"\"\"\n        verified_images = []\n        \n        for img_name, doc_type in test_images:\n            img_path = self.datasets_path / img_name\n            if img_path.exists():\n                verified_images.append((img_name, doc_type))\n        \n        return verified_images\n    \n    def print_verification_report(self, test_images: List[Tuple[str, str]], verified_images: List[Tuple[str, str]]):\n        \"\"\"Print dataset verification report\"\"\"\n        print(\"üìä DATASET VERIFICATION\")\n        print(\"=\" * 50)\n        \n        for img_name, doc_type in test_images:\n            img_path = self.datasets_path / img_name\n            if img_path.exists():\n                print(f\"   ‚úÖ {img_name:<12} ‚Üí {doc_type}\")\n            else:\n                print(f\"   ‚ùå {img_name:<12} ‚Üí {doc_type} (MISSING)\")\n        \n        print(f\"\\nüìã Dataset Summary:\")\n        print(f\"   - Expected: {len(test_images)} documents\")\n        print(f\"   - Found: {len(verified_images)} documents\")\n        print(f\"   - Missing: {len(test_images) - len(verified_images)} documents\")\n        \n        if len(verified_images) == 0:\n            print(\"‚ùå No test images found! Check datasets/ directory\")\n            raise FileNotFoundError(\"No test images found\")\n        elif len(verified_images) < len(test_images):\n            print(\"‚ö†Ô∏è Some test images missing but proceeding with available images\")\n        else:\n            print(\"‚úÖ All test images found\")\n\n# Initialize global utilities\nmemory_manager = MemoryManager()\nrepetition_controller = UltraAggressiveRepetitionController()\nyaml_analyzer = YAMLExtractionAnalyzer()\ndataset_manager = DatasetManager()\n\nprint(\"‚úÖ Modular classes initialized:\")\nprint(\"   - MemoryManager for GPU cleanup\")\nprint(\"   - UltraAggressiveRepetitionController for text cleanup\")\nprint(\"   - YAMLExtractionAnalyzer for results analysis\")\nprint(\"   - DatasetManager for image verification\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Dataset Verification\n# Use the modular DatasetManager class\n\nverified_extraction_images = dataset_manager.verify_images(CONFIG[\"test_images\"])\ndataset_manager.print_verification_report(CONFIG[\"test_images\"], verified_extraction_images)\n\nprint(f\"\\nüî¨ YAML Extraction Prompt:\")\nprint(f\"   {CONFIG['extraction_prompt'][:60]}...\")\nprint(f\"\\nüìã Ready for model testing\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model Loading Classes\nclass LlamaModelLoader:\n    \"\"\"Modular Llama model loader with validation\"\"\"\n    \n    @staticmethod\n    def load_model(model_path: str, enable_quantization: bool = True):\n        \"\"\"Load Llama model with proper configuration\"\"\"\n        from transformers import AutoProcessor, MllamaForConditionalGeneration\n        from transformers import BitsAndBytesConfig\n        \n        processor = AutoProcessor.from_pretrained(\n            model_path, trust_remote_code=True, local_files_only=True\n        )\n        \n        model_kwargs = {\n            \"torch_dtype\": torch.float16,\n            \"local_files_only\": True\n        }\n        \n        if enable_quantization:\n            quantization_config = BitsAndBytesConfig(\n                load_in_8bit=True,\n                llm_int8_enable_fp32_cpu_offload=True,\n                llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n            )\n            model_kwargs[\"quantization_config\"] = quantization_config\n        \n        model = MllamaForConditionalGeneration.from_pretrained(\n            model_path, **model_kwargs\n        ).eval()\n        \n        return model, processor\n    \n    @staticmethod\n    def run_inference(model, processor, prompt: str, image, max_new_tokens: int = 64):\n        \"\"\"Run inference with proper device handling\"\"\"\n        inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n        device = next(model.parameters()).device\n        if device.type != \"cpu\":\n            device_target = str(device).split(\":\")[0]\n            inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=False,\n                pad_token_id=processor.tokenizer.eos_token_id,\n                eos_token_id=processor.tokenizer.eos_token_id,\n                use_cache=True,\n            )\n        \n        raw_response = processor.decode(\n            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n            skip_special_tokens=True\n        )\n        \n        # Cleanup tensors immediately\n        del inputs, outputs\n        \n        return raw_response\n\nclass InternVLModelLoader:\n    \"\"\"Modular InternVL model loader with validation\"\"\"\n    \n    @staticmethod\n    def load_model(model_path: str, enable_quantization: bool = True):\n        \"\"\"Load InternVL model with proper configuration\"\"\"\n        from transformers import AutoModel, AutoTokenizer\n        \n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, local_files_only=True\n        )\n        \n        model_kwargs = {\n            \"trust_remote_code\": True,\n            \"torch_dtype\": torch.bfloat16,\n            \"local_files_only\": True\n        }\n        \n        if enable_quantization:\n            model_kwargs[\"load_in_8bit\"] = True\n        \n        model = AutoModel.from_pretrained(\n            model_path, **model_kwargs\n        ).eval()\n        \n        return model, tokenizer\n    \n    @staticmethod\n    def run_inference(model, tokenizer, prompt: str, image, max_new_tokens: int = 64):\n        \"\"\"Run inference with proper image preprocessing\"\"\"\n        import torchvision.transforms as T\n        from torchvision.transforms.functional import InterpolationMode\n        \n        transform = T.Compose([\n            T.Resize((448, 448), interpolation=InterpolationMode.BICUBIC),\n            T.ToTensor(),\n            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n        ])\n        \n        pixel_values = transform(image).unsqueeze(0)\n        if torch.cuda.is_available():\n            pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n        \n        raw_response = model.chat(\n            tokenizer=tokenizer,\n            pixel_values=pixel_values,\n            question=prompt,\n            generation_config={\"max_new_tokens\": max_new_tokens, \"do_sample\": False}\n        )\n        \n        if isinstance(raw_response, tuple):\n            raw_response = raw_response[0]\n        \n        # Cleanup tensors immediately\n        del pixel_values\n        \n        return raw_response\n\ndef validate_model(model_loader_class, model_path: str, config: Dict) -> Tuple[bool, Optional[Any], Optional[Any], float]:\n    \"\"\"Generic model validation function\"\"\"\n    memory_manager.cleanup_gpu_memory()\n    model_start_time = time.time()\n    \n    try:\n        print(f\"Loading model from {model_path}...\")\n        \n        model, processor_or_tokenizer = model_loader_class.load_model(\n            model_path, config[\"enable_quantization\"]\n        )\n        \n        model_load_time = time.time() - model_start_time\n        print(f\"‚úÖ Model loaded in {model_load_time:.1f}s\")\n        \n        # Test image14.png first to validate\n        print(f\"\\nüîç Testing image14.png for validation...\")\n        img_path = dataset_manager.datasets_path / \"image14.png\"\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        inference_start = time.time()\n        \n        raw_response = model_loader_class.run_inference(\n            model, processor_or_tokenizer, config[\"extraction_prompt\"], \n            image, config[\"max_new_tokens\"]\n        )\n        \n        inference_time = time.time() - inference_start\n        cleaned_response = repetition_controller.clean_response(raw_response)\n        validation_analysis = yaml_analyzer.analyze(cleaned_response, \"image14.png\")\n        \n        print(f\"   Validation result: {validation_analysis['extraction_score']}/3 fields extracted\")\n        print(f\"   YAML format: {'‚úÖ' if validation_analysis['is_yaml'] else '‚ùå'}\")\n        print(f\"   Inference time: {inference_time:.1f}s\")\n        \n        if validation_analysis[\"successful\"]:\n            print(f\"‚úÖ Validation passed - model working\")\n            return True, model, processor_or_tokenizer, model_load_time\n        else:\n            print(f\"‚ùå Validation failed - check prompts and model\")\n            del model, processor_or_tokenizer\n            memory_manager.cleanup_gpu_memory()\n            return False, None, None, model_load_time\n            \n    except Exception as e:\n        print(f\"‚ùå Model failed to load: {str(e)[:100]}...\")\n        memory_manager.cleanup_gpu_memory()\n        return False, None, None, 0.0\n\nprint(\"‚úÖ Model loader classes defined:\")\nprint(\"   - LlamaModelLoader with validation\")\nprint(\"   - InternVLModelLoader with validation\")\nprint(\"   - validate_model() generic function\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Llama Model\nprint(\"üî¨ TESTING LLAMA MODEL\")\nprint(\"=\" * 50)\n\n# Initialize results storage\nextraction_results = {\n    \"llama\": {\"documents\": [], \"successful\": 0, \"total_time\": 0},\n    \"internvl\": {\"documents\": [], \"successful\": 0, \"total_time\": 0}\n}\n\n# Use modular validation function\nllama_valid, llama_model, llama_processor, llama_load_time = validate_model(\n    LlamaModelLoader, \n    CONFIG[\"model_paths\"][\"llama\"], \n    CONFIG\n)\n\nif llama_valid:\n    print(\"‚úÖ Llama model ready for full testing\")\n    # Store for next cell\n    globals()['llama_model'] = llama_model\n    globals()['llama_processor'] = llama_processor\n    globals()['llama_load_time'] = llama_load_time\nelse:\n    print(\"‚ùå Llama model validation failed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test InternVL Model  \nprint(\"üî¨ TESTING INTERNVL MODEL\")\nprint(\"=\" * 50)\n\n# Use modular validation function\ninternvl_valid, internvl_model, internvl_tokenizer, internvl_load_time = validate_model(\n    InternVLModelLoader,\n    CONFIG[\"model_paths\"][\"internvl\"],\n    CONFIG\n)\n\nif internvl_valid:\n    print(\"‚úÖ InternVL model ready for full testing\")\n    # Store for next cell\n    globals()['internvl_model'] = internvl_model\n    globals()['internvl_tokenizer'] = internvl_tokenizer\n    globals()['internvl_load_time'] = internvl_load_time\nelse:\n    print(\"‚ùå InternVL model validation failed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run Full Extraction Test - Llama\nif 'llama_model' in globals() and llama_model is not None:\n    print(\"üîç FULL EXTRACTION TEST - LLAMA\")\n    print(\"=\" * 50)\n    \n    total_inference_time = 0\n    \n    for i, (img_name, doc_type) in enumerate(verified_extraction_images, 1):\n        try:\n            img_path = dataset_manager.datasets_path / img_name\n            image = Image.open(img_path).convert(\"RGB\")\n            \n            inference_start = time.time()\n            \n            raw_response = LlamaModelLoader.run_inference(\n                llama_model, llama_processor, CONFIG[\"extraction_prompt\"],\n                image, CONFIG[\"max_new_tokens\"]\n            )\n            \n            inference_time = time.time() - inference_start\n            total_inference_time += inference_time\n            \n            cleaned_response = repetition_controller.clean_response(raw_response)\n            analysis = yaml_analyzer.analyze(cleaned_response, img_name)\n            analysis[\"inference_time\"] = inference_time\n            analysis[\"doc_type\"] = doc_type\n            \n            extraction_results[\"llama\"][\"documents\"].append(analysis)\n            \n            if analysis[\"successful\"]:\n                extraction_results[\"llama\"][\"successful\"] += 1\n            \n            # Consistent output format as requested\n            status = \"‚úÖ\" if analysis[\"successful\"] else \"‚ùå\"\n            yaml_status = \"Y\" if analysis[\"is_yaml\"] else \"T\"\n            print(f\"   {i:2d}. {img_name:<12} {status} {inference_time:.1f}s | {yaml_status} | {analysis['extraction_score']}/3\")\n            \n            # Immediate tensor cleanup - minimizing memory footprint\n            del image\n            \n            # Periodic GPU cleanup every 3 images\n            if i % 3 == 0:\n                memory_manager.cleanup_gpu_memory()\n            \n        except Exception as e:\n            print(f\"   {i:2d}. {img_name:<12} ‚ùå Error: {str(e)[:30]}...\")\n    \n    extraction_results[\"llama\"][\"total_time\"] = total_inference_time\n    extraction_results[\"llama\"][\"avg_time\"] = total_inference_time / len(verified_extraction_images)\n    \n    print(f\"\\nüìä Llama Results:\")\n    print(f\"   Success rate: {extraction_results['llama']['successful']}/{len(verified_extraction_images)}\")\n    print(f\"   Average time: {extraction_results['llama']['avg_time']:.1f}s per document\")\n    \n    # Cleanup Llama model to free memory for InternVL\n    del llama_model, llama_processor\n    memory_manager.cleanup_gpu_memory()\n    \nelse:\n    print(\"‚ö†Ô∏è Llama model not available - skipping full test\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run Full Extraction Test - InternVL\nif 'internvl_model' in globals() and internvl_model is not None:\n    print(\"üîç FULL EXTRACTION TEST - INTERNVL\")\n    print(\"=\" * 50)\n    \n    total_inference_time = 0\n    \n    for i, (img_name, doc_type) in enumerate(verified_extraction_images, 1):\n        try:\n            img_path = dataset_manager.datasets_path / img_name\n            image = Image.open(img_path).convert(\"RGB\")\n            \n            inference_start = time.time()\n            \n            raw_response = InternVLModelLoader.run_inference(\n                internvl_model, internvl_tokenizer, CONFIG[\"extraction_prompt\"],\n                image, CONFIG[\"max_new_tokens\"]\n            )\n            \n            inference_time = time.time() - inference_start\n            total_inference_time += inference_time\n            \n            cleaned_response = repetition_controller.clean_response(raw_response)\n            analysis = yaml_analyzer.analyze(cleaned_response, img_name)\n            analysis[\"inference_time\"] = inference_time\n            analysis[\"doc_type\"] = doc_type\n            \n            extraction_results[\"internvl\"][\"documents\"].append(analysis)\n            \n            if analysis[\"successful\"]:\n                extraction_results[\"internvl\"][\"successful\"] += 1\n            \n            # Consistent output format as requested\n            status = \"‚úÖ\" if analysis[\"successful\"] else \"‚ùå\"\n            yaml_status = \"Y\" if analysis[\"is_yaml\"] else \"T\"\n            print(f\"   {i:2d}. {img_name:<12} {status} {inference_time:.1f}s | {yaml_status} | {analysis['extraction_score']}/3\")\n            \n            # Immediate tensor cleanup - minimizing memory footprint\n            del image\n            \n            # Periodic GPU cleanup every 3 images\n            if i % 3 == 0:\n                memory_manager.cleanup_gpu_memory()\n            \n        except Exception as e:\n            print(f\"   {i:2d}. {img_name:<12} ‚ùå Error: {str(e)[:30]}...\")\n    \n    extraction_results[\"internvl\"][\"total_time\"] = total_inference_time\n    extraction_results[\"internvl\"][\"avg_time\"] = total_inference_time / len(verified_extraction_images)\n    \n    print(f\"\\nüìä InternVL Results:\")\n    print(f\"   Success rate: {extraction_results['internvl']['successful']}/{len(verified_extraction_images)}\")\n    print(f\"   Average time: {extraction_results['internvl']['avg_time']:.1f}s per document\")\n    \n    # Cleanup InternVL model \n    del internvl_model, internvl_tokenizer\n    memory_manager.cleanup_gpu_memory()\n    \nelse:\n    print(\"‚ö†Ô∏è InternVL model not available - skipping full test\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final Comparison and Recommendation\nclass ResultsAnalyzer:\n    \"\"\"Modular results analysis and comparison\"\"\"\n    \n    @staticmethod\n    def print_final_comparison(extraction_results: Dict, verified_images: List):\n        \"\"\"Print final comparison between models\"\"\"\n        print(f\"\\n{'=' * 80}\")\n        print(\"üèÜ FINAL RECOMMENDATION: BEST MODEL FOR INFORMATION EXTRACTION\")\n        print(f\"{'=' * 80}\")\n        \n        # Compare both models' performance\n        llama_success = 0\n        llama_total = 0\n        llama_avg_time = 0\n        internvl_success = 0\n        internvl_total = 0\n        internvl_avg_time = 0\n        \n        if extraction_results[\"llama\"][\"documents\"]:\n            llama_total = len(extraction_results[\"llama\"][\"documents\"])\n            llama_success = extraction_results[\"llama\"][\"successful\"]\n            llama_avg_time = extraction_results[\"llama\"][\"avg_time\"]\n        \n        if extraction_results[\"internvl\"][\"documents\"]:\n            internvl_total = len(extraction_results[\"internvl\"][\"documents\"])\n            internvl_success = extraction_results[\"internvl\"][\"successful\"]\n            internvl_avg_time = extraction_results[\"internvl\"][\"avg_time\"]\n        \n        print(f\"üìä INFORMATION EXTRACTION COMPARISON:\")\n        print(f\"{'Model':<12} {'Success Rate':<15} {'Avg Time':<12} {'Best For'}\")\n        print(\"-\" * 60)\n        \n        if llama_total > 0:\n            llama_rate = llama_success / llama_total * 100\n            print(f\"{'LLAMA':<12} {llama_rate:.1f}% ({llama_success}/{llama_total}){'':<5} {llama_avg_time:.1f}s{'':<7} Large context\")\n        \n        if internvl_total > 0:\n            internvl_rate = internvl_success / internvl_total * 100\n            print(f\"{'INTERNVL':<12} {internvl_rate:.1f}% ({internvl_success}/{internvl_total}){'':<5} {internvl_avg_time:.1f}s{'':<7} Production speed\")\n        \n        # Make recommendation\n        if internvl_total > 0 and llama_total > 0:\n            internvl_rate = internvl_success / internvl_total * 100\n            llama_rate = llama_success / llama_total * 100\n            \n            if internvl_rate > llama_rate:\n                recommended = \"INTERNVL\"\n                reason = f\"Higher success rate ({internvl_rate:.1f}% vs {llama_rate:.1f}%) and faster inference\"\n            elif llama_rate > internvl_rate:\n                recommended = \"LLAMA\"\n                reason = f\"Higher success rate ({llama_rate:.1f}% vs {internvl_rate:.1f}%)\"\n            else:\n                recommended = \"INTERNVL\"\n                reason = f\"Equal success rate but {internvl_avg_time/llama_avg_time:.1f}x faster inference\"\n            \n            print(f\"\\nü•á RECOMMENDED FOR INFORMATION EXTRACTION: {recommended}\")\n            print(f\"   Reason: {reason}\")\n            print(f\"   Use case: Business document processing (receipts, invoices, statements)\")\n        elif internvl_total > 0:\n            print(f\"\\nü•á RECOMMENDED: INTERNVL (only model tested successfully)\")\n        elif llama_total > 0:\n            print(f\"\\nü•á RECOMMENDED: LLAMA (only model tested successfully)\")\n        else:\n            print(f\"\\n‚ö†Ô∏è No successful tests - investigate model loading issues\")\n        \n        print(f\"\\n‚úÖ COMPLETE: Information extraction performance comparison finished!\")\n        print(f\"üìã This answers the user's question about best model for their information extraction job\")\n\n# Use the modular analyzer\nresults_analyzer = ResultsAnalyzer()\nresults_analyzer.print_final_comparison(extraction_results, verified_extraction_images)\n\n# Show the YAML prompt being used\nprint(f\"\\nüî¨ YAML PROMPT USED:\")\nprint(f\"{'='*50}\")\nprint(CONFIG[\"extraction_prompt\"])\nprint(f\"{'='*50}\")\nprint(f\"‚úÖ Confirmed: Using structured YAML prompts (NOT JSON)\")"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèõÔ∏è COMPREHENSIVE TAXPAYER DOCUMENT CLASSIFICATION TEST\n",
      "üß™ Using IMPROVED research-based prompting techniques\n",
      "================================================================================\n",
      "üìä Testing 11 documents with HUMAN ANNOTATIONS:\n",
      "   1. image14.png  ‚Üí TAX_INVOICE\n",
      "   2. image65.png  ‚Üí TAX_INVOICE\n",
      "   3. image71.png  ‚Üí TAX_INVOICE\n",
      "   4. image74.png  ‚Üí TAX_INVOICE\n",
      "   5. image205.png ‚Üí FUEL_RECEIPT\n",
      "   6. image23.png  ‚Üí TAX_INVOICE\n",
      "   7. image45.png  ‚Üí TAX_INVOICE\n",
      "   8. image1.png   ‚Üí BANK_STATEMENT\n",
      "   9. image203.png ‚Üí BANK_STATEMENT\n",
      "   10. image204.png ‚Üí FUEL_RECEIPT\n",
      "   11. image206.png ‚Üí OTHER\n",
      "\n",
      "üß™ Available classification prompts:\n",
      "   - json_format: 322 chars\n",
      "   - simple_format: 280 chars\n",
      "   - ultra_simple: 23 chars\n",
      "\n",
      "============================================================\n",
      "üîç TESTING LLAMA WITH IMPROVED PROMPTING\n",
      "============================================================\n",
      "üßπ Pre-cleanup for llama...\n",
      "   GPU Memory: 0.03GB allocated, 0.03GB reserved\n",
      "üìù Using SIMPLE FORMAT prompt (research-based)\n",
      "Loading llama model from /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision...\n",
      "üîÑ Loading Llama (will use ~6-8GB GPU memory)...\n",
      "‚úÖ Using 8-bit quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ace2e7fc2974c43b4c76e6c3f487231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ llama model loaded in 5.2s\n",
      "   GPU Memory: 10.52GB allocated, 10.58GB reserved\n",
      "\n",
      "üìÑ Document 1/11: image14.png (expected: TAX_INVOICE)\n",
      "   ‚ùå FUEL_RECEIPT (5.7s)\n",
      "\n",
      "üìÑ Document 2/11: image65.png (expected: TAX_INVOICE)\n",
      "   ‚ùå FUEL_RECEIPT (5.8s)\n",
      "\n",
      "üìÑ Document 3/11: image71.png (expected: TAX_INVOICE)\n",
      "   ‚ùå FUEL_RECEIPT (5.8s)\n",
      "\n",
      "üìÑ Document 4/11: image74.png (expected: TAX_INVOICE)\n",
      "   ‚ùå FUEL_RECEIPT (5.7s)\n",
      "\n",
      "üìÑ Document 5/11: image205.png (expected: FUEL_RECEIPT)\n",
      "   ‚úÖ FUEL_RECEIPT (5.7s)\n",
      "\n",
      "üìÑ Document 6/11: image23.png (expected: TAX_INVOICE)\n",
      "   ‚ùå FUEL_RECEIPT (5.8s)\n",
      "\n",
      "üìÑ Document 7/11: image45.png (expected: TAX_INVOICE)\n",
      "   ‚ùå FUEL_RECEIPT (5.7s)\n",
      "\n",
      "üìÑ Document 8/11: image1.png (expected: BANK_STATEMENT)\n",
      "   ‚úÖ BANK_STATEMENT (5.7s)\n",
      "\n",
      "üìÑ Document 9/11: image203.png (expected: BANK_STATEMENT)\n",
      "   ‚ùå FUEL_RECEIPT (5.6s)\n",
      "\n",
      "üìÑ Document 10/11: image204.png (expected: FUEL_RECEIPT)\n",
      "   ‚úÖ FUEL_RECEIPT (5.8s)\n",
      "\n",
      "üìÑ Document 11/11: image206.png (expected: OTHER)\n",
      "   ‚ùå UNKNOWN (5.5s)\n",
      "\n",
      "üßπ Cleaning up llama...\n",
      "   GPU Memory: 0.03GB allocated, 0.03GB reserved\n",
      "\n",
      "üìä LLAMA SUMMARY:\n",
      "   Accuracy: 27.3% (3/11)\n",
      "   Total Time: 69.8s\n",
      "   Avg Time/Doc: 5.7s\n",
      "\n",
      "============================================================\n",
      "üîç TESTING INTERNVL WITH IMPROVED PROMPTING\n",
      "============================================================\n",
      "üßπ Pre-cleanup for internvl...\n",
      "   GPU Memory: 0.03GB allocated, 0.03GB reserved\n",
      "üìù Using JSON FORMAT prompt\n",
      "Loading internvl model from /home/jovyan/nfs_share/models/InternVL3-8B...\n",
      "üîÑ Loading InternVL (will use ~4-6GB GPU memory)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 8-bit quantization enabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829b18e726d94b9abf74bd455cb5b631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ internvl model loaded in 3.5s\n",
      "   GPU Memory: 8.46GB allocated, 8.60GB reserved\n",
      "\n",
      "üìÑ Document 1/11: image14.png (expected: TAX_INVOICE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TAX_INVOICE (1.5s)\n",
      "      Raw: ```json\n",
      "{\n",
      "  \"document_type\": \"TAX_INVOICE\"\n",
      "}\n",
      "```\n",
      "\n",
      "üìÑ Document 2/11: image65.png (expected: TAX_INVOICE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ùå BUSINESS_RECEIPT (1.6s)\n",
      "      Raw: ```json\n",
      "{\n",
      "  \"document_type\": \"BUSINESS_RECEIPT\"\n",
      "}\n",
      "```\n",
      "\n",
      "üìÑ Document 3/11: image71.png (expected: TAX_INVOICE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TAX_INVOICE (1.5s)\n",
      "      Raw: ```json\n",
      "{\n",
      "  \"document_type\": \"TAX_INVOICE\"\n",
      "}\n",
      "```\n",
      "\n",
      "üìÑ Document 4/11: image74.png (expected: TAX_INVOICE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ùå MEAL_RECEIPT (1.6s)\n",
      "      Raw: ```json\n",
      "{\n",
      "  \"document_type\": \"MEAL_RECEIPT\"\n",
      "}\n",
      "```\n",
      "\n",
      "üìÑ Document 5/11: image205.png (expected: FUEL_RECEIPT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ FUEL_RECEIPT (1.6s)\n",
      "      Raw: ```json\n",
      "{\n",
      "  \"document_type\": \"FUEL_RECEIPT\"\n",
      "}\n",
      "```\n",
      "\n",
      "üìÑ Document 6/11: image23.png (expected: TAX_INVOICE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ùå BUSINESS_RECEIPT (1.6s)\n",
      "      Raw: ```json\n",
      "{\n",
      "  \"document_type\": \"BUSINESS_RECEIPT\"\n",
      "}\n",
      "```\n",
      "\n",
      "üìÑ Document 7/11: image45.png (expected: TAX_INVOICE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ùå BUSINESS_RECEIPT (1.6s)\n",
      "      Raw: ```json\n",
      "{\n",
      "  \"document_type\": \"BUSINESS_RECEIPT\"\n",
      "}\n",
      "```\n",
      "\n",
      "üìÑ Document 8/11: image1.png (expected: BANK_STATEMENT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ BANK_STATEMENT (1.5s)\n",
      "      Raw: ```json\n",
      "{\n",
      "  \"document_type\": \"BANK_STATEMENT\"\n",
      "}\n",
      "```\n",
      "\n",
      "üìÑ Document 9/11: image203.png (expected: BANK_STATEMENT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ BANK_STATEMENT (1.5s)\n",
      "      Raw: ```json\n",
      "{\n",
      "  \"document_type\": \"BANK_STATEMENT\"\n",
      "}\n",
      "```\n",
      "\n",
      "üìÑ Document 10/11: image204.png (expected: FUEL_RECEIPT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ùå TAX_INVOICE (1.5s)\n",
      "      Raw: ```json\n",
      "{\n",
      "  \"document_type\": \"TAX_INVOICE\"\n",
      "}\n",
      "```\n",
      "\n",
      "üìÑ Document 11/11: image206.png (expected: OTHER)\n",
      "   ‚úÖ OTHER (1.2s)\n",
      "      Raw: ```json\n",
      "{\n",
      "  \"document_type\": \"OTHER\"\n",
      "}\n",
      "```\n",
      "\n",
      "üßπ Cleaning up internvl...\n",
      "   GPU Memory: 0.03GB allocated, 0.03GB reserved\n",
      "\n",
      "üìä INTERNVL SUMMARY:\n",
      "   Accuracy: 54.5% (6/11)\n",
      "   Total Time: 21.1s\n",
      "   Avg Time/Doc: 1.5s\n",
      "\n",
      "================================================================================\n",
      "üèÜ IMPROVED PROMPTING ACCURACY ANALYSIS\n",
      "================================================================================\n",
      "Image      Expected   Llama      ‚úì  InternVL   ‚úì\n",
      "---------- ---------- ---------- -  ---------- -\n",
      "image14.   TAX_INVO   FUEL_REC   ‚ùå  TAX_INVO   ‚úÖ\n",
      "image65.   TAX_INVO   FUEL_REC   ‚ùå  BUSINESS   ‚ùå\n",
      "image71.   TAX_INVO   FUEL_REC   ‚ùå  TAX_INVO   ‚úÖ\n",
      "image74.   TAX_INVO   FUEL_REC   ‚ùå  MEAL_REC   ‚ùå\n",
      "image205   FUEL_REC   FUEL_REC   ‚úÖ  FUEL_REC   ‚úÖ\n",
      "image23.   TAX_INVO   FUEL_REC   ‚ùå  BUSINESS   ‚ùå\n",
      "image45.   TAX_INVO   FUEL_REC   ‚ùå  BUSINESS   ‚ùå\n",
      "image1.p   BANK_STA   BANK_STA   ‚úÖ  BANK_STA   ‚úÖ\n",
      "image203   BANK_STA   FUEL_REC   ‚ùå  BANK_STA   ‚úÖ\n",
      "image204   FUEL_REC   FUEL_REC   ‚úÖ  TAX_INVO   ‚ùå\n",
      "image206   OTHER      UNKNOWN    ‚ùå  OTHER      ‚úÖ\n",
      "\n",
      "üìà IMPROVED PROMPTING RESULTS:\n",
      "LLAMA: 27.3% accuracy, 5.73s/doc average\n",
      "INTERNVL: 54.5% accuracy, 1.51s/doc average\n",
      "\n",
      "üß† Final Memory State:\n",
      "   GPU Memory: 0.03GB allocated, 0.03GB reserved\n",
      "\n",
      "‚úÖ Improved prompting classification completed!\n",
      "üìã Compare with previous results to see improvement\n"
     ]
    }
   ],
   "source": [
    "# Multi-Document Classification - Improved Llama 3.2 Vision Prompting\n",
    "print(\"üèõÔ∏è COMPREHENSIVE TAXPAYER DOCUMENT CLASSIFICATION TEST\")\n",
    "print(\"üß™ Using IMPROVED research-based prompting techniques\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "# Memory management function\n",
    "def cleanup_gpu_memory():\n",
    "    \"\"\"Aggressive GPU memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"   GPU Memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved\")\n",
    "\n",
    "# Standard document types\n",
    "DOCUMENT_TYPES = [\n",
    "    \"FUEL_RECEIPT\", \"BUSINESS_RECEIPT\", \"TAX_INVOICE\", \"BANK_STATEMENT\",\n",
    "    \"MEAL_RECEIPT\", \"ACCOMMODATION_RECEIPT\", \"TRAVEL_DOCUMENT\", \n",
    "    \"PARKING_TOLL_RECEIPT\", \"PROFESSIONAL_SERVICES\", \"EQUIPMENT_SUPPLIES\", \"OTHER\"\n",
    "]\n",
    "\n",
    "# Human annotated ground truth\n",
    "test_images_with_annotations = [\n",
    "    (\"image14.png\", \"TAX_INVOICE\"),\n",
    "    (\"image65.png\", \"TAX_INVOICE\"),\n",
    "    (\"image71.png\", \"TAX_INVOICE\"),\n",
    "    (\"image74.png\", \"TAX_INVOICE\"),\n",
    "    (\"image205.png\", \"FUEL_RECEIPT\"),\n",
    "    (\"image23.png\", \"TAX_INVOICE\"),\n",
    "    (\"image45.png\", \"TAX_INVOICE\"),\n",
    "    (\"image1.png\", \"BANK_STATEMENT\"),\n",
    "    (\"image203.png\", \"BANK_STATEMENT\"),\n",
    "    (\"image204.png\", \"FUEL_RECEIPT\"),\n",
    "    (\"image206.png\", \"OTHER\"),\n",
    "]\n",
    "\n",
    "# Verify test images exist\n",
    "datasets_path = Path(\"datasets\")\n",
    "verified_test_images = []\n",
    "verified_ground_truth = {}\n",
    "\n",
    "for img_name, annotation in test_images_with_annotations:\n",
    "    img_path = datasets_path / img_name\n",
    "    if img_path.exists():\n",
    "        verified_test_images.append(img_name)\n",
    "        verified_ground_truth[img_name] = annotation\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Missing: {img_name} (expected: {annotation})\")\n",
    "\n",
    "print(f\"üìä Testing {len(verified_test_images)} documents with HUMAN ANNOTATIONS:\")\n",
    "for i, img_name in enumerate(verified_test_images, 1):\n",
    "    annotation = verified_ground_truth[img_name]\n",
    "    print(f\"   {i}. {img_name:<12} ‚Üí {annotation}\")\n",
    "\n",
    "# IMPROVED classification prompts based on research\n",
    "classification_prompts = {\n",
    "    \"json_format\": f\"\"\"<|image|>Classify this business document in JSON format:\n",
    "{{\n",
    "  \"document_type\": \"\"\n",
    "}}\n",
    "\n",
    "Categories: {', '.join(DOCUMENT_TYPES)}\n",
    "Return only valid JSON, no explanations.\"\"\",\n",
    "    \n",
    "    \"simple_format\": f\"\"\"<|image|>What type of business document is this?\n",
    "\n",
    "Choose from: {', '.join(DOCUMENT_TYPES)}\n",
    "\n",
    "Answer with one category only:\"\"\",\n",
    "    \n",
    "    \"ultra_simple\": \"<|image|>Document type:\",\n",
    "}\n",
    "\n",
    "print(f\"\\nüß™ Available classification prompts:\")\n",
    "for name, prompt in classification_prompts.items():\n",
    "    print(f\"   - {name}: {len(prompt)} chars\")\n",
    "\n",
    "# Results storage with accuracy tracking\n",
    "multi_doc_results = {\n",
    "    \"llama\": {\"classifications\": [], \"times\": [], \"errors\": [], \"correct\": 0, \"total\": 0},\n",
    "    \"internvl\": {\"classifications\": [], \"times\": [], \"errors\": [], \"correct\": 0, \"total\": 0}\n",
    "}\n",
    "\n",
    "# Test both models with IMPROVED prompting\n",
    "for model_name in [\"llama\", \"internvl\"]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"üîç TESTING {model_name.upper()} WITH IMPROVED PROMPTING\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # AGGRESSIVE pre-cleanup before loading model\n",
    "    print(f\"üßπ Pre-cleanup for {model_name}...\")\n",
    "    for var in ['model', 'processor', 'tokenizer', 'inputs', 'outputs', 'pixel_values']:\n",
    "        if var in locals():\n",
    "            del locals()[var]\n",
    "        if var in globals():\n",
    "            del globals()[var]\n",
    "    cleanup_gpu_memory()\n",
    "    \n",
    "    model_start_time = time.time()\n",
    "    \n",
    "    # Select best prompt for model type\n",
    "    if model_name == \"llama\":\n",
    "        # Use simple format to avoid safety triggers\n",
    "        classification_prompt = classification_prompts[\"simple_format\"]\n",
    "        print(f\"üìù Using SIMPLE FORMAT prompt (research-based)\")\n",
    "    else:\n",
    "        # InternVL can handle JSON better\n",
    "        classification_prompt = classification_prompts[\"json_format\"]\n",
    "        print(f\"üìù Using JSON FORMAT prompt\")\n",
    "    \n",
    "    try:\n",
    "        # Load model using ROBUST patterns from cell 3\n",
    "        model_path = CONFIG[\"model_paths\"][model_name]\n",
    "        print(f\"Loading {model_name} model from {model_path}...\")\n",
    "        \n",
    "        if model_name == \"llama\":\n",
    "            print(f\"üîÑ Loading Llama (will use ~6-8GB GPU memory)...\")\n",
    "            \n",
    "            processor = AutoProcessor.from_pretrained(\n",
    "                model_path, trust_remote_code=True, local_files_only=True\n",
    "            )\n",
    "            \n",
    "            model_loading_args = {\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"device_map\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "                \"local_files_only\": True\n",
    "            }\n",
    "            \n",
    "            if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "                try:\n",
    "                    from transformers import BitsAndBytesConfig\n",
    "                    quantization_config = BitsAndBytesConfig(\n",
    "                        load_in_8bit=True,\n",
    "                        llm_int8_enable_fp32_cpu_offload=True,\n",
    "                        llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n",
    "                    )\n",
    "                    model_loading_args[\"quantization_config\"] = quantization_config\n",
    "                    print(\"‚úÖ Using 8-bit quantization\")\n",
    "                except ImportError:\n",
    "                    pass\n",
    "            \n",
    "            model = MllamaForConditionalGeneration.from_pretrained(\n",
    "                model_path, **model_loading_args\n",
    "            ).eval()\n",
    "            \n",
    "        elif model_name == \"internvl\":\n",
    "            print(f\"üîÑ Loading InternVL (will use ~4-6GB GPU memory)...\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_path, trust_remote_code=True, local_files_only=True\n",
    "            )\n",
    "            \n",
    "            model_kwargs = {\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "                \"trust_remote_code\": True,\n",
    "                \"torch_dtype\": torch.bfloat16,\n",
    "                \"local_files_only\": True\n",
    "            }\n",
    "            \n",
    "            if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "                try:\n",
    "                    model_kwargs[\"load_in_8bit\"] = True\n",
    "                    print(\"‚úÖ 8-bit quantization enabled\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            model = AutoModel.from_pretrained(model_path, **model_kwargs).eval()\n",
    "            \n",
    "            if torch.cuda.is_available() and not CONFIG[\"enable_quantization\"]:\n",
    "                model = model.cuda()\n",
    "        \n",
    "        model_load_time = time.time() - model_start_time\n",
    "        print(f\"‚úÖ {model_name} model loaded in {model_load_time:.1f}s\")\n",
    "        cleanup_gpu_memory()\n",
    "        \n",
    "        # Test each document with IMPROVED prompting\n",
    "        for i, img_name in enumerate(verified_test_images, 1):\n",
    "            expected_classification = verified_ground_truth[img_name]\n",
    "            print(f\"\\nüìÑ Document {i}/{len(verified_test_images)}: {img_name} (expected: {expected_classification})\")\n",
    "            \n",
    "            try:\n",
    "                # Load image\n",
    "                img_path = datasets_path / img_name\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "                inference_start = time.time()\n",
    "                \n",
    "                if model_name == \"llama\":\n",
    "                    inputs = processor(text=classification_prompt, images=image, return_tensors=\"pt\")\n",
    "                    device = next(model.parameters()).device\n",
    "                    if device.type != \"cpu\":\n",
    "                        device_target = str(device).split(\":\")[0] if \":\" in str(device) else str(device)\n",
    "                        inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "                    \n",
    "                    # RESEARCH-BASED: Deterministic generation\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model.generate(\n",
    "                            **inputs,\n",
    "                            max_new_tokens=64,  # Short for classification\n",
    "                            do_sample=False,    # Deterministic\n",
    "                            temperature=None,   # Disable temperature\n",
    "                            top_p=None,         # Disable top_p\n",
    "                            top_k=None,         # Disable top_k\n",
    "                            pad_token_id=processor.tokenizer.eos_token_id,\n",
    "                            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                            use_cache=True,\n",
    "                        )\n",
    "                    \n",
    "                    raw_response = processor.decode(\n",
    "                        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "                        skip_special_tokens=True\n",
    "                    )\n",
    "                    \n",
    "                    # Immediate cleanup of inference tensors\n",
    "                    del inputs, outputs\n",
    "                    \n",
    "                elif model_name == \"internvl\":\n",
    "                    transform = T.Compose([\n",
    "                        T.Resize((448, 448), interpolation=InterpolationMode.BICUBIC),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                    ])\n",
    "                    \n",
    "                    pixel_values = transform(image).unsqueeze(0)\n",
    "                    if torch.cuda.is_available():\n",
    "                        pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n",
    "                    \n",
    "                    raw_response = model.chat(\n",
    "                        tokenizer=tokenizer,\n",
    "                        pixel_values=pixel_values,\n",
    "                        question=classification_prompt,\n",
    "                        generation_config={\"max_new_tokens\": 64, \"do_sample\": False}\n",
    "                    )\n",
    "                    \n",
    "                    if isinstance(raw_response, tuple):\n",
    "                        raw_response = raw_response[0]\n",
    "                    \n",
    "                    # Immediate cleanup of inference tensors\n",
    "                    del pixel_values\n",
    "                \n",
    "                inference_time = time.time() - inference_start\n",
    "                \n",
    "                # IMPROVED extraction: Handle JSON and text responses\n",
    "                extracted_classification = \"UNKNOWN\"\n",
    "                response_clean = raw_response.strip()\n",
    "                \n",
    "                # Try JSON extraction first\n",
    "                if response_clean.startswith('{') and response_clean.endswith('}'):\n",
    "                    try:\n",
    "                        json_data = json.loads(response_clean)\n",
    "                        if \"document_type\" in json_data:\n",
    "                            extracted_classification = json_data[\"document_type\"].upper()\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "                \n",
    "                # Fallback to text extraction\n",
    "                if extracted_classification == \"UNKNOWN\":\n",
    "                    response_upper = response_clean.upper()\n",
    "                    for doc_type in DOCUMENT_TYPES:\n",
    "                        if doc_type in response_upper:\n",
    "                            extracted_classification = doc_type\n",
    "                            break\n",
    "                \n",
    "                # Calculate accuracy against human annotation\n",
    "                is_correct = extracted_classification == expected_classification\n",
    "                multi_doc_results[model_name][\"total\"] += 1\n",
    "                if is_correct:\n",
    "                    multi_doc_results[model_name][\"correct\"] += 1\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    \"image\": img_name,\n",
    "                    \"predicted\": extracted_classification,\n",
    "                    \"expected\": expected_classification,\n",
    "                    \"correct\": is_correct,\n",
    "                    \"inference_time\": inference_time,\n",
    "                    \"raw_response\": raw_response[:60] + \"...\" if len(raw_response) > 60 else raw_response\n",
    "                }\n",
    "                \n",
    "                multi_doc_results[model_name][\"classifications\"].append(result)\n",
    "                multi_doc_results[model_name][\"times\"].append(inference_time)\n",
    "                \n",
    "                # Show result\n",
    "                status = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "                print(f\"   {status} {extracted_classification} ({inference_time:.1f}s)\")\n",
    "                if len(raw_response) < 100:\n",
    "                    print(f\"      Raw: {raw_response}\")\n",
    "                \n",
    "                # Periodic memory cleanup every 3 images\n",
    "                if i % 3 == 0:\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                multi_doc_results[model_name][\"errors\"].append({\n",
    "                    \"image\": img_name,\n",
    "                    \"expected\": expected_classification,\n",
    "                    \"error\": str(e)[:100]\n",
    "                })\n",
    "                multi_doc_results[model_name][\"total\"] += 1\n",
    "                print(f\"   ‚ùå ERROR: {str(e)[:60]}...\")\n",
    "        \n",
    "        # AGGRESSIVE cleanup after model testing\n",
    "        print(f\"\\nüßπ Cleaning up {model_name}...\")\n",
    "        del model\n",
    "        if model_name == \"llama\":\n",
    "            del processor\n",
    "        elif model_name == \"internvl\":\n",
    "            del tokenizer\n",
    "        \n",
    "        cleanup_gpu_memory()\n",
    "        \n",
    "        total_time = time.time() - model_start_time\n",
    "        accuracy = multi_doc_results[model_name][\"correct\"] / multi_doc_results[model_name][\"total\"] * 100 if multi_doc_results[model_name][\"total\"] > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüìä {model_name.upper()} SUMMARY:\")\n",
    "        print(f\"   Accuracy: {accuracy:.1f}% ({multi_doc_results[model_name]['correct']}/{multi_doc_results[model_name]['total']})\")\n",
    "        print(f\"   Total Time: {total_time:.1f}s\")\n",
    "        print(f\"   Avg Time/Doc: {sum(multi_doc_results[model_name]['times'])/max(1,len(multi_doc_results[model_name]['times'])):.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {model_name.upper()} FAILED TO LOAD: {str(e)[:100]}...\")\n",
    "        \n",
    "        # Emergency cleanup\n",
    "        for var in ['model', 'processor', 'tokenizer', 'inputs', 'outputs', 'pixel_values']:\n",
    "            if var in locals():\n",
    "                del locals()[var]\n",
    "        cleanup_gpu_memory()\n",
    "        \n",
    "        multi_doc_results[model_name][\"model_error\"] = str(e)\n",
    "\n",
    "# Final Analysis with IMPROVED prompting results\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"üèÜ IMPROVED PROMPTING ACCURACY ANALYSIS\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "# Comparison table\n",
    "comparison_data = []\n",
    "comparison_data.append([\"Image\", \"Expected\", \"Llama\", \"‚úì\", \"InternVL\", \"‚úì\"])\n",
    "comparison_data.append([\"-\" * 10, \"-\" * 10, \"-\" * 10, \"-\", \"-\" * 10, \"-\"])\n",
    "\n",
    "llama_results = {r[\"image\"]: r for r in multi_doc_results[\"llama\"][\"classifications\"]}\n",
    "internvl_results = {r[\"image\"]: r for r in multi_doc_results[\"internvl\"][\"classifications\"]}\n",
    "\n",
    "for img_name in verified_test_images:\n",
    "    expected = verified_ground_truth[img_name]\n",
    "    llama_result = llama_results.get(img_name, {\"predicted\": \"ERROR\", \"correct\": False})\n",
    "    internvl_result = internvl_results.get(img_name, {\"predicted\": \"ERROR\", \"correct\": False})\n",
    "    \n",
    "    comparison_data.append([\n",
    "        img_name[:8],\n",
    "        expected[:8],\n",
    "        llama_result[\"predicted\"][:8],\n",
    "        \"‚úÖ\" if llama_result[\"correct\"] else \"‚ùå\",\n",
    "        internvl_result[\"predicted\"][:8],\n",
    "        \"‚úÖ\" if internvl_result[\"correct\"] else \"‚ùå\"\n",
    "    ])\n",
    "\n",
    "for row in comparison_data:\n",
    "    print(f\"{row[0]:<10} {row[1]:<10} {row[2]:<10} {row[3]:<2} {row[4]:<10} {row[5]}\")\n",
    "\n",
    "# Final statistics with improvement comparison\n",
    "print(f\"\\nüìà IMPROVED PROMPTING RESULTS:\")\n",
    "for model_name in [\"llama\", \"internvl\"]:\n",
    "    if multi_doc_results[model_name][\"total\"] > 0:\n",
    "        accuracy = multi_doc_results[model_name][\"correct\"] / multi_doc_results[model_name][\"total\"] * 100\n",
    "        avg_time = sum(multi_doc_results[model_name][\"times\"]) / len(multi_doc_results[model_name][\"times\"])\n",
    "        print(f\"{model_name.upper()}: {accuracy:.1f}% accuracy, {avg_time:.2f}s/doc average\")\n",
    "\n",
    "# Final memory state\n",
    "print(f\"\\nüß† Final Memory State:\")\n",
    "cleanup_gpu_memory()\n",
    "\n",
    "print(f\"\\n‚úÖ Improved prompting classification completed!\")\n",
    "print(f\"üìã Compare with previous results to see improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Information Extraction Comparison Test - Minimized Memory Footprint\nprint(\"üî¨ INFORMATION EXTRACTION TEST: YAML Format with Memory Optimization\")\nprint(\"=\" * 70)\n\n# Results storage for information extraction comparison  \nextraction_results = {\n    \"llama\": {\"documents\": [], \"successful\": 0, \"total_time\": 0},\n    \"internvl\": {\"documents\": [], \"successful\": 0, \"total_time\": 0}\n}\n\ndef cleanup_gpu_memory():\n    \"\"\"Minimize memory footprint as requested\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\ndef analyze_yaml_extraction(response: str, img_name: str):\n    \"\"\"Analyze YAML extraction results with consistent format\"\"\"\n    response_clean = response.strip()\n    \n    # Detect YAML format\n    is_yaml = bool(re.search(r'(store_name:|date:|total:)', response_clean, re.IGNORECASE))\n    \n    # Extract data from YAML or text\n    store_match = re.search(r'store_name:\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)\n    date_match = re.search(r'date:\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)  \n    total_match = re.search(r'total:\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)\n    \n    # Fallback detection for non-YAML responses\n    if not store_match:\n        store_match = re.search(r'(spotlight|store)', response_clean, re.IGNORECASE)\n    if not date_match:\n        date_match = re.search(r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}', response_clean)\n    if not total_match:\n        total_match = re.search(r'(\\$\\d+\\.\\d{2}|\\$\\d+)', response_clean)\n    \n    has_store = bool(store_match)\n    has_date = bool(date_match)\n    has_total = bool(total_match)\n    \n    extraction_score = sum([has_store, has_date, has_total])\n    \n    return {\n        \"img_name\": img_name,\n        \"response\": response_clean,\n        \"is_yaml\": is_yaml,\n        \"has_store\": has_store,\n        \"has_date\": has_date,\n        \"has_total\": has_total,\n        \"extraction_score\": extraction_score,\n        \"successful\": extraction_score >= 2  # At least 2/3 fields\n    }\n\n# Test each model for information extraction\ntest_models = [\"llama\", \"internvl\"]\n\nfor model_name in test_models:\n    print(f\"\\n{'=' * 60}\")\n    print(f\"üî¨ TESTING {model_name.upper()} INFORMATION EXTRACTION\")\n    print(f\"{'=' * 60}\")\n    \n    cleanup_gpu_memory()\n    model_start_time = time.time()\n    \n    try:\n        model_path = CONFIG[\"model_paths\"][model_name]\n        print(f\"Loading {model_name} model with 8-bit quantization...\")\n        \n        if model_name == \"llama\":\n            from transformers import AutoProcessor, MllamaForConditionalGeneration\n            from transformers import BitsAndBytesConfig\n            \n            processor = AutoProcessor.from_pretrained(\n                model_path, trust_remote_code=True, local_files_only=True\n            )\n            \n            quantization_config = BitsAndBytesConfig(\n                load_in_8bit=True,\n                llm_int8_enable_fp32_cpu_offload=True,\n                llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n            )\n            \n            model = MllamaForConditionalGeneration.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                torch_dtype=torch.float16,\n                local_files_only=True\n            ).eval()\n            \n        elif model_name == \"internvl\":\n            from transformers import AutoModel, AutoTokenizer\n            import torchvision.transforms as T\n            from torchvision.transforms.functional import InterpolationMode\n            \n            tokenizer = AutoTokenizer.from_pretrained(\n                model_path, trust_remote_code=True, local_files_only=True\n            )\n            \n            model = AutoModel.from_pretrained(\n                model_path,\n                load_in_8bit=True,\n                trust_remote_code=True,\n                torch_dtype=torch.bfloat16,\n                local_files_only=True\n            ).eval()\n        \n        model_load_time = time.time() - model_start_time\n        print(f\"‚úÖ Model loaded in {model_load_time:.1f}s\")\n        \n        # Test image14.png first to validate working parameters\n        print(f\"\\nüîç Testing image14.png first...\")\n        img_path = datasets_path / \"image14.png\"\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        inference_start = time.time()\n        \n        if model_name == \"llama\":\n            inputs = processor(text=CONFIG[\"extraction_prompt\"], images=image, return_tensors=\"pt\")\n            device = next(model.parameters()).device\n            if device.type != \"cpu\":\n                device_target = str(device).split(\":\")[0]\n                inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=CONFIG[\"max_new_tokens\"],\n                    do_sample=False,\n                    pad_token_id=processor.tokenizer.eos_token_id,\n                    eos_token_id=processor.tokenizer.eos_token_id,\n                    use_cache=True,\n                )\n            \n            raw_response = processor.decode(\n                outputs[0][inputs[\"input_ids\"].shape[-1]:],\n                skip_special_tokens=True\n            )\n            del inputs, outputs\n            \n        elif model_name == \"internvl\":\n            transform = T.Compose([\n                T.Resize((448, 448), interpolation=InterpolationMode.BICUBIC),\n                T.ToTensor(),\n                T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n            ])\n            \n            pixel_values = transform(image).unsqueeze(0)\n            if torch.cuda.is_available():\n                pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n            \n            raw_response = model.chat(\n                tokenizer=tokenizer,\n                pixel_values=pixel_values,\n                question=CONFIG[\"extraction_prompt\"],\n                generation_config={\"max_new_tokens\": CONFIG[\"max_new_tokens\"], \"do_sample\": False}\n            )\n            \n            if isinstance(raw_response, tuple):\n                raw_response = raw_response[0]\n            \n            del pixel_values\n        \n        inference_time = time.time() - inference_start\n        cleaned_response = repetition_controller.clean_response(raw_response)\n        validation_analysis = analyze_yaml_extraction(cleaned_response, \"image14.png\")\n        \n        print(f\"   Validation result: {validation_analysis['extraction_score']}/3 fields extracted\")\n        print(f\"   YAML format: {'‚úÖ' if validation_analysis['is_yaml'] else '‚ùå'}\")\n        print(f\"   Inference time: {inference_time:.1f}s\")\n        \n        if validation_analysis[\"successful\"]:\n            print(f\"‚úÖ Validation passed - proceeding with all {len(verified_extraction_images)} documents\")\n            \n            total_inference_time = 0\n            \n            for i, (img_name, doc_type) in enumerate(verified_extraction_images, 1):\n                try:\n                    img_path = datasets_path / img_name\n                    image = Image.open(img_path).convert(\"RGB\")\n                    \n                    inference_start = time.time()\n                    \n                    if model_name == \"llama\":\n                        inputs = processor(text=CONFIG[\"extraction_prompt\"], images=image, return_tensors=\"pt\")\n                        device = next(model.parameters()).device\n                        if device.type != \"cpu\":\n                            device_target = str(device).split(\":\")[0]\n                            inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n                        \n                        with torch.no_grad():\n                            outputs = model.generate(\n                                **inputs,\n                                max_new_tokens=CONFIG[\"max_new_tokens\"],\n                                do_sample=False,\n                                pad_token_id=processor.tokenizer.eos_token_id,\n                                eos_token_id=processor.tokenizer.eos_token_id,\n                                use_cache=True,\n                            )\n                        \n                        raw_response = processor.decode(\n                            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n                            skip_special_tokens=True\n                        )\n                        del inputs, outputs\n                        \n                    elif model_name == \"internvl\":\n                        transform = T.Compose([\n                            T.Resize((448, 448), interpolation=InterpolationMode.BICUBIC),\n                            T.ToTensor(),\n                            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n                        ])\n                        \n                        pixel_values = transform(image).unsqueeze(0)\n                        if torch.cuda.is_available():\n                            pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n                        \n                        raw_response = model.chat(\n                            tokenizer=tokenizer,\n                            pixel_values=pixel_values,\n                            question=CONFIG[\"extraction_prompt\"],\n                            generation_config={\"max_new_tokens\": CONFIG[\"max_new_tokens\"], \"do_sample\": False}\n                        )\n                        \n                        if isinstance(raw_response, tuple):\n                            raw_response = raw_response[0]\n                        \n                        del pixel_values\n                    \n                    inference_time = time.time() - inference_start\n                    total_inference_time += inference_time\n                    \n                    cleaned_response = repetition_controller.clean_response(raw_response)\n                    analysis = analyze_yaml_extraction(cleaned_response, img_name)\n                    analysis[\"inference_time\"] = inference_time\n                    analysis[\"doc_type\"] = doc_type\n                    \n                    extraction_results[model_name][\"documents\"].append(analysis)\n                    \n                    if analysis[\"successful\"]:\n                        extraction_results[model_name][\"successful\"] += 1\n                    \n                    # Consistent output format as requested\n                    status = \"‚úÖ\" if analysis[\"successful\"] else \"‚ùå\"\n                    yaml_status = \"Y\" if analysis[\"is_yaml\"] else \"T\"\n                    print(f\"   {i:2d}. {img_name:<12} {status} {inference_time:.1f}s | {yaml_status} | {analysis['extraction_score']}/3\")\n                    \n                    # Immediate tensor cleanup - minimizing memory footprint\n                    del image\n                    \n                    # Periodic GPU cleanup every 3 images\n                    if i % 3 == 0:\n                        gc.collect()\n                        if torch.cuda.is_available():\n                            torch.cuda.empty_cache()\n                    \n                except Exception as e:\n                    print(f\"   {i:2d}. {img_name:<12} ‚ùå Error: {str(e)[:30]}...\")\n            \n            extraction_results[model_name][\"total_time\"] = total_inference_time\n            extraction_results[model_name][\"avg_time\"] = total_inference_time / len(verified_extraction_images)\n            \n        else:\n            print(f\"‚ùå Validation failed - skipping full test\")\n        \n        # Cleanup model to minimize memory footprint\n        del model\n        if model_name == \"llama\":\n            del processor\n        elif model_name == \"internvl\":\n            del tokenizer\n        cleanup_gpu_memory()\n        \n    except Exception as e:\n        print(f\"‚ùå {model_name.upper()} FAILED TO LOAD: {str(e)[:100]}...\")\n\n# FINAL COMPARISON: Information Extraction Performance\nprint(f\"\\n{'=' * 80}\")\nprint(\"üèÜ FINAL RECOMMENDATION: BEST MODEL FOR INFORMATION EXTRACTION\")\nprint(f\"{'=' * 80}\")\n\n# Compare both models' performance\nllama_success = 0\nllama_total = 0\nllama_avg_time = 0\ninternvl_success = 0\ninternvl_total = 0\ninternvl_avg_time = 0\n\nif extraction_results[\"llama\"][\"documents\"]:\n    llama_total = len(extraction_results[\"llama\"][\"documents\"])\n    llama_success = extraction_results[\"llama\"][\"successful\"]\n    llama_avg_time = extraction_results[\"llama\"][\"avg_time\"]\n\nif extraction_results[\"internvl\"][\"documents\"]:\n    internvl_total = len(extraction_results[\"internvl\"][\"documents\"])\n    internvl_success = extraction_results[\"internvl\"][\"successful\"]\n    internvl_avg_time = extraction_results[\"internvl\"][\"avg_time\"]\n\nprint(f\"üìä INFORMATION EXTRACTION COMPARISON:\")\nprint(f\"{'Model':<12} {'Success Rate':<15} {'Avg Time':<12} {'Best For'}\")\nprint(\"-\" * 60)\n\nif llama_total > 0:\n    llama_rate = llama_success / llama_total * 100\n    print(f\"{'LLAMA':<12} {llama_rate:.1f}% ({llama_success}/{llama_total}){'':<5} {llama_avg_time:.1f}s{'':<7} Large context\")\n\nif internvl_total > 0:\n    internvl_rate = internvl_success / internvl_total * 100\n    print(f\"{'INTERNVL':<12} {internvl_rate:.1f}% ({internvl_success}/{internvl_total}){'':<5} {internvl_avg_time:.1f}s{'':<7} Production speed\")\n\n# Make recommendation\nif internvl_total > 0 and llama_total > 0:\n    internvl_rate = internvl_success / internvl_total * 100\n    llama_rate = llama_success / llama_total * 100\n    \n    if internvl_rate > llama_rate:\n        recommended = \"INTERNVL\"\n        reason = f\"Higher success rate ({internvl_rate:.1f}% vs {llama_rate:.1f}%) and faster inference\"\n    elif llama_rate > internvl_rate:\n        recommended = \"LLAMA\"\n        reason = f\"Higher success rate ({llama_rate:.1f}% vs {internvl_rate:.1f}%)\"\n    else:\n        recommended = \"INTERNVL\"\n        reason = f\"Equal success rate but {internvl_avg_time/llama_avg_time:.1f}x faster inference\"\n    \n    print(f\"\\nü•á RECOMMENDED FOR INFORMATION EXTRACTION: {recommended}\")\n    print(f\"   Reason: {reason}\")\n    print(f\"   Use case: Business document processing (receipts, invoices, statements)\")\nelif internvl_total > 0:\n    print(f\"\\nü•á RECOMMENDED: INTERNVL (only model tested successfully)\")\nelif llama_total > 0:\n    print(f\"\\nü•á RECOMMENDED: LLAMA (only model tested successfully)\")\nelse:\n    print(f\"\\n‚ö†Ô∏è No successful tests - investigate model loading issues\")\n\nprint(f\"\\n‚úÖ COMPLETE: Information extraction performance comparison finished!\")\nprint(f\"üìã This answers the user's question about best model for their information extraction job\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Vision Model Test\n",
    "\n",
    "Direct model loading and testing without using the unified_vision_processor package.\n",
    "\n",
    "All configuration is embedded in the notebook for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Model: llama (using WORKING vision_processor patterns)\n",
      "Image: datasets/image14.png\n",
      "Prompt: <|image|>Extract data from this receipt in KEY-VALUE format.\n",
      "\n",
      "Output format:\n",
      "DATE: [date from receip...\n",
      "\n",
      "✅ Using PROVEN working patterns from vision_processor/models/llama_model.py\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Modify as needed\n",
    "CONFIG = {\n",
    "    # Model selection: \"llama\" or \"internvl\"\n",
    "    \"model_type\": \"llama\",  # BACK TO LLAMA with working code patterns\n",
    "    \n",
    "    # Model paths\n",
    "    \"model_paths\": {\n",
    "        \"llama\": \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision\",\n",
    "        \"internvl\": \"/home/jovyan/nfs_share/models/InternVL3-8B\"\n",
    "    },\n",
    "    \n",
    "    # Test image path\n",
    "    \"test_image\": \"datasets/image14.png\",\n",
    "    \n",
    "    # WORKING prompt pattern from vision_processor (KEY-VALUE format)\n",
    "    \"prompt\": \"<|image|>Extract data from this receipt in KEY-VALUE format.\\n\\nOutput format:\\nDATE: [date from receipt]\\nSTORE: [store name]\\nTOTAL: [total amount]\\n\\nExtract all visible text and format as KEY: VALUE pairs only.\",\n",
    "    \n",
    "    # EXACT working generation parameters from LlamaVisionModel\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"enable_quantization\": True\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"Model: {CONFIG['model_type']} (using WORKING vision_processor patterns)\")\n",
    "print(f\"Image: {CONFIG['test_image']}\")\n",
    "print(f\"Prompt: {CONFIG['prompt'][:100]}...\")\n",
    "print(\"\\n✅ Using PROVEN working patterns from vision_processor/models/llama_model.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful for llama ✓\n"
     ]
    }
   ],
   "source": [
    "# Imports - Direct model loading\n",
    "import time\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Model-specific imports based on selection\n",
    "if CONFIG[\"model_type\"] == \"llama\":\n",
    "    from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    import torchvision.transforms as T\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "print(f\"Imports successful for {CONFIG['model_type']} ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama model from /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision...\n",
      "✅ Using WORKING quantization config (skipping vision modules)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24deff564e0d4b97aa7bcff53eb19bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Applied WORKING generation config (no sampling parameters)\n",
      "✅ Model loaded successfully in 5.92s\n",
      "Model device: cuda:0\n",
      "Quantization active: True\n"
     ]
    }
   ],
   "source": [
    "# Load model directly - USING WORKING VISION_PROCESSOR PATTERNS\n",
    "model_path = CONFIG[\"model_paths\"][CONFIG[\"model_type\"]]\n",
    "print(f\"Loading {CONFIG['model_type']} model from {model_path}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    if CONFIG[\"model_type\"] == \"llama\":\n",
    "        # EXACT pattern from vision_processor/models/llama_model.py\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        # Working quantization config from LlamaVisionModel\n",
    "        quantization_config = None\n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,\n",
    "                    llm_int8_enable_fp32_cpu_offload=True,\n",
    "                    llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n",
    "                    llm_int8_threshold=6.0,\n",
    "                )\n",
    "                print(\"✅ Using WORKING quantization config (skipping vision modules)\")\n",
    "            except ImportError:\n",
    "                print(\"Quantization not available, using FP16\")\n",
    "                CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        # Working model loading args from LlamaVisionModel\n",
    "        model_loading_args = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if quantization_config:\n",
    "            model_loading_args[\"quantization_config\"] = quantization_config\n",
    "        \n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            **model_loading_args\n",
    "        ).eval()\n",
    "        \n",
    "        # CRITICAL: Set working generation config exactly like LlamaVisionModel\n",
    "        model.generation_config.max_new_tokens = CONFIG[\"max_new_tokens\"]\n",
    "        model.generation_config.do_sample = False\n",
    "        model.generation_config.temperature = None  # Disable temperature\n",
    "        model.generation_config.top_p = None        # Disable top_p  \n",
    "        model.generation_config.top_k = None        # Disable top_k\n",
    "        model.config.use_cache = True               # Enable KV cache\n",
    "        \n",
    "        print(\"✅ Applied WORKING generation config (no sampling parameters)\")\n",
    "        \n",
    "    elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "        # Load InternVL3\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        model_kwargs = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                model_kwargs[\"load_in_8bit\"] = True\n",
    "                print(\"8-bit quantization enabled\")\n",
    "            except Exception:\n",
    "                print(\"Quantization not available, using bfloat16\")\n",
    "                CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            **model_kwargs\n",
    "        ).eval()\n",
    "        \n",
    "        if torch.cuda.is_available() and not CONFIG[\"enable_quantization\"]:\n",
    "            model = model.cuda()\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"✅ Model loaded successfully in {load_time:.2f}s\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Quantization active: {CONFIG['enable_quantization']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Model loading failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Image loaded: (2048, 2048)\n",
      "  File size: 211.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess image\n",
    "test_image_path = Path(CONFIG[\"test_image\"])\n",
    "\n",
    "if not test_image_path.exists():\n",
    "    print(f\"✗ Test image not found: {test_image_path}\")\n",
    "    available = list(Path(\"datasets\").glob(\"*.png\"))[:5]\n",
    "    print(f\"Available images: {[img.name for img in available]}\")\n",
    "    raise FileNotFoundError(f\"Test image not found: {test_image_path}\")\n",
    "\n",
    "# Load image\n",
    "image = Image.open(test_image_path)\n",
    "if image.mode != \"RGB\":\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "print(f\"✓ Image loaded: {image.size}\")\n",
    "print(f\"  File size: {test_image_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference with llama...\n",
      "Prompt: <|image|>Extract data from this receipt in KEY-VALUE format.\n",
      "\n",
      "Output format:\n",
      "DATE: [date from receip...\n",
      "--------------------------------------------------\n",
      "Input tensor shapes: [('input_ids', torch.Size([1, 49])), ('attention_mask', torch.Size([1, 49])), ('pixel_values', torch.Size([1, 1, 4, 3, 448, 448])), ('aspect_ratio_ids', torch.Size([1, 1])), ('aspect_ratio_mask', torch.Size([1, 1, 4])), ('cross_attention_mask', torch.Size([1, 49, 1, 4]))]\n",
      "Device target: cuda:0\n",
      "Using ultra-short max_new_tokens: 384 (was 1024)\n",
      "✅ Using ULTRA-AGGRESSIVE repetition control + shorter generation\n",
      "Raw response (first 200 chars):  \n",
      "DATE: 11-07-2022\n",
      "STORE: SPOTLIGHT\n",
      "TOTAL: $22.45\n",
      "ITEM: Apples (kg)\n",
      "QUANTITY: 1\n",
      "PRICE: $3.96\n",
      "TOTAL: $3.96\n",
      "ITEM: Tea Bags (box)\n",
      "QUANTITY: 1\n",
      "PRICE: $4.53\n",
      "TOTAL: $4.53\n",
      "ITEM: Free Range Eggs (d)\n",
      "QUANTITY:...\n",
      "Raw response length: 1151 characters\n",
      "⚠️ Still repetitive after ultra-aggressive cleaning - truncating heavily\n",
      "🧹 Cleaning: 1151 → 164 chars (85.8% reduction)\n",
      "❌ STILL REPETITIVE after ultra-aggressive cleaning!\n",
      "   This indicates a fundamental issue with the model's generation pattern\n",
      "✅ Inference completed in 32.43s\n",
      "Final response length: 164 characters\n",
      "Final response ready for display (length: 164 characters)\n"
     ]
    }
   ],
   "source": [
    "# Run inference - ULTRA-AGGRESSIVE REPETITION CONTROL\n",
    "prompt = CONFIG[\"prompt\"]\n",
    "print(f\"Running inference with {CONFIG['model_type']}...\")\n",
    "print(f\"Prompt: {prompt[:100]}...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class UltraAggressiveRepetitionController:\n",
    "    \"\"\"Ultra-aggressive repetition detection and control specifically for Llama-3.2-Vision.\"\"\"\n",
    "    \n",
    "    def __init__(self, word_threshold: float = 0.15, phrase_threshold: int = 2):\n",
    "        \"\"\"\n",
    "        Initialize ultra-aggressive repetition controller.\n",
    "        \n",
    "        Args:\n",
    "            word_threshold: If any word appears more than this % of total words, it's repetitive (15% vs 30%)\n",
    "            phrase_threshold: Minimum repetitions to trigger cleaning (2 vs 3)\n",
    "        \"\"\"\n",
    "        self.word_threshold = word_threshold\n",
    "        self.phrase_threshold = phrase_threshold\n",
    "        \n",
    "        # Known problematic patterns from Llama-3.2-Vision\n",
    "        self.toxic_patterns = [\n",
    "            r\"THANK YOU FOR SHOPPING WITH US[^.]*\",\n",
    "            r\"All prices include GST where applicable[^.]*\",\n",
    "            r\"\\\\+[a-zA-Z]*\\{[^}]*\\}\",  # LaTeX artifacts\n",
    "            r\"\\(\\s*\\)\",  # Empty parentheses\n",
    "            r\"[.-]\\s*THANK YOU\",  # Dash/period before thank you\n",
    "        ]\n",
    "    \n",
    "    def detect_repetitive_generation(self, text: str, min_words: int = 3) -> bool:\n",
    "        \"\"\"Ultra-sensitive repetition detection.\"\"\"\n",
    "        words = text.split()\n",
    "        \n",
    "        # Much stricter minimum content requirement\n",
    "        if len(words) < min_words:\n",
    "            return True\n",
    "        \n",
    "        # Check for known toxic patterns first\n",
    "        if self._has_toxic_patterns(text):\n",
    "            return True\n",
    "            \n",
    "        # Ultra-aggressive word repetition check (15% threshold vs 30%)\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_lower = word.lower().strip('.,!?()[]{}')\n",
    "            if len(word_lower) > 2:  # Ignore very short words\n",
    "                word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n",
    "        \n",
    "        total_words = len([w for w in words if len(w.strip('.,!?()[]{}')) > 2])\n",
    "        if total_words > 0:\n",
    "            for word, count in word_counts.items():\n",
    "                if count > total_words * self.word_threshold:  # 15% threshold\n",
    "                    return True\n",
    "        \n",
    "        # Ultra-aggressive phrase repetition\n",
    "        if self._detect_aggressive_phrase_repetition(text):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def _has_toxic_patterns(self, text: str) -> bool:\n",
    "        \"\"\"Check for known problematic patterns.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        for pattern in self.toxic_patterns:\n",
    "            matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "            if len(matches) >= 2:  # Even 2 occurrences is too many\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _detect_aggressive_phrase_repetition(self, text: str) -> bool:\n",
    "        \"\"\"Ultra-aggressive phrase repetition detection.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Check for 3+ word phrases repeated even twice\n",
    "        words = text.split()\n",
    "        for i in range(len(words) - 6):  # Need at least 6 words for 3+3\n",
    "            phrase = ' '.join(words[i:i+3]).lower()\n",
    "            remainder = ' '.join(words[i+3:]).lower()\n",
    "            if phrase in remainder:\n",
    "                return True\n",
    "        \n",
    "        # Check sentences/segments\n",
    "        segments = re.split(r'[.!?]+', text)\n",
    "        segment_counts = {}\n",
    "        \n",
    "        for segment in segments:\n",
    "            segment_clean = re.sub(r'\\s+', ' ', segment.strip().lower())\n",
    "            # Much shorter minimum segment length\n",
    "            if len(segment_clean) > 5:  # Was 10, now 5\n",
    "                segment_counts[segment_clean] = segment_counts.get(segment_clean, 0) + 1\n",
    "        \n",
    "        # Any segment appearing twice is problematic\n",
    "        for count in segment_counts.values():\n",
    "            if count >= self.phrase_threshold:  # Now 2 instead of 3\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "    \n",
    "    def clean_response(self, response: str) -> str:\n",
    "        \"\"\"Ultra-aggressive cleaning with early truncation.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        if not response or len(response.strip()) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        original_length = len(response)\n",
    "        \n",
    "        # Step 1: Early truncation at first major repetition\n",
    "        response = self._early_truncate_at_repetition(response)\n",
    "        \n",
    "        # Step 2: Remove toxic patterns aggressively\n",
    "        response = self._remove_toxic_patterns(response)\n",
    "        \n",
    "        # Step 3: Remove safety warnings\n",
    "        response = self._remove_safety_warnings(response)\n",
    "        \n",
    "        # Step 4: Ultra-aggressive repetition removal\n",
    "        response = self._ultra_aggressive_word_removal(response)\n",
    "        response = self._ultra_aggressive_phrase_removal(response)\n",
    "        response = self._ultra_aggressive_sentence_removal(response)\n",
    "        \n",
    "        # Step 5: Clean artifacts\n",
    "        response = self._clean_artifacts(response)\n",
    "        \n",
    "        # Step 6: Final validation and truncation\n",
    "        response = self._final_validation_truncate(response)\n",
    "        \n",
    "        final_length = len(response)\n",
    "        reduction = ((original_length - final_length) / original_length * 100) if original_length > 0 else 0\n",
    "        \n",
    "        print(f\"🧹 Cleaning: {original_length} → {final_length} chars ({reduction:.1f}% reduction)\")\n",
    "        \n",
    "        return response.strip()\n",
    "    \n",
    "    def _early_truncate_at_repetition(self, text: str) -> str:\n",
    "        \"\"\"Truncate immediately when repetition starts.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Find first occurrence of toxic patterns and truncate there\n",
    "        for pattern in self.toxic_patterns:\n",
    "            match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                # Find the SECOND occurrence and truncate before it\n",
    "                remaining = text[match.end():]\n",
    "                second_match = re.search(pattern, remaining, flags=re.IGNORECASE)\n",
    "                if second_match:\n",
    "                    truncate_point = match.end() + second_match.start()\n",
    "                    print(f\"🔪 Early truncation at repetition: {len(text)} → {truncate_point} chars\")\n",
    "                    return text[:truncate_point]\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _remove_toxic_patterns(self, text: str) -> str:\n",
    "        \"\"\"Aggressively remove known toxic patterns.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        for pattern in self.toxic_patterns:\n",
    "            # Remove ALL occurrences, not just duplicates\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _remove_safety_warnings(self, text: str) -> str:\n",
    "        \"\"\"Remove safety warnings.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        safety_patterns = [\n",
    "            r\"I'm not able to provide.*?information\\.?\",\n",
    "            r\"I cannot provide.*?information\\.?\", \n",
    "            r\"I'm unable to.*?\\.?\",\n",
    "            r\"I can't.*?\\.?\",\n",
    "            r\"Sorry, I cannot.*?\\.?\",\n",
    "            r\".*could compromise.*privacy.*\",\n",
    "        ]\n",
    "        \n",
    "        for pattern in safety_patterns:\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _ultra_aggressive_word_removal(self, text: str) -> str:\n",
    "        \"\"\"Ultra-aggressive word repetition removal.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Remove 2+ consecutive identical words (was 3+)\n",
    "        text = re.sub(r'\\b(\\w+)(\\s+\\1){1,}', r'\\1', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove any word appearing more than 3 times total\n",
    "        words = text.split()\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_lower = word.lower().strip('.,!?()[]{}')\n",
    "            word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n",
    "        \n",
    "        # Rebuild text, limiting each word to max 3 occurrences\n",
    "        result_words = []\n",
    "        word_usage = {}\n",
    "        \n",
    "        for word in words:\n",
    "            word_lower = word.lower().strip('.,!?()[]{}')\n",
    "            current_count = word_usage.get(word_lower, 0)\n",
    "            \n",
    "            if current_count < 3:  # Allow max 3 occurrences\n",
    "                result_words.append(word)\n",
    "                word_usage[word_lower] = current_count + 1\n",
    "        \n",
    "        return ' '.join(result_words)\n",
    "    \n",
    "    def _ultra_aggressive_phrase_removal(self, text: str) -> str:\n",
    "        \"\"\"Ultra-aggressive phrase removal.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Remove repeated 2-6 word phrases (expanded range)\n",
    "        for phrase_length in range(2, 7):\n",
    "            pattern = r'\\b((?:\\w+\\s+){' + str(phrase_length-1) + r'}\\w+)(\\s+\\1){1,}'  # 1+ repetitions vs 2+\n",
    "            text = re.sub(pattern, r'\\1', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _ultra_aggressive_sentence_removal(self, text: str) -> str:\n",
    "        \"\"\"Ultra-aggressive sentence removal.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        \n",
    "        # Keep only first occurrence of any sentence\n",
    "        seen = set()\n",
    "        unique_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_clean = re.sub(r'\\s+', ' ', sentence.strip().lower())\n",
    "            sentence_clean = re.sub(r'[^\\w\\s]', '', sentence_clean)  # Remove all punctuation for comparison\n",
    "            \n",
    "            if sentence_clean and len(sentence_clean) > 3:  # Very short minimum\n",
    "                if sentence_clean not in seen:\n",
    "                    seen.add(sentence_clean)\n",
    "                    unique_sentences.append(sentence.strip())\n",
    "        \n",
    "        return '. '.join(unique_sentences)\n",
    "    \n",
    "    def _clean_artifacts(self, text: str) -> str:\n",
    "        \"\"\"Aggressive artifact cleaning.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Remove whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove LaTeX/markdown aggressively\n",
    "        text = re.sub(r'\\\\+[a-zA-Z]*\\{[^}]*\\}', '', text)\n",
    "        text = re.sub(r'\\\\+[a-zA-Z]+', '', text)\n",
    "        text = re.sub(r'```+[^`]*```+', '', text)\n",
    "        text = re.sub(r'[{}]+', '', text)\n",
    "        \n",
    "        # Remove excessive punctuation\n",
    "        text = re.sub(r'[.]{2,}', '.', text)\n",
    "        text = re.sub(r'[!]{2,}', '!', text)\n",
    "        text = re.sub(r'[?]{2,}', '?', text)\n",
    "        text = re.sub(r'[,]{2,}', ',', text)\n",
    "        \n",
    "        # Remove empty parentheses and brackets\n",
    "        text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "        text = re.sub(r'\\[\\s*\\]', '', text)\n",
    "        \n",
    "        # Remove standalone punctuation\n",
    "        text = re.sub(r'\\s+[.,!?;:]\\s+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _final_validation_truncate(self, text: str, max_length: int = 800) -> str:\n",
    "        \"\"\"Final validation with aggressive truncation.\"\"\"\n",
    "        # If still repetitive after all cleaning, something is very wrong\n",
    "        if self.detect_repetitive_generation(text):\n",
    "            print(\"⚠️ Still repetitive after ultra-aggressive cleaning - truncating heavily\")\n",
    "            # Find last good sentence in first half\n",
    "            half_point = len(text) // 2\n",
    "            truncated = text[:half_point]\n",
    "            last_period = truncated.rfind('.')\n",
    "            if last_period > half_point * 0.5:\n",
    "                return truncated[:last_period + 1]\n",
    "            else:\n",
    "                return truncated[:half_point] + \"...\"\n",
    "        \n",
    "        # Aggressive length limit\n",
    "        if len(text) > max_length:\n",
    "            truncated = text[:max_length]\n",
    "            last_period = truncated.rfind('.')\n",
    "            if last_period > max_length * 0.7:\n",
    "                return truncated[:last_period + 1]\n",
    "            else:\n",
    "                return truncated + \"...\"\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Initialize ultra-aggressive repetition controller\n",
    "repetition_controller = UltraAggressiveRepetitionController(\n",
    "    word_threshold=0.15,  # Much stricter: 15% vs 30%\n",
    "    phrase_threshold=2    # Much stricter: 2 vs 3 repetitions\n",
    ")\n",
    "\n",
    "try:\n",
    "    if CONFIG[\"model_type\"] == \"llama\":\n",
    "        # EXACT input preparation from LlamaVisionModel._prepare_inputs()\n",
    "        prompt_with_image = prompt if prompt.startswith(\"<|image|>\") else f\"<|image|>{prompt}\"\n",
    "        \n",
    "        inputs = processor(text=prompt_with_image, images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # WORKING device handling from LlamaVisionModel\n",
    "        device = next(model.parameters()).device\n",
    "        if device.type != \"cpu\":\n",
    "            device_target = str(device).split(\":\")[0] if \":\" in str(device) else str(device)\n",
    "            inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "        \n",
    "        print(f\"Input tensor shapes: {[(k, v.shape) for k, v in inputs.items() if hasattr(v, 'shape')]}\")\n",
    "        print(f\"Device target: {device}\")\n",
    "        \n",
    "        # ULTRA-AGGRESSIVE: Even shorter token limit\n",
    "        effective_max_tokens = min(CONFIG[\"max_new_tokens\"], 384)  # Further reduced: 384 vs 512\n",
    "        print(f\"Using ultra-short max_new_tokens: {effective_max_tokens} (was {CONFIG['max_new_tokens']})\")\n",
    "        \n",
    "        # EXACT generation kwargs from LlamaVisionModel.generate()\n",
    "        generation_kwargs = {\n",
    "            **inputs,\n",
    "            \"max_new_tokens\": effective_max_tokens,\n",
    "            \"do_sample\": False,  # Deterministic generation\n",
    "            \"pad_token_id\": processor.tokenizer.eos_token_id,\n",
    "            \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "            \"use_cache\": True,\n",
    "        }\n",
    "        \n",
    "        print(\"✅ Using ULTRA-AGGRESSIVE repetition control + shorter generation\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**generation_kwargs)\n",
    "        \n",
    "        raw_response = processor.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Raw response (first 200 chars): {raw_response[:200]}...\")\n",
    "        print(f\"Raw response length: {len(raw_response)} characters\")\n",
    "        \n",
    "        # ULTRA-AGGRESSIVE: Enhanced repetition control\n",
    "        response = repetition_controller.clean_response(raw_response)\n",
    "        \n",
    "        # Final check with stricter detection\n",
    "        if repetition_controller.detect_repetitive_generation(response):\n",
    "            print(\"❌ STILL REPETITIVE after ultra-aggressive cleaning!\")\n",
    "            print(\"   This indicates a fundamental issue with the model's generation pattern\")\n",
    "        else:\n",
    "            print(\"✅ Ultra-aggressive cleaning successful - repetition eliminated\")\n",
    "        \n",
    "    elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "        # InternVL inference with ultra-aggressive repetition control\n",
    "        image_size = 448\n",
    "        transform = T.Compose([\n",
    "            T.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        \n",
    "        pixel_values = transform(image).unsqueeze(0)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n",
    "        else:\n",
    "            pixel_values = pixel_values.contiguous()\n",
    "        \n",
    "        generation_config = {\n",
    "            \"max_new_tokens\": min(CONFIG[\"max_new_tokens\"], 384),\n",
    "            \"do_sample\": False,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id\n",
    "        }\n",
    "        \n",
    "        raw_response = model.chat(\n",
    "            tokenizer=tokenizer,\n",
    "            pixel_values=pixel_values,\n",
    "            question=prompt,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        \n",
    "        if isinstance(raw_response, tuple):\n",
    "            raw_response = raw_response[0]\n",
    "        \n",
    "        # Apply ultra-aggressive repetition control\n",
    "        response = repetition_controller.clean_response(raw_response)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    print(f\"✅ Inference completed in {inference_time:.2f}s\")\n",
    "    print(f\"Final response length: {len(response)} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Inference failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    response = f\"Error: Inference failed - {str(e)}\"\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"Final response ready for display (length: {len(response) if 'response' in locals() else 0} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTED TEXT:\n",
      "============================================================\n",
      "DATE: 11-07-2022 STORE: SPOTLIGHT TOTAL: $22. 45 ITEM: Apples (kg) QUANTITY: 1 PRICE: $3. 96 TOTAL: $3. 96 ITEM: Tea Bags (box) QUANTITY: 1 PRICE: $4. 53 TOTAL: $4.\n",
      "============================================================\n",
      "\n",
      "SUMMARY:\n",
      "Model: llama\n",
      "Response length: 164 characters\n",
      "Processing time: 32.43s\n",
      "Quantization enabled: True\n",
      "Device: CUDA\n",
      "\n",
      "RESPONSE ANALYSIS:\n",
      "✅ KEY-VALUE format detected\n",
      "Extracted fields:\n",
      "  DATE: 11-07-2022 STORE: SPOTLIGHT TOTAL: $22. 45 ITEM: Apples (kg) QUANTITY: 1 PRICE: $3. 96 TOTAL: $3. 96 ITEM: Tea Bags (box) QUANTITY: 1 PRICE: $4. 53 TOTAL: $4.\n",
      "\n",
      "⚠️ ACCEPTABLE performance: 32.4s\n",
      "\n",
      "🎯 For production use:\n",
      "- Llama-3.2-Vision: Use simple JSON prompts only\n",
      "- InternVL3: More flexible, handles complex prompts better\n",
      "- Both models: Shorter max_new_tokens prevents issues\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTED TEXT:\")\n",
    "print(\"=\" * 60)\n",
    "print(response)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"Model: {CONFIG['model_type']}\")\n",
    "print(f\"Response length: {len(response)} characters\")\n",
    "print(f\"Processing time: {inference_time:.2f}s\")\n",
    "print(f\"Quantization enabled: {CONFIG['enable_quantization']}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Enhanced JSON parsing with validation\n",
    "print(f\"\\nRESPONSE ANALYSIS:\")\n",
    "if response.strip().startswith('{') and response.strip().endswith('}'):\n",
    "    try:\n",
    "        import json\n",
    "        parsed = json.loads(response.strip())\n",
    "        print(f\"✅ VALID JSON EXTRACTED:\")\n",
    "        for key, value in parsed.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Validate completeness\n",
    "        expected_fields = [\"DATE\", \"STORE\", \"TOTAL\"]\n",
    "        missing = [field for field in expected_fields if field not in parsed or not parsed[field]]\n",
    "        if missing:\n",
    "            print(f\"⚠️ Missing fields: {missing}\")\n",
    "        else:\n",
    "            print(f\"✅ All expected fields present\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ Invalid JSON: {e}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        \n",
    "elif any(keyword in response for keyword in [\"DATE:\", \"STORE:\", \"TOTAL:\"]):\n",
    "    print(f\"✅ KEY-VALUE format detected\")\n",
    "    # Try to extract key-value pairs\n",
    "    import re\n",
    "    matches = re.findall(r'([A-Z]+):\\s*([^\\n]+)', response)\n",
    "    if matches:\n",
    "        print(f\"Extracted fields:\")\n",
    "        for key, value in matches:\n",
    "            print(f\"  {key}: {value.strip()}\")\n",
    "            \n",
    "elif any(phrase in response.lower() for phrase in [\"not able\", \"cannot provide\", \"sorry\"]):\n",
    "    print(f\"❌ SAFETY MODE TRIGGERED\")\n",
    "    print(f\"This indicates the prompt triggered Llama's safety restrictions\")\n",
    "    print(f\"Solution: Use simpler JSON format prompts\")\n",
    "    \n",
    "else:\n",
    "    print(f\"⚠️ UNSTRUCTURED RESPONSE\")\n",
    "    print(f\"Response doesn't match expected patterns\")\n",
    "    print(f\"Consider using different prompt format\")\n",
    "\n",
    "# Performance assessment\n",
    "if inference_time < 30:\n",
    "    print(f\"\\n⚡ GOOD performance: {inference_time:.1f}s\")\n",
    "elif inference_time < 60:\n",
    "    print(f\"\\n⚠️ ACCEPTABLE performance: {inference_time:.1f}s\") \n",
    "else:\n",
    "    print(f\"\\n❌ SLOW performance: {inference_time:.1f}s\")\n",
    "\n",
    "print(f\"\\n🎯 For production use:\")\n",
    "print(f\"- Llama-3.2-Vision: Use simple JSON prompts only\")\n",
    "print(f\"- InternVL3: More flexible, handles complex prompts better\")\n",
    "print(f\"- Both models: Shorter max_new_tokens prevents issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing additional prompts with ULTRA-AGGRESSIVE REPETITION CONTROL...\n",
      "\n",
      "Test 1: <|image|>Extract store name and total amount in KEY-VALUE fo...\n",
      "🧹 Cleaning: 184 → 49 chars (73.4% reduction)\n",
      "✅ SUCCESS (7.9s): <OCR/> SPOTLIGHT TAX INVOICE 888Park 3:53PM QTY 1...\n",
      "   Length: 49 chars - repetition eliminated\n",
      "--------------------------------------------------\n",
      "Test 2: <|image|>What type of business document is this? Answer: rec...\n",
      "⚠️ Still repetitive after ultra-aggressive cleaning - truncating heavily\n",
      "🧹 Cleaning: 511 → 3 chars (99.4% reduction)\n",
      "❌ STILL REPETITIVE (8.1s): ......\n",
      "   Even ultra-aggressive cleaning failed - model has fundamental repetition issue\n",
      "--------------------------------------------------\n",
      "Test 3: <|image|>Extract the date from this document in format DD/MM...\n",
      "⚠️ Still repetitive after ultra-aggressive cleaning - truncating heavily\n",
      "🧹 Cleaning: 173 → 20 chars (88.4% reduction)\n",
      "❌ STILL REPETITIVE (8.2s): 11-07-2022, 11-07......\n",
      "   Even ultra-aggressive cleaning failed - model has fundamental repetition issue\n",
      "--------------------------------------------------\n",
      "\n",
      "🎯 ULTRA-AGGRESSIVE REPETITION CONTROL FEATURES:\n",
      "🔥 UltraAggressiveRepetitionController - Nuclear option for repetition\n",
      "🔥 Stricter thresholds:\n",
      "   - Word repetition: 15% threshold (was 30%)\n",
      "   - Phrase repetition: 2 occurrences trigger (was 3)\n",
      "   - Sentence repetition: Any duplicate removed\n",
      "🔥 Toxic pattern targeting:\n",
      "   - 'THANK YOU FOR SHOPPING...' pattern recognition\n",
      "   - 'All prices include GST...' pattern recognition\n",
      "   - LaTeX artifact removal\n",
      "🔥 Early truncation at first repetition detection\n",
      "🔥 Max 3 occurrences per word across entire text\n",
      "🔥 Ultra-short token limits (384 main, 96 tests)\n",
      "🔥 Aggressive artifact cleaning (punctuation, parentheses, etc.)\n",
      "\n",
      "💡 If this still shows repetition, the issue is in the model's generation\n",
      "   pattern itself, not the post-processing cleaning.\n"
     ]
    }
   ],
   "source": [
    "# Test additional prompts - WITH ULTRA-AGGRESSIVE REPETITION CONTROL\n",
    "working_test_prompts = [\n",
    "    \"<|image|>Extract store name and total amount in KEY-VALUE format.\\n\\nOutput format:\\nSTORE: [store name]\\nTOTAL: [total amount]\",\n",
    "    \"<|image|>What type of business document is this? Answer: receipt, invoice, or statement.\",\n",
    "    \"<|image|>Extract the date from this document in format DD/MM/YYYY.\"\n",
    "]\n",
    "\n",
    "print(\"Testing additional prompts with ULTRA-AGGRESSIVE REPETITION CONTROL...\\n\")\n",
    "\n",
    "for i, test_prompt in enumerate(working_test_prompts, 1):\n",
    "    print(f\"Test {i}: {test_prompt[:60]}...\")\n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        if CONFIG[\"model_type\"] == \"llama\":\n",
    "            # Use EXACT same pattern as main inference\n",
    "            prompt_with_image = test_prompt if test_prompt.startswith(\"<|image|>\") else f\"<|image|>{test_prompt}\"\n",
    "            \n",
    "            inputs = processor(text=prompt_with_image, images=image, return_tensors=\"pt\")\n",
    "            \n",
    "            # Same device handling\n",
    "            device = next(model.parameters()).device\n",
    "            if device.type != \"cpu\":\n",
    "                device_target = str(device).split(\":\")[0] if \":\" in str(device) else str(device)\n",
    "                inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "            \n",
    "            # ULTRA-AGGRESSIVE: Extremely short tokens for tests\n",
    "            generation_kwargs = {\n",
    "                **inputs,\n",
    "                \"max_new_tokens\": 96,  # Even shorter: 96 vs 128\n",
    "                \"do_sample\": False,\n",
    "                \"pad_token_id\": processor.tokenizer.eos_token_id,\n",
    "                \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "                \"use_cache\": True,\n",
    "            }\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**generation_kwargs)\n",
    "            \n",
    "            raw_result = processor.decode(\n",
    "                outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Apply ultra-aggressive repetition control\n",
    "            result = repetition_controller.clean_response(raw_result)\n",
    "            \n",
    "        elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "            result = model.chat(\n",
    "                tokenizer=tokenizer,\n",
    "                pixel_values=pixel_values,\n",
    "                question=test_prompt,\n",
    "                generation_config={\n",
    "                    \"max_new_tokens\": 96, \n",
    "                    \"do_sample\": False\n",
    "                }\n",
    "            )\n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "            result = repetition_controller.clean_response(result)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Ultra-strict analysis of results\n",
    "        if repetition_controller.detect_repetitive_generation(result):\n",
    "            print(f\"❌ STILL REPETITIVE ({elapsed:.1f}s): {result[:60]}...\")\n",
    "            print(f\"   Even ultra-aggressive cleaning failed - model has fundamental repetition issue\")\n",
    "        elif any(phrase in result.lower() for phrase in [\"not able\", \"cannot provide\", \"sorry\"]):\n",
    "            print(f\"⚠️ Safety mode triggered ({elapsed:.1f}s): {result[:60]}...\")\n",
    "        elif len(result.strip()) < 3:\n",
    "            print(f\"⚠️ Over-cleaned ({elapsed:.1f}s): '{result}' - may be too aggressive\")\n",
    "        else:\n",
    "            print(f\"✅ SUCCESS ({elapsed:.1f}s): {result[:80]}...\")\n",
    "            print(f\"   Length: {len(result)} chars - repetition eliminated\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)[:100]}...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n🎯 ULTRA-AGGRESSIVE REPETITION CONTROL FEATURES:\")\n",
    "print(\"🔥 UltraAggressiveRepetitionController - Nuclear option for repetition\")\n",
    "print(\"🔥 Stricter thresholds:\")\n",
    "print(\"   - Word repetition: 15% threshold (was 30%)\")  \n",
    "print(\"   - Phrase repetition: 2 occurrences trigger (was 3)\")\n",
    "print(\"   - Sentence repetition: Any duplicate removed\")\n",
    "print(\"🔥 Toxic pattern targeting:\")\n",
    "print(\"   - 'THANK YOU FOR SHOPPING...' pattern recognition\")\n",
    "print(\"   - 'All prices include GST...' pattern recognition\")\n",
    "print(\"   - LaTeX artifact removal\")\n",
    "print(\"🔥 Early truncation at first repetition detection\")\n",
    "print(\"🔥 Max 3 occurrences per word across entire text\")\n",
    "print(\"🔥 Ultra-short token limits (384 main, 96 tests)\")\n",
    "print(\"🔥 Aggressive artifact cleaning (punctuation, parentheses, etc.)\")\n",
    "print(\"\\n💡 If this still shows repetition, the issue is in the model's generation\")\n",
    "print(\"   pattern itself, not the post-processing cleaning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up memory...\n",
      "✓ Model deleted\n",
      "✓ Processor deleted\n",
      "✓ CUDA cache cleared\n",
      "✓ Memory cleanup completed\n",
      "\n",
      "🎉 Test completed!\n",
      "\n",
      "📋 SUMMARY OF FIXES APPLIED:\n",
      "1. ❌ FIXED: Removed repetition_penalty (causes CUDA assert errors)\n",
      "2. ✅ SAFE: Using minimal generation parameters\n",
      "3. 🔧 ROBUST: Added proper error handling\n",
      "4. 🧹 CLEAN: Safe memory cleanup with existence checks\n",
      "\n",
      "🚀 Ready for testing on remote machine!\n"
     ]
    }
   ],
   "source": [
    "# Memory cleanup\n",
    "print(\"Cleaning up memory...\")\n",
    "\n",
    "# Safe cleanup with existence checks\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    try:\n",
    "        del model\n",
    "        print(\"✓ Model deleted\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if CONFIG[\"model_type\"] == \"llama\":\n",
    "    if 'processor' in locals() or 'processor' in globals():\n",
    "        try:\n",
    "            del processor\n",
    "            print(\"✓ Processor deleted\")\n",
    "        except:\n",
    "            pass\n",
    "elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "    if 'tokenizer' in locals() or 'tokenizer' in globals():\n",
    "        try:\n",
    "            del tokenizer\n",
    "            print(\"✓ Tokenizer deleted\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"✓ CUDA cache cleared\")\n",
    "\n",
    "print(\"✓ Memory cleanup completed\")\n",
    "print(\"\\n🎉 Test completed!\")\n",
    "print(\"\\n📋 SUMMARY OF FIXES APPLIED:\")\n",
    "print(\"1. ❌ FIXED: Removed repetition_penalty (causes CUDA assert errors)\")\n",
    "print(\"2. ✅ SAFE: Using minimal generation parameters\")\n",
    "print(\"3. 🔧 ROBUST: Added proper error handling\")\n",
    "print(\"4. 🧹 CLEAN: Safe memory cleanup with existence checks\")\n",
    "print(\"\\n🚀 Ready for testing on remote machine!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Document Type Classification for Taxpayer Work-Related Expense Substantiation\nprint(\"🏛️ TAXPAYER WORK-RELATED EXPENSE DOCUMENT CLASSIFICATION\")\nprint(\"=\" * 70)\n\n# Standard document types for taxpayer substantiation\nSTANDARD_DOCUMENT_TYPES = [\n    \"FUEL_RECEIPT\",           # Fuel and automotive expenses\n    \"BUSINESS_RECEIPT\",       # General business purchases  \n    \"TAX_INVOICE\",           # Business-to-business transactions\n    \"BANK_STATEMENT\",        # Financial transaction records\n    \"MEAL_RECEIPT\",          # Business meal expenses\n    \"ACCOMMODATION_RECEIPT\", # Travel accommodation\n    \"TRAVEL_DOCUMENT\",       # Transport tickets, boarding passes\n    \"PARKING_TOLL_RECEIPT\",  # Parking and toll expenses\n    \"PROFESSIONAL_SERVICES\", # Consultancy, legal, accounting\n    \"EQUIPMENT_SUPPLIES\",    # Office supplies, equipment purchases\n    \"OTHER_BUSINESS\"         # Other legitimate business expenses\n]\n\n# Create classification prompt\nclassification_prompt = f\"\"\"<|image|>Classify this business document for taxpayer work-related expense substantiation.\n\nChoose EXACTLY ONE category from this list:\n{chr(10).join([f\"- {doc_type}\" for doc_type in STANDARD_DOCUMENT_TYPES])}\n\nRequirements:\n- Analyze document content, layout, and business purpose\n- Choose the MOST SPECIFIC applicable category\n- If document doesn't clearly fit any category, use OTHER_BUSINESS\n- Provide brief justification (max 20 words)\n\nFormat:\nCLASSIFICATION: [CATEGORY_NAME]\nJUSTIFICATION: [brief reason]\"\"\"\n\nprint(f\"Classification Categories ({len(STANDARD_DOCUMENT_TYPES)} types):\")\nfor i, doc_type in enumerate(STANDARD_DOCUMENT_TYPES, 1):\n    print(f\"  {i:2d}. {doc_type}\")\n\nprint(f\"\\nPrompt length: {len(classification_prompt)} characters\")\nprint(f\"Test image: {CONFIG['test_image']}\")\n\n# Test classification with both models if available\nclassification_results = {}\n\nfor model_name in [\"llama\", \"internvl\"]:\n    print(f\"\\n{'-' * 50}\")\n    print(f\"🔍 Testing {model_name.upper()} Classification...\")\n    \n    try:\n        # Temporarily switch model for testing\n        original_model_type = CONFIG[\"model_type\"]\n        CONFIG[\"model_type\"] = model_name\n        model_path = CONFIG[\"model_paths\"][model_name]\n        \n        print(f\"Loading {model_name} model...\")\n        start_load = time.time()\n        \n        if model_name == \"llama\":\n            # Load Llama model for classification\n            if 'model' in locals():\n                del model\n                torch.cuda.empty_cache()\n                \n            processor = AutoProcessor.from_pretrained(\n                model_path, trust_remote_code=True, local_files_only=True\n            )\n            \n            model_loading_args = {\n                \"low_cpu_mem_usage\": True,\n                \"torch_dtype\": torch.float16,\n                \"device_map\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n                \"local_files_only\": True\n            }\n            \n            if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n                try:\n                    from transformers import BitsAndBytesConfig\n                    quantization_config = BitsAndBytesConfig(\n                        load_in_8bit=True,\n                        llm_int8_enable_fp32_cpu_offload=True,\n                        llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n                    )\n                    model_loading_args[\"quantization_config\"] = quantization_config\n                except ImportError:\n                    pass\n            \n            model = MllamaForConditionalGeneration.from_pretrained(\n                model_path, **model_loading_args\n            ).eval()\n            \n            load_time = time.time() - start_load\n            print(f\"✅ Llama loaded in {load_time:.1f}s\")\n            \n            # Run classification\n            start_inference = time.time()\n            \n            inputs = processor(text=classification_prompt, images=image, return_tensors=\"pt\")\n            device = next(model.parameters()).device\n            if device.type != \"cpu\":\n                device_target = str(device).split(\":\")[0] if \":\" in str(device) else str(device)\n                inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n            \n            generation_kwargs = {\n                **inputs,\n                \"max_new_tokens\": 64,  # Short response for classification\n                \"do_sample\": False,\n                \"pad_token_id\": processor.tokenizer.eos_token_id,\n                \"eos_token_id\": processor.tokenizer.eos_token_id,\n                \"use_cache\": True,\n            }\n            \n            with torch.no_grad():\n                outputs = model.generate(**generation_kwargs)\n            \n            raw_response = processor.decode(\n                outputs[0][inputs[\"input_ids\"].shape[-1]:],\n                skip_special_tokens=True\n            )\n            \n            # Clean response (minimal cleaning for classification)\n            response = raw_response.strip()\n            if len(response) > 200:  # If too long, truncate\n                response = response[:200] + \"...\"\n                \n            inference_time = time.time() - start_inference\n            \n        elif model_name == \"internvl\":\n            # Load InternVL model for classification  \n            if 'model' in locals():\n                del model\n                torch.cuda.empty_cache()\n                \n            tokenizer = AutoTokenizer.from_pretrained(\n                model_path, trust_remote_code=True, local_files_only=True\n            )\n            \n            model_kwargs = {\n                \"low_cpu_mem_usage\": True,\n                \"trust_remote_code\": True,\n                \"torch_dtype\": torch.bfloat16,\n                \"local_files_only\": True\n            }\n            \n            if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n                try:\n                    model_kwargs[\"load_in_8bit\"] = True\n                except Exception:\n                    pass\n            \n            model = AutoModel.from_pretrained(model_path, **model_kwargs).eval()\n            \n            if torch.cuda.is_available() and not CONFIG[\"enable_quantization\"]:\n                model = model.cuda()\n                \n            load_time = time.time() - start_load\n            print(f\"✅ InternVL loaded in {load_time:.1f}s\")\n            \n            # Run classification\n            start_inference = time.time()\n            \n            # Prepare image for InternVL\n            image_size = 448\n            transform = T.Compose([\n                T.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n                T.ToTensor(),\n                T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n            ])\n            \n            pixel_values = transform(image).unsqueeze(0)\n            if torch.cuda.is_available():\n                pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n            \n            generation_config = {\n                \"max_new_tokens\": 64,\n                \"do_sample\": False,\n                \"pad_token_id\": tokenizer.eos_token_id\n            }\n            \n            response = model.chat(\n                tokenizer=tokenizer,\n                pixel_values=pixel_values,\n                question=classification_prompt,\n                generation_config=generation_config\n            )\n            \n            if isinstance(response, tuple):\n                response = response[0]\n                \n            inference_time = time.time() - start_inference\n        \n        # Analyze classification result\n        print(f\"✅ Classification completed in {inference_time:.1f}s\")\n        print(f\"Response length: {len(response)} characters\")\n        \n        # Extract classification and justification\n        import re\n        classification_match = re.search(r'CLASSIFICATION:\\s*([A-Z_]+)', response)\n        justification_match = re.search(r'JUSTIFICATION:\\s*([^\\n]+)', response)\n        \n        extracted_classification = classification_match.group(1) if classification_match else \"UNKNOWN\"\n        extracted_justification = justification_match.group(1) if justification_match else \"No justification provided\"\n        \n        # Validate classification\n        is_valid = extracted_classification in STANDARD_DOCUMENT_TYPES\n        \n        classification_results[model_name] = {\n            \"classification\": extracted_classification,\n            \"justification\": extracted_justification,\n            \"valid\": is_valid,\n            \"inference_time\": inference_time,\n            \"load_time\": load_time,\n            \"raw_response\": response\n        }\n        \n        print(f\"📋 CLASSIFICATION: {extracted_classification}\")\n        print(f\"📝 JUSTIFICATION: {extracted_justification}\")\n        print(f\"✅ VALID: {'Yes' if is_valid else 'No'}\")\n        \n        if not is_valid:\n            print(f\"⚠️  '{extracted_classification}' not in standard categories\")\n            # Find closest match\n            from difflib import get_close_matches\n            close_matches = get_close_matches(extracted_classification, STANDARD_DOCUMENT_TYPES, n=3, cutoff=0.6)\n            if close_matches:\n                print(f\"🔍 Similar categories: {', '.join(close_matches)}\")\n        \n        # Memory cleanup\n        if 'model' in locals():\n            del model\n        if model_name == \"llama\" and 'processor' in locals():\n            del processor\n        elif model_name == \"internvl\" and 'tokenizer' in locals():\n            del tokenizer\n        torch.cuda.empty_cache()\n        \n    except Exception as e:\n        print(f\"❌ {model_name.upper()} classification failed: {str(e)[:100]}...\")\n        classification_results[model_name] = {\n            \"error\": str(e),\n            \"inference_time\": 0,\n            \"load_time\": 0\n        }\n    \n    # Restore original model type\n    CONFIG[\"model_type\"] = original_model_type\n\n# Final comparison\nprint(f\"\\n{'=' * 70}\")\nprint(\"🏆 CLASSIFICATION COMPARISON RESULTS\")\nprint(f\"{'=' * 70}\")\n\ncomparison_table = []\ncomparison_table.append([\"Model\", \"Classification\", \"Valid\", \"Time (s)\", \"Justification\"])\ncomparison_table.append([\"-\" * 10, \"-\" * 15, \"-\" * 5, \"-\" * 8, \"-\" * 30])\n\nfor model_name, result in classification_results.items():\n    if \"error\" not in result:\n        comparison_table.append([\n            model_name.upper(),\n            result[\"classification\"],\n            \"✅\" if result[\"valid\"] else \"❌\",\n            f\"{result['inference_time']:.1f}\",\n            result[\"justification\"][:30] + \"...\" if len(result[\"justification\"]) > 30 else result[\"justification\"]\n        ])\n    else:\n        comparison_table.append([\n            model_name.upper(),\n            \"ERROR\",\n            \"❌\",\n            \"0.0\",\n            result[\"error\"][:30] + \"...\"\n        ])\n\n# Print table\nfor row in comparison_table:\n    print(f\"{row[0]:<10} {row[1]:<15} {row[2]:<5} {row[3]:<8} {row[4]}\")\n\n# Analysis\nsuccessful_results = [result for result in classification_results.values() if \"error\" not in result]\nif len(successful_results) >= 2:\n    print(f\"\\n🔍 ANALYSIS:\")\n    \n    # Check agreement\n    classifications = [result[\"classification\"] for result in successful_results]\n    if len(set(classifications)) == 1:\n        print(f\"✅ MODELS AGREE: Both classified as {classifications[0]}\")\n    else:\n        print(f\"⚠️  MODELS DISAGREE: {', '.join(classifications)}\")\n    \n    # Performance comparison\n    times = [result[\"inference_time\"] for result in successful_results]\n    fastest_idx = times.index(min(times))\n    model_names = [name for name in classification_results.keys() if \"error\" not in classification_results[name]]\n    \n    print(f\"⚡ FASTEST: {model_names[fastest_idx].upper()} ({min(times):.1f}s)\")\n    \n    # Validity check\n    valid_results = [result for result in successful_results if result[\"valid\"]]\n    if valid_results:\n        print(f\"✅ VALID CLASSIFICATIONS: {len(valid_results)}/{len(successful_results)}\")\n    else:\n        print(f\"⚠️  NO VALID CLASSIFICATIONS PRODUCED\")\n\nprint(f\"\\n📚 DOCUMENT TYPE STANDARD:\")\nprint(f\"This test validates compliance with taxpayer work-related expense\")\nprint(f\"substantiation requirements using {len(STANDARD_DOCUMENT_TYPES)} standard categories.\")\nprint(f\"\\n🎯 For production use:\")\nprint(f\"- Use the model that produces valid classifications consistently\")\nprint(f\"- Consider ensemble approach if models disagree frequently\")\nprint(f\"- Monitor classification accuracy against manual validation\")\n\nprint(f\"\\n✅ Document classification test completed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vision_env)",
   "language": "python",
   "name": "vision_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
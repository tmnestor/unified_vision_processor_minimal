{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Vision Model Test\n",
    "\n",
    "Direct model loading and testing without using the unified_vision_processor package.\n",
    "\n",
    "All configuration is embedded in the notebook for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration - Modify as needed\nCONFIG = {\n    # Model selection: \"llama\" or \"internvl\"\n    \"model_type\": \"llama\",\n    \n    # Model paths\n    \"model_paths\": {\n        \"llama\": \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision\",\n        \"internvl\": \"/home/jovyan/nfs_share/models/InternVL3-8B\"\n    },\n    \n    # Test image path\n    \"test_image\": \"datasets/image14.png\",\n    \n    # PROVEN WORKING prompt pattern for Llama safety bypass\n    \"prompt\": \"<|image|>Extract receipt details in JSON: {\\\"DATE\\\": \\\"\\\", \\\"STORE\\\": \\\"\\\", \\\"TOTAL\\\": \\\"\\\"}\",\n    \n    # Generation parameters - optimized for reliable output\n    \"max_new_tokens\": 128,  # Shorter to prevent repetition\n    \"enable_quantization\": True,\n    \n    # Safety mode bypass settings\n    \"bypass_safety\": True,\n    \"deterministic_generation\": True\n}\n\nprint(f\"Configuration loaded:\")\nprint(f\"Model: {CONFIG['model_type']}\")\nprint(f\"Image: {CONFIG['test_image']}\")\nprint(f\"Prompt: {CONFIG['prompt']}\")\nprint(f\"Safety bypass: {CONFIG['bypass_safety']}\")\nprint(\"\\n✓ Using PROVEN JSON prompt pattern that bypasses Llama safety mode\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful for llama ✓\n"
     ]
    }
   ],
   "source": [
    "# Imports - Direct model loading\n",
    "import time\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Model-specific imports based on selection\n",
    "if CONFIG[\"model_type\"] == \"llama\":\n",
    "    from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    import torchvision.transforms as T\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "print(f\"Imports successful for {CONFIG['model_type']} ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama model from /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision...\n",
      "8-bit quantization enabled (skipping vision modules)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eff79a96e844f34ae638866746bb2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully in 5.61s\n",
      "Model device: cuda:0\n",
      "Quantization active: True\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "model_path = CONFIG[\"model_paths\"][CONFIG[\"model_type\"]]\n",
    "print(f\"Loading {CONFIG['model_type']} model from {model_path}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    if CONFIG[\"model_type\"] == \"llama\":\n",
    "        # Load Llama-3.2-Vision\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        model_kwargs = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,\n",
    "                    llm_int8_enable_fp32_cpu_offload=True,\n",
    "                    llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n",
    "                    llm_int8_threshold=6.0\n",
    "                )\n",
    "                model_kwargs[\"quantization_config\"] = quantization_config\n",
    "                print(\"8-bit quantization enabled (skipping vision modules)\")\n",
    "            except ImportError:\n",
    "                print(\"Quantization not available, using FP16\")\n",
    "                CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            **model_kwargs\n",
    "        ).eval()\n",
    "        \n",
    "        # Configure generation settings\n",
    "        model.generation_config.max_new_tokens = 1024\n",
    "        model.generation_config.do_sample = False\n",
    "        model.generation_config.use_cache = True\n",
    "        \n",
    "    elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "        # Load InternVL3\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        model_kwargs = {\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "        \n",
    "        if CONFIG[\"enable_quantization\"] and torch.cuda.is_available():\n",
    "            try:\n",
    "                model_kwargs[\"load_in_8bit\"] = True\n",
    "                print(\"8-bit quantization enabled\")\n",
    "            except Exception:\n",
    "                print(\"Quantization not available, using bfloat16\")\n",
    "                CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            **model_kwargs\n",
    "        ).eval()\n",
    "        \n",
    "        if torch.cuda.is_available() and not CONFIG[\"enable_quantization\"]:\n",
    "            model = model.cuda()\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"✓ Model loaded successfully in {load_time:.2f}s\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Quantization active: {CONFIG['enable_quantization']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Model loading failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Try loading without quantization as fallback\n",
    "    if CONFIG[\"enable_quantization\"]:\n",
    "        print(\"\\nRetrying without quantization...\")\n",
    "        CONFIG[\"enable_quantization\"] = False\n",
    "        \n",
    "        if CONFIG[\"model_type\"] == \"llama\":\n",
    "            model = MllamaForConditionalGeneration.from_pretrained(\n",
    "                model_path,\n",
    "                low_cpu_mem_usage=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "                local_files_only=True\n",
    "            ).eval()\n",
    "            \n",
    "            # Configure generation settings\n",
    "            model.generation_config.max_new_tokens = 1024\n",
    "            model.generation_config.do_sample = False\n",
    "            model.generation_config.use_cache = True\n",
    "            \n",
    "        elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_path,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                local_files_only=True\n",
    "            ).eval()\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"✓ Model loaded without quantization in {load_time:.2f}s\")\n",
    "        print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    else:\n",
    "        print(\"Cannot proceed without model - please check configuration\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Image loaded: (2048, 2048)\n",
      "  File size: 211.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess image\n",
    "test_image_path = Path(CONFIG[\"test_image\"])\n",
    "\n",
    "if not test_image_path.exists():\n",
    "    print(f\"✗ Test image not found: {test_image_path}\")\n",
    "    available = list(Path(\"datasets\").glob(\"*.png\"))[:5]\n",
    "    print(f\"Available images: {[img.name for img in available]}\")\n",
    "    raise FileNotFoundError(f\"Test image not found: {test_image_path}\")\n",
    "\n",
    "# Load image\n",
    "image = Image.open(test_image_path)\n",
    "if image.mode != \"RGB\":\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "print(f\"✓ Image loaded: {image.size}\")\n",
    "print(f\"  File size: {test_image_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run inference\nprompt = CONFIG[\"prompt\"]\nprint(f\"Running inference with {CONFIG['model_type']}...\")\nprint(f\"Prompt: {prompt}\")\nprint(\"-\" * 50)\n\nstart_time = time.time()\n\ndef clean_response(response: str) -> str:\n    \"\"\"Clean response from repetitive text and artifacts.\"\"\"\n    import re\n    \n    # Remove excessive repetition of ANY word repeated 3+ times consecutively\n    response = re.sub(r'\\b(\\w+)(\\s+\\1){2,}', r'\\1', response, flags=re.IGNORECASE)\n    \n    # Remove excessive repetition of longer phrases\n    response = re.sub(r'\\b((?:\\w+\\s+){1,3})(?:\\1){2,}', r'\\1', response, flags=re.IGNORECASE)\n    \n    # Remove safety warnings and repetitive content\n    safety_patterns = [\n        r\"I'm not able to provide.*?information\\.\",\n        r\"I cannot provide.*?information\\.\",\n        r\"I'm unable to.*?\\.\",\n        r\"I can't.*?\\.\",\n        r\"Sorry, I cannot.*?\\.\"\n    ]\n    \n    for pattern in safety_patterns:\n        response = re.sub(pattern, \"\", response, flags=re.IGNORECASE)\n    \n    # Clean up excessive whitespace and artifacts\n    response = re.sub(r'\\s+', ' ', response)\n    response = re.sub(r'[{}]+', '', response)  # Remove extra braces\n    response = re.sub(r'\\\\+', '', response)    # Remove backslashes\n    \n    # Extract JSON if present\n    json_match = re.search(r'\\{[^{}]*\\}', response)\n    if json_match:\n        response = json_match.group(0)\n    \n    return response.strip()\n\ntry:\n    if CONFIG[\"model_type\"] == \"llama\":\n        # Enhanced Llama inference with safety bypass\n        if not prompt.startswith(\"<|image|>\"):\n            prompt = f\"<|image|>{prompt}\"\n        \n        inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n        \n        # Move to device and ensure contiguity for quantized models\n        if torch.cuda.is_available():\n            device = \"cuda\"\n            inputs = {k: v.to(device).contiguous() if hasattr(v, \"to\") else v for k, v in inputs.items()}\n        else:\n            device = \"cpu\"\n            inputs = {k: v.contiguous() if hasattr(v, \"contiguous\") else v for k, v in inputs.items()}\n        \n        print(f\"Input tensor shapes: {[(k, v.shape) for k, v in inputs.items() if hasattr(v, 'shape')]}\")\n        \n        # Generation settings optimized for safety bypass\n        generation_kwargs = {\n            **inputs,\n            \"max_new_tokens\": CONFIG[\"max_new_tokens\"],\n            \"do_sample\": False,  # Critical for safety bypass\n            \"pad_token_id\": processor.tokenizer.eos_token_id,\n            \"eos_token_id\": processor.tokenizer.eos_token_id,\n            \"use_cache\": True,\n            \"repetition_penalty\": 1.1,  # Reduce repetition\n            \"temperature\": None,  # Force deterministic\n            \"top_p\": None,        # Force deterministic\n            \"top_k\": None         # Force deterministic\n        }\n        \n        with torch.no_grad():\n            outputs = model.generate(**generation_kwargs)\n        \n        raw_response = processor.decode(\n            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n            skip_special_tokens=True\n        )\n        \n        print(f\"Raw response (first 200 chars): {raw_response[:200]}...\")\n        \n        # Clean the response\n        response = clean_response(raw_response)\n        \n    elif CONFIG[\"model_type\"] == \"internvl\":\n        # InternVL inference - generally more stable\n        image_size = 448\n        transform = T.Compose([\n            T.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n            T.ToTensor(),\n            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n        ])\n        \n        pixel_values = transform(image).unsqueeze(0)\n        \n        if torch.cuda.is_available():\n            pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n        else:\n            pixel_values = pixel_values.contiguous()\n        \n        generation_config = {\n            \"max_new_tokens\": CONFIG[\"max_new_tokens\"],\n            \"do_sample\": False,\n            \"pad_token_id\": tokenizer.eos_token_id,\n            \"repetition_penalty\": 1.1\n        }\n        \n        raw_response = model.chat(\n            tokenizer=tokenizer,\n            pixel_values=pixel_values,\n            question=prompt,\n            generation_config=generation_config\n        )\n        \n        if isinstance(raw_response, tuple):\n            raw_response = raw_response[0]\n        \n        # Clean the response\n        response = clean_response(raw_response)\n    \n    inference_time = time.time() - start_time\n    print(f\"✓ Inference completed in {inference_time:.2f}s\")\n    print(f\"Cleaned response: {response}\")\n    \nexcept Exception as e:\n    print(f\"✗ Inference failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    \n    # Fallback: Try without quantization\n    print(\"\\nTrying fallback without quantization...\")\n    if CONFIG[\"model_type\"] == \"llama\":\n        try:\n            # Reload model without quantization\n            print(\"Reloading model without quantization...\")\n            del model\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            model = MllamaForConditionalGeneration.from_pretrained(\n                CONFIG[\"model_paths\"][\"llama\"],\n                low_cpu_mem_usage=True,\n                torch_dtype=torch.float16,\n                device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n                local_files_only=True\n            ).eval()\n            \n            # Retry inference with strict safety bypass\n            inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n            if torch.cuda.is_available():\n                inputs = {k: v.to(\"cuda\").contiguous() if hasattr(v, \"to\") else v for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=CONFIG[\"max_new_tokens\"],\n                    do_sample=False,\n                    pad_token_id=processor.tokenizer.eos_token_id,\n                    repetition_penalty=1.1\n                )\n            \n            raw_response = processor.decode(\n                outputs[0][inputs[\"input_ids\"].shape[-1]:],\n                skip_special_tokens=True\n            )\n            \n            response = clean_response(raw_response)\n            inference_time = time.time() - start_time\n            print(f\"✓ Fallback inference completed in {inference_time:.2f}s\")\n            \n        except Exception as e2:\n            print(f\"✗ Fallback also failed: {e2}\")\n            response = f\"Error: Both primary and fallback inference failed. Primary: {str(e)}, Fallback: {str(e2)}\"\n            inference_time = time.time() - start_time\n    else:\n        response = f\"Error: Inference failed - {str(e)}\"\n        inference_time = time.time() - start_time\n\nprint(f\"Final response ready for display (length: {len(response) if 'response' in locals() else 0} characters)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display results\nprint(\"=\" * 60)\nprint(\"EXTRACTED TEXT:\")\nprint(\"=\" * 60)\nprint(response)\nprint(\"=\" * 60)\n\n# Summary\nprint(f\"\\nSUMMARY:\")\nprint(f\"Model: {CONFIG['model_type']}\")\nprint(f\"Response length: {len(response)} characters\")\nprint(f\"Processing time: {inference_time:.2f}s\")\nprint(f\"Quantization enabled: {CONFIG['enable_quantization']}\")\nprint(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n\n# Enhanced JSON parsing with validation\nprint(f\"\\nRESPONSE ANALYSIS:\")\nif response.strip().startswith('{') and response.strip().endswith('}'):\n    try:\n        import json\n        parsed = json.loads(response.strip())\n        print(f\"✅ VALID JSON EXTRACTED:\")\n        for key, value in parsed.items():\n            print(f\"  {key}: {value}\")\n        \n        # Validate completeness\n        expected_fields = [\"DATE\", \"STORE\", \"TOTAL\"]\n        missing = [field for field in expected_fields if field not in parsed or not parsed[field]]\n        if missing:\n            print(f\"⚠️ Missing fields: {missing}\")\n        else:\n            print(f\"✅ All expected fields present\")\n            \n    except json.JSONDecodeError as e:\n        print(f\"❌ Invalid JSON: {e}\")\n        print(f\"Raw response: {response}\")\n        \nelif any(keyword in response for keyword in [\"DATE:\", \"STORE:\", \"TOTAL:\"]):\n    print(f\"✅ KEY-VALUE format detected\")\n    # Try to extract key-value pairs\n    import re\n    matches = re.findall(r'([A-Z]+):\\s*([^\\n]+)', response)\n    if matches:\n        print(f\"Extracted fields:\")\n        for key, value in matches:\n            print(f\"  {key}: {value.strip()}\")\n            \nelif any(phrase in response.lower() for phrase in [\"not able\", \"cannot provide\", \"sorry\"]):\n    print(f\"❌ SAFETY MODE TRIGGERED\")\n    print(f\"This indicates the prompt triggered Llama's safety restrictions\")\n    print(f\"Solution: Use simpler JSON format prompts\")\n    \nelse:\n    print(f\"⚠️ UNSTRUCTURED RESPONSE\")\n    print(f\"Response doesn't match expected patterns\")\n    print(f\"Consider using different prompt format\")\n\n# Performance assessment\nif inference_time < 30:\n    print(f\"\\n⚡ GOOD performance: {inference_time:.1f}s\")\nelif inference_time < 60:\n    print(f\"\\n⚠️ ACCEPTABLE performance: {inference_time:.1f}s\") \nelse:\n    print(f\"\\n❌ SLOW performance: {inference_time:.1f}s\")\n\nprint(f\"\\n🎯 For production use:\")\nprint(f\"- Llama-3.2-Vision: Use simple JSON prompts only\")\nprint(f\"- InternVL3: More flexible, handles complex prompts better\")\nprint(f\"- Both models: Shorter max_new_tokens prevents issues\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optional: Test different prompts - Using SAFE patterns only\nsafe_test_prompts = [\n    \"<|image|>Extract store and total in JSON: {\\\"STORE\\\": \\\"\\\", \\\"TOTAL\\\": \\\"\\\"}\",\n    \"<|image|>Document type in JSON: {\\\"TYPE\\\": \\\"\\\"}\",\n    \"<|image|>Extract numbers in JSON: {\\\"NUMBERS\\\": \\\"\\\"}\"\n]\n\nprint(\"Testing additional prompts with SAFE JSON patterns only...\\n\")\n\nfor i, test_prompt in enumerate(safe_test_prompts, 1):\n    print(f\"Test {i}: {test_prompt[:60]}...\")\n    try:\n        start = time.time()\n        \n        if CONFIG[\"model_type\"] == \"llama\":\n            inputs = processor(text=test_prompt, images=image, return_tensors=\"pt\")\n            if torch.cuda.is_available():\n                inputs = {k: v.to(\"cuda\").contiguous() if hasattr(v, \"to\") else v for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=64,  # Very short to prevent safety triggers\n                    do_sample=False,\n                    pad_token_id=processor.tokenizer.eos_token_id,\n                    repetition_penalty=1.2\n                )\n            \n            result = processor.decode(\n                outputs[0][inputs[\"input_ids\"].shape[-1]:],\n                skip_special_tokens=True\n            )\n            \n            # Clean result\n            result = clean_response(result)\n            \n        elif CONFIG[\"model_type\"] == \"internvl\":\n            result = model.chat(\n                tokenizer=tokenizer,\n                pixel_values=pixel_values,\n                question=test_prompt,\n                generation_config={\n                    \"max_new_tokens\": 64, \n                    \"do_sample\": False,\n                    \"repetition_penalty\": 1.2\n                }\n            )\n            if isinstance(result, tuple):\n                result = result[0]\n            result = clean_response(result)\n        \n        elapsed = time.time() - start\n        \n        # Check if result is a safety response\n        if any(phrase in result.lower() for phrase in [\"not able\", \"cannot provide\", \"sorry\"]):\n            print(f\"❌ Safety mode triggered ({elapsed:.1f}s): {result[:80]}...\")\n        else:\n            print(f\"✅ Success ({elapsed:.1f}s): {result}\")\n        \n    except Exception as e:\n        print(f\"❌ Error: {e}\")\n    print(\"-\" * 40)\n\nprint(\"\\n💡 TIP: For Llama-3.2-Vision, use ONLY simple JSON format prompts to avoid safety mode.\")\nprint(\"✅ Pattern: '<|image|>Extract [field] in JSON: {\\\"FIELD\\\": \\\"\\\"}'\")\nprint(\"❌ Avoid: Complex instructions, examples, or 'read all text' requests\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up memory...\n",
      "✓ Memory cleaned\n",
      "\n",
      "🎉 Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Memory cleanup\n",
    "print(\"Cleaning up memory...\")\n",
    "\n",
    "del model\n",
    "if CONFIG[\"model_type\"] == \"llama\":\n",
    "    del processor\n",
    "elif CONFIG[\"model_type\"] == \"internvl\":\n",
    "    del tokenizer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"✓ Memory cleaned\")\n",
    "print(\"\\n🎉 Test completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vision_env)",
   "language": "python",
   "name": "vision_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
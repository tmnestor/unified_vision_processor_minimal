{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration - All settings at top of notebook\nprint(\"üèÜ INFORMATION EXTRACTION COMPARISON: Llama 3.2 Vision vs InternVL3\")\nprint(\"üéØ Focus: Information extraction performance with model-specific prompts\")\nprint(\"=\" * 80)\n\n# CONFIGURATION - All settings defined here\nCONFIG = {\n    \"model_paths\": {\n        \"llama\": \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision\",\n        \"internvl\": \"/home/jovyan/nfs_share/models/InternVL3-8B\"\n    },\n    # FIXED: Use exact proven working Llama prompt pattern from CLAUDE.md\n    \"llama_extraction_prompt\": \"\"\"<|image|>Extract data from this receipt in KEY-VALUE format.\n\nOutput format:\nDATE: [date from receipt]\nSTORE: [store name]\nGST: [GST amount]\nTOTAL: [total amount]\nSUBTOTAL: [subtotal amount]\nITEMS: [item names separated by |]\n\nExtract all visible text and format as KEY: VALUE pairs only.\"\"\",\n    \n    # InternVL works better with YAML format\n    \"internvl_extraction_prompt\": \"\"\"<|image|>Extract key information in YAML format:\n\nstore_name: \"\"\ndate: \"\"\ntotal: \"\"\n\nOutput only YAML. Stop after completion.\"\"\",\n    \n    \"max_new_tokens\": 64,\n    \"enable_quantization\": True,\n    \"test_models\": [\"llama\", \"internvl\"],\n    \"test_images\": [\n        (\"image14.png\", \"TAX_INVOICE\"),\n        (\"image65.png\", \"TAX_INVOICE\"), \n        (\"image71.png\", \"TAX_INVOICE\"),\n        (\"image74.png\", \"TAX_INVOICE\"),\n        (\"image205.png\", \"FUEL_RECEIPT\"),\n        (\"image23.png\", \"TAX_INVOICE\"),\n        (\"image45.png\", \"TAX_INVOICE\"),\n        (\"image1.png\", \"BANK_STATEMENT\"),\n        (\"image203.png\", \"BANK_STATEMENT\"),\n        (\"image204.png\", \"FUEL_RECEIPT\"),\n        (\"image206.png\", \"OTHER\"),\n    ]\n}\n\nprint(f\"‚úÖ Configuration loaded:\")\nprint(f\"   - Models: {', '.join(CONFIG['test_models'])}\")\nprint(f\"   - Documents: {len(CONFIG['test_images'])} test images\")\nprint(f\"   - Llama prompt: Proven KEY-VALUE format (bypasses safety mode)\")\nprint(f\"   - InternVL prompt: YAML format (works best)\")\nprint(f\"   - Max tokens: {CONFIG['max_new_tokens']}\")\nprint(f\"   - Quantization: {CONFIG['enable_quantization']}\")\nprint(f\"\\nüìã Ready for step-by-step information extraction comparison\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports and Modular Classes\nimport time\nimport torch\nimport json\nimport re\nimport gc\nfrom pathlib import Path\nfrom PIL import Image\nfrom typing import Dict, List, Tuple, Optional, Any\n\nclass MemoryManager:\n    \"\"\"Memory management utilities for model testing\"\"\"\n    \n    @staticmethod\n    def cleanup_gpu_memory():\n        \"\"\"Minimize memory footprint as requested\"\"\"\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n    \n    @staticmethod\n    def get_memory_usage() -> Dict[str, float]:\n        \"\"\"Get current GPU memory usage in GB\"\"\"\n        if torch.cuda.is_available():\n            return {\n                \"allocated\": torch.cuda.memory_allocated() / 1024**3,\n                \"reserved\": torch.cuda.memory_reserved() / 1024**3\n            }\n        return {\"allocated\": 0.0, \"reserved\": 0.0}\n\nclass UltraAggressiveRepetitionController:\n    \"\"\"Business document repetition detection and cleanup\"\"\"\n    \n    def __init__(self, word_threshold: float = 0.15, phrase_threshold: int = 2):\n        self.word_threshold = word_threshold\n        self.phrase_threshold = phrase_threshold\n        \n        # Business document specific repetition patterns\n        self.toxic_patterns = [\n            r\"THANK YOU FOR SHOPPING WITH US[^.]*\",\n            r\"All prices include GST where applicable[^.]*\",\n            r\"applicable\\.\\s*applicable\\.\",\n            r\"GST where applicable[^.]*applicable\",\n            r\"\\\\+[a-zA-Z]*\\{[^}]*\\}\",  # LaTeX artifacts\n            r\"\\(\\s*\\)\",  # Empty parentheses\n            r\"[.-]\\s*THANK YOU\",\n        ]\n    \n    def clean_response(self, response: str) -> str:\n        \"\"\"Clean business document extraction response\"\"\"\n        if not response or len(response.strip()) == 0:\n            return \"\"\n        \n        # Remove toxic business document patterns\n        response = self._remove_business_patterns(response)\n        \n        # Remove repetitive words and phrases\n        response = self._remove_word_repetition(response)\n        response = self._remove_phrase_repetition(response)\n        \n        # Clean artifacts\n        response = re.sub(r'\\s+', ' ', response)\n        response = re.sub(r'[.]{2,}', '.', response)\n        response = re.sub(r'[!]{2,}', '!', response)\n        \n        return response.strip()\n    \n    def _remove_business_patterns(self, text: str) -> str:\n        \"\"\"Remove business document specific repetitive patterns\"\"\"\n        for pattern in self.toxic_patterns:\n            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n        \n        # Remove excessive \"applicable\" repetition\n        text = re.sub(r'(applicable\\.\\s*){2,}', 'applicable. ', text, flags=re.IGNORECASE)\n        \n        return text\n    \n    def _remove_word_repetition(self, text: str) -> str:\n        \"\"\"Remove word repetition in business documents\"\"\"\n        # Remove consecutive identical words\n        text = re.sub(r'\\b(\\w+)(\\s+\\1){1,}', r'\\1', text, flags=re.IGNORECASE)\n        \n        return text\n    \n    def _remove_phrase_repetition(self, text: str) -> str:\n        \"\"\"Remove phrase repetition\"\"\"\n        for phrase_length in range(2, 7):\n            pattern = r'\\b((?:\\w+\\s+){' + str(phrase_length-1) + r'}\\w+)(\\s+\\1){1,}'\n            text = re.sub(pattern, r'\\1', text, flags=re.IGNORECASE)\n        \n        return text\n\nclass KeyValueExtractionAnalyzer:\n    \"\"\"Analyzer for KEY-VALUE extraction results (updated from YAML to handle Llama safety mode)\"\"\"\n    \n    @staticmethod\n    def analyze(response: str, img_name: str) -> Dict[str, Any]:\n        \"\"\"Analyze KEY-VALUE extraction results with consistent format\"\"\"\n        response_clean = response.strip()\n        \n        # Detect KEY-VALUE format (both YAML and KEY: VALUE patterns)\n        is_structured = bool(re.search(r'(store_name:|date:|total:|STORE:|DATE:|TOTAL:)', response_clean, re.IGNORECASE))\n        \n        # Extract data from both YAML and KEY-VALUE formats\n        # Try YAML format first\n        store_match = re.search(r'(?:store_name|STORE):\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)\n        date_match = re.search(r'(?:date|DATE):\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)  \n        total_match = re.search(r'(?:total|TOTAL):\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)\n        \n        # Fallback detection for non-structured responses\n        if not store_match:\n            store_match = re.search(r'(spotlight|store|business)', response_clean, re.IGNORECASE)\n        if not date_match:\n            date_match = re.search(r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}', response_clean)\n        if not total_match:\n            total_match = re.search(r'(\\$\\d+\\.\\d{2}|\\$\\d+)', response_clean)\n        \n        has_store = bool(store_match)\n        has_date = bool(date_match)\n        has_total = bool(total_match)\n        \n        extraction_score = sum([has_store, has_date, has_total])\n        \n        return {\n            \"img_name\": img_name,\n            \"response\": response_clean,\n            \"is_structured\": is_structured,\n            \"has_store\": has_store,\n            \"has_date\": has_date,\n            \"has_total\": has_total,\n            \"extraction_score\": extraction_score,\n            \"successful\": extraction_score >= 2  # At least 2/3 fields\n        }\n\nclass DatasetManager:\n    \"\"\"Dataset verification and management\"\"\"\n    \n    def __init__(self, datasets_path: str = \"datasets\"):\n        self.datasets_path = Path(datasets_path)\n    \n    def verify_images(self, test_images: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n        \"\"\"Verify that test images exist and return verified list\"\"\"\n        verified_images = []\n        \n        for img_name, doc_type in test_images:\n            img_path = self.datasets_path / img_name\n            if img_path.exists():\n                verified_images.append((img_name, doc_type))\n        \n        return verified_images\n    \n    def print_verification_report(self, test_images: List[Tuple[str, str]], verified_images: List[Tuple[str, str]]):\n        \"\"\"Print dataset verification report\"\"\"\n        print(\"üìä DATASET VERIFICATION\")\n        print(\"=\" * 50)\n        \n        for img_name, doc_type in test_images:\n            img_path = self.datasets_path / img_name\n            if img_path.exists():\n                print(f\"   ‚úÖ {img_name:<12} ‚Üí {doc_type}\")\n            else:\n                print(f\"   ‚ùå {img_name:<12} ‚Üí {doc_type} (MISSING)\")\n        \n        print(f\"\\nüìã Dataset Summary:\")\n        print(f\"   - Expected: {len(test_images)} documents\")\n        print(f\"   - Found: {len(verified_images)} documents\")\n        print(f\"   - Missing: {len(test_images) - len(verified_images)} documents\")\n        \n        if len(verified_images) == 0:\n            print(\"‚ùå No test images found! Check datasets/ directory\")\n            raise FileNotFoundError(\"No test images found\")\n        elif len(verified_images) < len(test_images):\n            print(\"‚ö†Ô∏è Some test images missing but proceeding with available images\")\n        else:\n            print(\"‚úÖ All test images found\")\n\n# Initialize global utilities\nmemory_manager = MemoryManager()\nrepetition_controller = UltraAggressiveRepetitionController()\nextraction_analyzer = KeyValueExtractionAnalyzer()  # Updated name\ndataset_manager = DatasetManager()\n\nprint(\"‚úÖ Modular classes initialized:\")\nprint(\"   - MemoryManager for GPU cleanup\")\nprint(\"   - UltraAggressiveRepetitionController for text cleanup\")\nprint(\"   - KeyValueExtractionAnalyzer for KEY-VALUE results analysis\")\nprint(\"   - DatasetManager for image verification\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Dataset Verification\n# Use the modular DatasetManager class\n\nverified_extraction_images = dataset_manager.verify_images(CONFIG[\"test_images\"])\ndataset_manager.print_verification_report(CONFIG[\"test_images\"], verified_extraction_images)\n\nprint(f\"\\nüî¨ KEY-VALUE Extraction Prompt:\")\nprint(f\"   {CONFIG['extraction_prompt'][:60]}...\")\nprint(f\"\\nüìã Ready for model testing\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model Loading Classes\nclass LlamaModelLoader:\n    \"\"\"Modular Llama model loader with validation\"\"\"\n    \n    @staticmethod\n    def load_model(model_path: str, enable_quantization: bool = True):\n        \"\"\"Load Llama model with proper configuration\"\"\"\n        from transformers import AutoProcessor, MllamaForConditionalGeneration\n        from transformers import BitsAndBytesConfig\n        \n        processor = AutoProcessor.from_pretrained(\n            model_path, trust_remote_code=True, local_files_only=True\n        )\n        \n        model_kwargs = {\n            \"torch_dtype\": torch.float16,\n            \"local_files_only\": True\n        }\n        \n        if enable_quantization:\n            quantization_config = BitsAndBytesConfig(\n                load_in_8bit=True,\n                llm_int8_enable_fp32_cpu_offload=True,\n                llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n            )\n            model_kwargs[\"quantization_config\"] = quantization_config\n        \n        model = MllamaForConditionalGeneration.from_pretrained(\n            model_path, **model_kwargs\n        ).eval()\n        \n        return model, processor\n    \n    @staticmethod\n    def run_inference(model, processor, prompt: str, image, max_new_tokens: int = 64):\n        \"\"\"Run inference with proper device handling\"\"\"\n        inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n        device = next(model.parameters()).device\n        if device.type != \"cpu\":\n            device_target = str(device).split(\":\")[0]\n            inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=False,\n                pad_token_id=processor.tokenizer.eos_token_id,\n                eos_token_id=processor.tokenizer.eos_token_id,\n                use_cache=True,\n            )\n        \n        raw_response = processor.decode(\n            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n            skip_special_tokens=True\n        )\n        \n        # Cleanup tensors immediately\n        del inputs, outputs\n        \n        return raw_response\n\nclass InternVLModelLoader:\n    \"\"\"Modular InternVL model loader with validation\"\"\"\n    \n    @staticmethod\n    def load_model(model_path: str, enable_quantization: bool = True):\n        \"\"\"Load InternVL model with proper configuration\"\"\"\n        from transformers import AutoModel, AutoTokenizer\n        \n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, local_files_only=True\n        )\n        \n        model_kwargs = {\n            \"trust_remote_code\": True,\n            \"torch_dtype\": torch.bfloat16,\n            \"local_files_only\": True\n        }\n        \n        if enable_quantization:\n            model_kwargs[\"load_in_8bit\"] = True\n        \n        model = AutoModel.from_pretrained(\n            model_path, **model_kwargs\n        ).eval()\n        \n        return model, tokenizer\n    \n    @staticmethod\n    def run_inference(model, tokenizer, prompt: str, image, max_new_tokens: int = 64):\n        \"\"\"Run inference with proper image preprocessing\"\"\"\n        import torchvision.transforms as T\n        from torchvision.transforms.functional import InterpolationMode\n        \n        transform = T.Compose([\n            T.Resize((448, 448), interpolation=InterpolationMode.BICUBIC),\n            T.ToTensor(),\n            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n        ])\n        \n        pixel_values = transform(image).unsqueeze(0)\n        if torch.cuda.is_available():\n            pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n        \n        raw_response = model.chat(\n            tokenizer=tokenizer,\n            pixel_values=pixel_values,\n            question=prompt,\n            generation_config={\"max_new_tokens\": max_new_tokens, \"do_sample\": False}\n        )\n        \n        if isinstance(raw_response, tuple):\n            raw_response = raw_response[0]\n        \n        # Cleanup tensors immediately\n        del pixel_values\n        \n        return raw_response\n\ndef get_model_prompt(model_name: str, config: Dict) -> str:\n    \"\"\"Get the appropriate prompt for each model\"\"\"\n    if model_name.lower() == \"llama\":\n        return config[\"llama_extraction_prompt\"]\n    elif model_name.lower() == \"internvl\":\n        return config[\"internvl_extraction_prompt\"]\n    else:\n        # Fallback to llama prompt\n        return config[\"llama_extraction_prompt\"]\n\ndef validate_model(model_loader_class, model_path: str, config: Dict, model_name: str) -> Tuple[bool, Optional[Any], Optional[Any], float]:\n    \"\"\"STEP 1: LOAD MODEL FIRST - separate from prompt testing\"\"\"\n    memory_manager.cleanup_gpu_memory()\n    model_start_time = time.time()\n    \n    try:\n        print(f\"üîÑ STEP 1: Loading model from {model_path}...\")\n        \n        # LOAD MODEL FIRST - no prompts yet\n        model, processor_or_tokenizer = model_loader_class.load_model(\n            model_path, config[\"enable_quantization\"]\n        )\n        \n        model_load_time = time.time() - model_start_time\n        print(f\"‚úÖ Model loaded successfully in {model_load_time:.1f}s\")\n        \n        # STEP 2: Simple validation that model can run basic inference\n        print(f\"üîç STEP 2: Testing basic model functionality...\")\n        img_path = dataset_manager.datasets_path / \"image14.png\"\n        \n        if not img_path.exists():\n            print(f\"‚ùå Test image not found: {img_path}\")\n            del model, processor_or_tokenizer\n            memory_manager.cleanup_gpu_memory()\n            return False, None, None, model_load_time\n        \n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # Use the simplest possible prompt to test model loading (not extraction quality)\n        simple_test_prompt = \"<|image|>What do you see?\"\n        \n        try:\n            raw_response = model_loader_class.run_inference(\n                model, processor_or_tokenizer, simple_test_prompt, \n                image, 32  # Short response for validation\n            )\n            \n            # MODEL VALIDATION: Just check that inference works\n            if raw_response and len(raw_response.strip()) > 0:\n                print(f\"‚úÖ Model validation passed - inference works\")\n                print(f\"   Test response: {raw_response[:50]}...\")\n                return True, model, processor_or_tokenizer, model_load_time\n            else:\n                print(f\"‚ùå Model validation failed - no response\")\n                del model, processor_or_tokenizer\n                memory_manager.cleanup_gpu_memory()\n                return False, None, None, model_load_time\n        \n        except Exception as inference_error:\n            print(f\"‚ùå Model validation failed - inference error: {str(inference_error)[:100]}...\")\n            del model, processor_or_tokenizer\n            memory_manager.cleanup_gpu_memory()\n            return False, None, None, model_load_time\n            \n    except Exception as e:\n        print(f\"‚ùå Model loading failed: {str(e)[:100]}...\")\n        memory_manager.cleanup_gpu_memory()\n        return False, None, None, 0.0\n\nprint(\"‚úÖ Model loader classes defined:\")\nprint(\"   - LlamaModelLoader with validation\")\nprint(\"   - InternVLModelLoader with validation\") \nprint(\"   - get_model_prompt() - Returns model-specific prompts\")\nprint(\"   - validate_model() - STEP 1: Load model, STEP 2: Test basic inference\")\nprint(\"   - SEPARATED: Model loading from prompt application\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Llama Model\nprint(\"üî¨ TESTING LLAMA MODEL\")\nprint(\"=\" * 50)\n\n# Initialize results storage\nextraction_results = {\n    \"llama\": {\"documents\": [], \"successful\": 0, \"total_time\": 0},\n    \"internvl\": {\"documents\": [], \"successful\": 0, \"total_time\": 0}\n}\n\n# Use modular validation function with model name\nllama_valid, llama_model, llama_processor, llama_load_time = validate_model(\n    LlamaModelLoader, \n    CONFIG[\"model_paths\"][\"llama\"], \n    CONFIG,\n    \"llama\"\n)\n\nif llama_valid:\n    print(\"‚úÖ Llama model ready for full testing\")\n    print(f\"üî¨ Using Llama-specific prompt: {CONFIG['llama_extraction_prompt'][:50]}...\")\n    # Store for next cell\n    globals()['llama_model'] = llama_model\n    globals()['llama_processor'] = llama_processor\n    globals()['llama_load_time'] = llama_load_time\nelse:\n    print(\"‚ùå Llama model validation failed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test InternVL Model  \nprint(\"üî¨ TESTING INTERNVL MODEL\")\nprint(\"=\" * 50)\n\n# Use modular validation function with model name\ninternvl_valid, internvl_model, internvl_tokenizer, internvl_load_time = validate_model(\n    InternVLModelLoader,\n    CONFIG[\"model_paths\"][\"internvl\"],\n    CONFIG,\n    \"internvl\"\n)\n\nif internvl_valid:\n    print(\"‚úÖ InternVL model ready for full testing\")\n    print(f\"üî¨ Using InternVL-specific prompt: {CONFIG['internvl_extraction_prompt'][:50]}...\")\n    # Store for next cell\n    globals()['internvl_model'] = internvl_model\n    globals()['internvl_tokenizer'] = internvl_tokenizer\n    globals()['internvl_load_time'] = internvl_load_time\nelse:\n    print(\"‚ùå InternVL model validation failed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run Full Extraction Test - Llama\nif 'llama_model' in globals() and llama_model is not None:\n    print(\"üîç FULL EXTRACTION TEST - LLAMA\")\n    print(\"=\" * 50)\n    print(f\"üéØ Using PROVEN Llama prompt pattern from CLAUDE.md\")\n    \n    # Get Llama-specific prompt\n    llama_prompt = get_model_prompt(\"llama\", CONFIG)\n    \n    total_inference_time = 0\n    \n    for i, (img_name, doc_type) in enumerate(verified_extraction_images, 1):\n        try:\n            img_path = dataset_manager.datasets_path / img_name\n            image = Image.open(img_path).convert(\"RGB\")\n            \n            inference_start = time.time()\n            \n            # Use Llama-specific prompt\n            raw_response = LlamaModelLoader.run_inference(\n                llama_model, llama_processor, llama_prompt,\n                image, CONFIG[\"max_new_tokens\"]\n            )\n            \n            inference_time = time.time() - inference_start\n            total_inference_time += inference_time\n            \n            cleaned_response = repetition_controller.clean_response(raw_response)\n            analysis = extraction_analyzer.analyze(cleaned_response, img_name)\n            analysis[\"inference_time\"] = inference_time\n            analysis[\"doc_type\"] = doc_type\n            \n            extraction_results[\"llama\"][\"documents\"].append(analysis)\n            \n            if analysis[\"successful\"]:\n                extraction_results[\"llama\"][\"successful\"] += 1\n            \n            # Consistent output format as requested\n            status = \"‚úÖ\" if analysis[\"successful\"] else \"‚ùå\"\n            structured_status = \"S\" if analysis[\"is_structured\"] else \"T\"\n            print(f\"   {i:2d}. {img_name:<12} {status} {inference_time:.1f}s | {structured_status} | {analysis['extraction_score']}/3\")\n            \n            # Immediate tensor cleanup - minimizing memory footprint\n            del image\n            \n            # Periodic GPU cleanup every 3 images\n            if i % 3 == 0:\n                memory_manager.cleanup_gpu_memory()\n            \n        except Exception as e:\n            print(f\"   {i:2d}. {img_name:<12} ‚ùå Error: {str(e)[:30]}...\")\n    \n    extraction_results[\"llama\"][\"total_time\"] = total_inference_time\n    extraction_results[\"llama\"][\"avg_time\"] = total_inference_time / len(verified_extraction_images)\n    \n    print(f\"\\nüìä Llama Results:\")\n    print(f\"   Success rate: {extraction_results['llama']['successful']}/{len(verified_extraction_images)}\")\n    print(f\"   Average time: {extraction_results['llama']['avg_time']:.1f}s per document\")\n    \n    # Cleanup Llama model to free memory for InternVL\n    del llama_model, llama_processor\n    memory_manager.cleanup_gpu_memory()\n    \nelse:\n    print(\"‚ö†Ô∏è Llama model not available - skipping full test\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run Full Extraction Test - InternVL\nif 'internvl_model' in globals() and internvl_model is not None:\n    print(\"üîç FULL EXTRACTION TEST - INTERNVL\")\n    print(\"=\" * 50)\n    print(f\"üéØ Using YAML format prompt (works best for InternVL)\")\n    \n    # Get InternVL-specific prompt\n    internvl_prompt = get_model_prompt(\"internvl\", CONFIG)\n    \n    total_inference_time = 0\n    \n    for i, (img_name, doc_type) in enumerate(verified_extraction_images, 1):\n        try:\n            img_path = dataset_manager.datasets_path / img_name\n            image = Image.open(img_path).convert(\"RGB\")\n            \n            inference_start = time.time()\n            \n            # Use InternVL-specific prompt\n            raw_response = InternVLModelLoader.run_inference(\n                internvl_model, internvl_tokenizer, internvl_prompt,\n                image, CONFIG[\"max_new_tokens\"]\n            )\n            \n            inference_time = time.time() - inference_start\n            total_inference_time += inference_time\n            \n            cleaned_response = repetition_controller.clean_response(raw_response)\n            analysis = extraction_analyzer.analyze(cleaned_response, img_name)\n            analysis[\"inference_time\"] = inference_time\n            analysis[\"doc_type\"] = doc_type\n            \n            extraction_results[\"internvl\"][\"documents\"].append(analysis)\n            \n            if analysis[\"successful\"]:\n                extraction_results[\"internvl\"][\"successful\"] += 1\n            \n            # Consistent output format as requested\n            status = \"‚úÖ\" if analysis[\"successful\"] else \"‚ùå\"\n            structured_status = \"S\" if analysis[\"is_structured\"] else \"T\"\n            print(f\"   {i:2d}. {img_name:<12} {status} {inference_time:.1f}s | {structured_status} | {analysis['extraction_score']}/3\")\n            \n            # Immediate tensor cleanup - minimizing memory footprint\n            del image\n            \n            # Periodic GPU cleanup every 3 images\n            if i % 3 == 0:\n                memory_manager.cleanup_gpu_memory()\n            \n        except Exception as e:\n            print(f\"   {i:2d}. {img_name:<12} ‚ùå Error: {str(e)[:30]}...\")\n    \n    extraction_results[\"internvl\"][\"total_time\"] = total_inference_time\n    extraction_results[\"internvl\"][\"avg_time\"] = total_inference_time / len(verified_extraction_images)\n    \n    print(f\"\\nüìä InternVL Results:\")\n    print(f\"   Success rate: {extraction_results['internvl']['successful']}/{len(verified_extraction_images)}\")\n    print(f\"   Average time: {extraction_results['internvl']['avg_time']:.1f}s per document\")\n    \n    # Cleanup InternVL model \n    del internvl_model, internvl_tokenizer\n    memory_manager.cleanup_gpu_memory()\n    \nelse:\n    print(\"‚ö†Ô∏è InternVL model not available - skipping full test\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final Comparison and Recommendation\nclass ResultsAnalyzer:\n    \"\"\"Modular results analysis and comparison\"\"\"\n    \n    @staticmethod\n    def print_final_comparison(extraction_results: Dict, verified_images: List):\n        \"\"\"Print final comparison between models\"\"\"\n        print(f\"\\n{'=' * 80}\")\n        print(\"üèÜ FINAL RECOMMENDATION: BEST MODEL FOR INFORMATION EXTRACTION\")\n        print(f\"{'=' * 80}\")\n        \n        # Compare both models' performance\n        llama_success = 0\n        llama_total = 0\n        llama_avg_time = 0\n        internvl_success = 0\n        internvl_total = 0\n        internvl_avg_time = 0\n        \n        if extraction_results[\"llama\"][\"documents\"]:\n            llama_total = len(extraction_results[\"llama\"][\"documents\"])\n            llama_success = extraction_results[\"llama\"][\"successful\"]\n            llama_avg_time = extraction_results[\"llama\"][\"avg_time\"]\n        \n        if extraction_results[\"internvl\"][\"documents\"]:\n            internvl_total = len(extraction_results[\"internvl\"][\"documents\"])\n            internvl_success = extraction_results[\"internvl\"][\"successful\"]\n            internvl_avg_time = extraction_results[\"internvl\"][\"avg_time\"]\n        \n        print(f\"üìä INFORMATION EXTRACTION COMPARISON:\")\n        print(f\"{'Model':<12} {'Success Rate':<15} {'Avg Time':<12} {'Best For'}\")\n        print(\"-\" * 60)\n        \n        if llama_total > 0:\n            llama_rate = llama_success / llama_total * 100\n            print(f\"{'LLAMA':<12} {llama_rate:.1f}% ({llama_success}/{llama_total}){'':<5} {llama_avg_time:.1f}s{'':<7} Large context\")\n        \n        if internvl_total > 0:\n            internvl_rate = internvl_success / internvl_total * 100\n            print(f\"{'INTERNVL':<12} {internvl_rate:.1f}% ({internvl_success}/{internvl_total}){'':<5} {internvl_avg_time:.1f}s{'':<7} Production speed\")\n        \n        # Make recommendation\n        if internvl_total > 0 and llama_total > 0:\n            internvl_rate = internvl_success / internvl_total * 100\n            llama_rate = llama_success / llama_total * 100\n            \n            if internvl_rate > llama_rate:\n                recommended = \"INTERNVL\"\n                reason = f\"Higher success rate ({internvl_rate:.1f}% vs {llama_rate:.1f}%) and faster inference\"\n            elif llama_rate > internvl_rate:\n                recommended = \"LLAMA\"\n                reason = f\"Higher success rate ({llama_rate:.1f}% vs {internvl_rate:.1f}%)\"\n            else:\n                recommended = \"INTERNVL\"\n                reason = f\"Equal success rate but {internvl_avg_time/llama_avg_time:.1f}x faster inference\"\n            \n            print(f\"\\nü•á RECOMMENDED FOR INFORMATION EXTRACTION: {recommended}\")\n            print(f\"   Reason: {reason}\")\n            print(f\"   Use case: Business document processing (receipts, invoices, statements)\")\n        elif internvl_total > 0:\n            print(f\"\\nü•á RECOMMENDED: INTERNVL (only model tested successfully)\")\n        elif llama_total > 0:\n            print(f\"\\nü•á RECOMMENDED: LLAMA (only model tested successfully)\")\n        else:\n            print(f\"\\n‚ö†Ô∏è No successful tests - investigate model loading issues\")\n        \n        print(f\"\\n‚úÖ COMPLETE: Information extraction performance comparison finished!\")\n        print(f\"üìã This answers the user's question about best model for their information extraction job\")\n\n# Use the modular analyzer\nresults_analyzer = ResultsAnalyzer()\nresults_analyzer.print_final_comparison(extraction_results, verified_extraction_images)\n\n# Show the model-specific prompts being used\nprint(f\"\\nüî¨ MODEL-SPECIFIC PROMPTS USED:\")\nprint(f\"{'='*50}\")\nprint(f\"üî• LLAMA PROMPT (Proven pattern from CLAUDE.md):\")\nprint(CONFIG[\"llama_extraction_prompt\"])\nprint(f\"\\n{'='*50}\")\nprint(f\"üéØ INTERNVL PROMPT (YAML format):\")\nprint(CONFIG[\"internvl_extraction_prompt\"])\nprint(f\"{'='*50}\")\nprint(f\"‚úÖ Confirmed: Using model-optimized prompts for maximum performance\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
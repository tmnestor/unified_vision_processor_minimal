{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f893fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\"\n",
    "# here, specify the name of the image\n",
    "imageName = \"/home/jovyan/nfs_share/tod/datasets/synthetic_invoice_014.png\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# open the image\n",
    "image = Image.open(imageName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430be3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a message data structure\n",
    "messageDataStructure = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"What type of document is this?\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text input\n",
    "textInput = processor.apply_chat_template(\n",
    "    messageDataStructure, add_generation_prompt=True\n",
    ")\n",
    "# call the processor\n",
    "inputs = processor(image, textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# here, change the number of tokens to get a more detailed answer\n",
    "output = model.generate(**inputs, max_new_tokens=2000)\n",
    "# here, we decode and store the response so we can print it\n",
    "generatedOutput = processor.decode(output[0])\n",
    "\n",
    "print(generatedOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the answer in a file\n",
    "with Path(\"/home/jovyan/nfs_share/tod/output/llama_output.txt\").open(\n",
    "    \"w\", encoding=\"utf-8\"\n",
    ") as text_file:\n",
    "    text_file.write(generatedOutput)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

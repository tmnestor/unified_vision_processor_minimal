{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nCell 1: Environment Setup and Model Loading for Key-Value Extraction\n\nPurpose:\n- Import all required libraries for InternVL3-2B vision-language model\n- Load the InternVL3-2B model following official documentation best practices\n- Initialize model with proper dtype and settings for optimal inference\n- Define global configuration variables for data paths\n\nKey Components (Following Official InternVL3 Documentation):\n- torch.bfloat16: Recommended precision for optimal performance\n- use_flash_attn=True: Enable Flash Attention for better efficiency (recommended)\n- low_cpu_mem_usage=True: Optimize CPU memory during loading\n- trust_remote_code=True: Allow loading custom model code from HuggingFace\n- .eval().cuda(): Set model to evaluation mode and move to GPU\n\nGlobal Configuration:\n- data_dir: Centralized data directory path for all image operations\n- model_path: Local path to InternVL3-2B model files\n- output_dir: Directory for saving extraction results\n\nOfficial Requirements:\n- transformers>=4.37.2\n- Flash Attention support for optimal performance\n- Proper dtype consistency throughout the pipeline\n\"\"\"\n\nfrom pathlib import Path\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nimport torchvision.transforms as T\n\n# Check transformers version (should be >=4.37.2)\nimport transformers\nprint(f\"üîç Transformers version: {transformers.__version__}\")\n\n# Global configuration variables\ndata_dir = \"/home/jovyan/nfs_share/tod/huaifeng_data\"\nmodel_path = \"/home/jovyan/nfs_share/models/InternVL3-2B\" \noutput_dir = \"/home/jovyan/nfs_share/tod/output\"\n\nprint(f\"üóÇÔ∏è  Data directory: {data_dir}\")\nprint(f\"üìÅ Output directory: {output_dir}\")\nprint(f\"üîß Loading InternVL3-2B model following official documentation from: {model_path}\")\n\n# Load model with official recommended settings (following InternVL documentation)\nmodel = AutoModel.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,   # Official recommendation: use bfloat16\n    low_cpu_mem_usage=True,       # Optimize CPU memory during loading\n    use_flash_attn=True,          # Enable Flash Attention (recommended)\n    trust_remote_code=True        # Allow custom model code execution\n).eval().cuda()                   # Set to evaluation mode and move to GPU\n\n# Load tokenizer with official settings\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path, \n    trust_remote_code=True,  # Allow custom tokenizer code\n    use_fast=False          # Use slower but more reliable tokenizer for structured tasks\n)\n\nprint(\"‚úÖ Model and tokenizer loaded successfully following official InternVL3 guidelines\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nCell 2: Official InternVL3 Dynamic Image Processing Pipeline\n\nPurpose:\n- Implement official InternVL3 dynamic image preprocessing following documentation\n- Support dynamic tiling with proper dtype consistency\n- Handle document formats with optimal preprocessing for text extraction\n\nOfficial Dynamic Preprocessing Features (from InternVL3 docs):\n1. build_transform(): Official transformation pipeline with proper normalization\n2. find_closest_aspect_ratio(): Aspect ratio optimization for multiple tiles\n3. dynamic_preprocess(): Official dynamic tiling algorithm (1-12 tiles max)\n4. load_image(): Complete preprocessing with proper dtype handling\n\nKey Requirements from Documentation:\n- Proper dtype consistency (bfloat16 throughout pipeline)\n- ImageNet normalization constants\n- BICUBIC interpolation for quality\n- Dynamic tiling with thumbnail support\n- Memory-safe processing with configurable max_num\n\"\"\"\n\nimport math\n\n# Official ImageNet normalization constants\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\ndef build_transform(input_size):\n    \"\"\"\n    Official InternVL3 image transformation pipeline\n    \n    Args:\n        input_size: Target size for image resizing (default 448)\n    \n    Returns:\n        torchvision.transforms.Compose: Official transformation pipeline\n    \"\"\"\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n    transform = T.Compose([\n        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n        T.Resize((input_size, input_size), interpolation=T.InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=MEAN, std=STD)\n    ])\n    return transform\n\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    \"\"\"\n    Official InternVL3 aspect ratio optimization algorithm\n    \"\"\"\n    best_ratio_diff = float('inf')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio\n\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n    \"\"\"\n    Official InternVL3 dynamic preprocessing algorithm\n    \"\"\"\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # Calculate the existing image aspect ratio\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # Find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # Calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # Resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # Split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n\ndef load_image(image_file, input_size=448, max_num=12):\n    \"\"\"\n    Official InternVL3 image loading with proper dtype handling\n    \n    Args:\n        image_file: Path to image file (relative to data_dir or absolute)\n        input_size: Target size for each tile\n        max_num: Maximum number of tiles to generate (1-12 as per docs)\n    \n    Returns:\n        torch.Tensor: Properly processed image tensor with correct dtype (bfloat16)\n    \"\"\"\n    # Handle both relative and absolute paths\n    if not image_file.startswith('/'):\n        image_file = f\"{data_dir}/{image_file}\"\n    \n    image = Image.open(image_file).convert('RGB')\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    \n    # CRITICAL: Ensure proper dtype for InternVL3 (must match model's bfloat16)\n    return pixel_values.to(torch.bfloat16).cuda()\n\n# Load and process document following official guidelines\ndocument_image = \"image2.png\"  # Configurable document filename\nprint(f\"üìÑ Loading document from: {data_dir}/{document_image}\")\n\n# Load original image for analysis\nimage_path = f\"{data_dir}/{document_image}\"\noriginal_image = Image.open(image_path)\nprint(f\"üì∑ Original document size: {original_image.size}\")\nprint(f\"üìê Document aspect ratio: {original_image.size[0]/original_image.size[1]:.2f}\")\n\n# Process with official dynamic preprocessing\nprint(\"üñºÔ∏è  Processing with official InternVL3 dynamic preprocessing...\")\npixel_values = load_image(document_image, max_num=12)\nprint(f\"‚úÖ Document processed into {pixel_values.shape[0]} tiles: {pixel_values.shape}\")\nprint(f\"üîç Tensor dtype: {pixel_values.dtype} (should be torch.bfloat16)\")\nprint(\"üìã Ready for InternVL3 key-value extraction\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Structured key-value extraction prompt configured\n",
      "üìÑ Prompt length: 1912 characters\n",
      "üîç Extracting 25 standardized business document fields\n",
      "‚öôÔ∏è Configured for deterministic, structured output\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 3: Structured Key-Value Extraction Prompt Configuration\n",
    "\n",
    "Purpose:\n",
    "- Define comprehensive prompt for extracting structured business document data\n",
    "- Configure extraction parameters for consistent, standardized output\n",
    "- Specify exact output format requirements for downstream processing\n",
    "\n",
    "Extraction Specifications:\n",
    "- 25 predefined fields covering common business document types\n",
    "- Supports invoices, receipts, bank statements, and tax documents\n",
    "- Handles missing fields gracefully with \"N/A\" placeholders\n",
    "- Enforces plain text output without markdown formatting\n",
    "- Ensures deterministic field ordering for automated processing\n",
    "\n",
    "Field Categories:\n",
    "1. Document metadata (type, dates)\n",
    "2. Supplier/business information (name, address, contact)\n",
    "3. Financial data (amounts, GST, totals)\n",
    "4. Transaction details (quantities, prices, descriptions)\n",
    "5. Banking information (account numbers, BSB, balances)\n",
    "\n",
    "Output Quality Controls:\n",
    "- Explicit formatting rules to prevent markdown artifacts\n",
    "- Character limits and validation requirements\n",
    "- Structured field validation for downstream systems\n",
    "\"\"\"\n",
    "\n",
    "# Comprehensive key-value extraction prompt optimized for business documents\n",
    "extraction_prompt = \"\"\"Extract data from this business document. \n",
    "Output ALL fields below with their exact keys. \n",
    "Use \"N/A\" if field is not visible or not present.\n",
    "\n",
    "REQUIRED OUTPUT FORMAT (output ALL lines exactly as shown):\n",
    "DOCUMENT_TYPE: [value or N/A]\n",
    "SUPPLIER: [value or N/A]\n",
    "ABN: [11-digit Australian Business Number or N/A]\n",
    "PAYER_NAME: [value or N/A]\n",
    "PAYER_ADDRESS: [value or N/A]\n",
    "PAYER_PHONE: [value or N/A]\n",
    "PAYER_EMAIL: [value or N/A]\n",
    "INVOICE_DATE: [value or N/A]\n",
    "DUE_DATE: [value or N/A]\n",
    "GST: [GST amount in dollars or N/A]\n",
    "TOTAL: [total amount in dollars or N/A]\n",
    "SUBTOTAL: [subtotal amount in dollars or N/A]\n",
    "SUPPLIER_WEBSITE: [value or N/A]\n",
    "QUANTITIES: [list of quantities or N/A]\n",
    "PRICES: [individual prices in dollars or N/A]\n",
    "BUSINESS_ADDRESS: [value or N/A]\n",
    "BUSINESS_PHONE: [value or N/A]\n",
    "BANK_NAME: [bank name from bank statements only or N/A]\n",
    "BSB_NUMBER: [6-digit BSB from bank statements only or N/A]\n",
    "BANK_ACCOUNT_NUMBER: [account number from bank statements only or N/A]\n",
    "ACCOUNT_HOLDER: [value or N/A]\n",
    "STATEMENT_PERIOD: [value or N/A]\n",
    "OPENING_BALANCE: [opening balance amount in dollars or N/A]\n",
    "CLOSING_BALANCE: [closing balance amount in dollars or N/A]\n",
    "DESCRIPTIONS: [list of transaction descriptions or N/A]\n",
    "\n",
    "CRITICAL: Output in PLAIN TEXT format only. Do NOT use markdown formatting.\n",
    "\n",
    "CORRECT format: DOCUMENT_TYPE: TAX INVOICE\n",
    "WRONG format: **DOCUMENT_TYPE:** TAX INVOICE\n",
    "WRONG format: **DOCUMENT_TYPE: TAX INVOICE**\n",
    "WRONG format: DOCUMENT_TYPE: **TAX INVOICE**\n",
    "\n",
    "Use exactly: KEY: value (with colon and space)\n",
    "Never use: **KEY:** or **KEY** or any asterisks\n",
    "Never use bold, italic, or any markdown formatting\n",
    "\n",
    "ABSOLUTELY CRITICAL: Output EXACTLY 25 lines using ONLY the keys listed above. \n",
    "Do NOT add extra fields like \\\"Balance\\\", \\\"Credit\\\", \\\"Debit\\\", \\\"Date\\\", \\\"Description\\\".\n",
    "Do NOT include ANY fields not in the required list above.\n",
    "Include ALL 25 keys listed above even if value is N/A.\n",
    "STOP after exactly 25 lines.\"\"\"\n",
    "\n",
    "# Format prompt for InternVL3 with proper image token\n",
    "question = f'<image>\\n{extraction_prompt}'\n",
    "\n",
    "print(\"üìã Structured key-value extraction prompt configured\")\n",
    "print(f\"üìÑ Prompt length: {len(extraction_prompt)} characters\")\n",
    "print(f\"üîç Extracting 25 standardized business document fields\")\n",
    "print(\"‚öôÔ∏è Configured for deterministic, structured output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Executing key-value extraction with InternVL3...\n",
      "‚öôÔ∏è Using deterministic generation for consistent field extraction\n",
      "‚ùå Error during key-value extraction: Input type (float) and bias type (c10::BFloat16) should be the same\n",
      "üîç Error type: RuntimeError\n",
      "\n",
      "üìã Troubleshooting suggestions:\n",
      "   ‚Ä¢ Check document image quality and readability\n",
      "   ‚Ä¢ Verify model and tokenizer are properly loaded\n",
      "   ‚Ä¢ Ensure sufficient GPU memory for processing\n",
      "   ‚Ä¢ Validate document contains extractable text fields\n",
      "\n",
      "üîß Full error traceback:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_8676/2058508661.py\", line 40, in <module>\n",
      "    response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.cache/huggingface/modules/transformers_modules/InternVL3-2B/modeling_internvl_chat.py\", line 291, in chat\n",
      "    generation_output = self.generate(\n",
      "                        ^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.cache/huggingface/modules/transformers_modules/InternVL3-2B/modeling_internvl_chat.py\", line 326, in generate\n",
      "    vit_embeds = self.extract_feature(pixel_values)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.cache/huggingface/modules/transformers_modules/InternVL3-2B/modeling_internvl_chat.py\", line 186, in extract_feature\n",
      "    vit_embeds = self.vision_model(\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.cache/huggingface/modules/transformers_modules/InternVL3-2B/modeling_intern_vit.py\", line 412, in forward\n",
      "    hidden_states = self.embeddings(pixel_values)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.cache/huggingface/modules/transformers_modules/InternVL3-2B/modeling_intern_vit.py\", line 164, in forward\n",
      "    patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, channel, width, height]\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "RuntimeError: Input type (float) and bias type (c10::BFloat16) should be the same\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 4: Key-Value Extraction Execution and Processing\n",
    "\n",
    "Purpose:\n",
    "- Execute structured field extraction using optimized generation parameters\n",
    "- Process document with InternVL3 model for consistent key-value pairs\n",
    "- Handle extraction errors gracefully with comprehensive error reporting\n",
    "\n",
    "Generation Configuration:\n",
    "- max_new_tokens=1000: Sufficient for 25 structured fields\n",
    "- do_sample=False: Deterministic output for consistent field extraction\n",
    "- pad_token_id=tokenizer.eos_token_id: Prevents padding warnings\n",
    "- Temperature disabled: Ensures reproducible extraction results\n",
    "\n",
    "Error Handling:\n",
    "- Comprehensive exception catching with detailed error reporting\n",
    "- Type-specific error identification for debugging\n",
    "- Stack trace output for development troubleshooting\n",
    "- Graceful failure with actionable error messages\n",
    "\n",
    "Output Validation:\n",
    "- Field count verification (should extract exactly 25 fields)\n",
    "- Format validation for downstream processing\n",
    "- Quality indicators for extraction success assessment\n",
    "\"\"\"\n",
    "\n",
    "# Generation configuration optimized for structured output\n",
    "generation_config = dict(\n",
    "    max_new_tokens=1000,                    # Adequate tokens for 25 structured fields\n",
    "    do_sample=False,                        # Deterministic for consistent field extraction\n",
    "    pad_token_id=tokenizer.eos_token_id     # Prevent pad_token_id warnings\n",
    "    # Note: Temperature omitted since do_sample=False\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Executing key-value extraction with InternVL3...\")\n",
    "print(\"‚öôÔ∏è Using deterministic generation for consistent field extraction\")\n",
    "\n",
    "try:\n",
    "    # Execute structured field extraction\n",
    "    response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "    \n",
    "    print(\"‚úÖ Key-value extraction completed successfully!\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXTRACTED BUSINESS DOCUMENT FIELDS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(response)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic validation of extraction results\n",
    "    lines = response.split('\\n')\n",
    "    field_lines = [line for line in lines if ':' in line and not line.strip().startswith('<')]\n",
    "    print(f\"\\nüìä Extraction Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Total field lines extracted: {len(field_lines)}\")\n",
    "    print(f\"   ‚Ä¢ Expected field count: 25\")\n",
    "    print(f\"   ‚Ä¢ Extraction completeness: {len(field_lines)/25*100:.1f}%\")\n",
    "    \n",
    "    if len(field_lines) == 25:\n",
    "        print(\"‚úÖ Perfect field extraction - all 25 fields captured\")\n",
    "    elif len(field_lines) > 0:\n",
    "        print(\"‚ö†Ô∏è Partial extraction - some fields may be missing\")\n",
    "    else:\n",
    "        print(\"‚ùå No structured fields detected in response\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during key-value extraction: {e}\")\n",
    "    print(f\"üîç Error type: {type(e).__name__}\")\n",
    "    print(\"\\nüìã Troubleshooting suggestions:\")\n",
    "    print(\"   ‚Ä¢ Check document image quality and readability\")\n",
    "    print(\"   ‚Ä¢ Verify model and tokenizer are properly loaded\")\n",
    "    print(\"   ‚Ä¢ Ensure sufficient GPU memory for processing\")\n",
    "    print(\"   ‚Ä¢ Validate document contains extractable text fields\")\n",
    "    \n",
    "    import traceback\n",
    "    print(f\"\\nüîß Full error traceback:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 5: Results Saving and Analysis Pipeline\n",
    "\n",
    "Purpose:\n",
    "- Save extracted key-value pairs to persistent storage for further processing\n",
    "- Perform quality analysis and validation of extraction results\n",
    "- Generate extraction reports and statistics for workflow integration\n",
    "\n",
    "File Operations:\n",
    "- Creates output directory using global output_dir configuration\n",
    "- Uses UTF-8 encoding for proper international character handling\n",
    "- Saves with descriptive filename including timestamp capability\n",
    "- Implements atomic file operations to prevent data corruption\n",
    "\n",
    "Quality Analysis Features:\n",
    "- Field completeness assessment (target: 25 fields)\n",
    "- Content validation for required field formats\n",
    "- Data quality indicators for downstream processing\n",
    "- Extraction confidence metrics and reporting\n",
    "\n",
    "Error Handling:\n",
    "- NameError: Handles case where response variable isn't defined\n",
    "- FileSystem errors: Permission issues, disk space, path problems\n",
    "- Encoding errors: Character set and formatting issues\n",
    "- Provides actionable troubleshooting guidance for each error type\n",
    "\n",
    "Integration Features:\n",
    "- Structured output suitable for database import\n",
    "- JSON-compatible field parsing for API integration\n",
    "- Batch processing support for multiple document workflows\n",
    "\"\"\"\n",
    "\n",
    "# Configure output path using global output_dir variable\n",
    "output_filename = \"internvl3_keyvalue_extraction.txt\"\n",
    "output_path = Path(output_dir) / output_filename\n",
    "\n",
    "print(f\"üíæ Saving extraction results to: {output_path}\")\n",
    "\n",
    "try:\n",
    "    # Ensure output directory exists with proper permissions\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Write extraction results with UTF-8 encoding for international support\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(response)\n",
    "    \n",
    "    print(f\"‚úÖ Key-value extraction results saved successfully!\")\n",
    "    print(f\"üìÑ File location: {output_path}\")\n",
    "    print(f\"üìä File size: {output_path.stat().st_size} bytes\")\n",
    "    \n",
    "    # Advanced extraction analysis and reporting\n",
    "    lines = response.split('\\n')\n",
    "    field_lines = [line for line in lines if ':' in line and not line.strip().startswith('<')]\n",
    "    \n",
    "    print(f\"\\nüìà Detailed Extraction Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Document processed: {document_image}\")\n",
    "    print(f\"   ‚Ä¢ Total response lines: {len(lines)}\")\n",
    "    print(f\"   ‚Ä¢ Structured field lines: {len(field_lines)}\")\n",
    "    print(f\"   ‚Ä¢ Field extraction rate: {len(field_lines)/25*100:.1f}%\")\n",
    "    \n",
    "    # Field content analysis\n",
    "    non_na_fields = [line for line in field_lines if not line.split(':')[1].strip().upper() in ['N/A', 'NA']]\n",
    "    print(f\"   ‚Ä¢ Fields with content: {len(non_na_fields)}\")\n",
    "    print(f\"   ‚Ä¢ Content coverage: {len(non_na_fields)/25*100:.1f}%\")\n",
    "    \n",
    "    # File validation\n",
    "    file_size = output_path.stat().st_size\n",
    "    if file_size > 100:\n",
    "        print(\"‚úÖ Output file validation: PASSED (sufficient content)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Output file validation: WARNING (minimal content detected)\")\n",
    "    \n",
    "    print(f\"\\nüîó Integration ready: Results saved in structured format\")\n",
    "    print(f\"üìÅ Output directory: {output_dir}\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå Error: Extraction response not available\")\n",
    "    print(\"üí° Solution: Execute Cell 4 first to generate extraction results\")\n",
    "    print(\"üîÑ Then re-run this cell to save the results\")\n",
    "    \n",
    "except PermissionError:\n",
    "    print(f\"‚ùå Permission Error: Cannot write to {output_path}\")\n",
    "    print(\"üí° Solutions:\")\n",
    "    print(\"   ‚Ä¢ Check directory write permissions\")\n",
    "    print(\"   ‚Ä¢ Verify output_dir path is accessible\")\n",
    "    print(\"   ‚Ä¢ Try running with appropriate user permissions\")\n",
    "    \n",
    "except OSError as e:\n",
    "    print(f\"‚ùå File System Error: {e}\")\n",
    "    print(\"üí° Solutions:\")\n",
    "    print(\"   ‚Ä¢ Check available disk space\")\n",
    "    print(\"   ‚Ä¢ Verify path validity and accessibility\")\n",
    "    print(\"   ‚Ä¢ Ensure parent directories exist\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error during file operations: {e}\")\n",
    "    print(f\"üîç Error type: {type(e).__name__}\")\n",
    "    print(\"üí° Check system resources and file path configuration\")\n",
    "    print(f\"üóÇÔ∏è Configured output directory: {output_dir}\")\n",
    "    \n",
    "    import traceback\n",
    "    print(f\"\\nüîß Full error details:\")\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
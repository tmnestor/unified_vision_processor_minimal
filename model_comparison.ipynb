{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - All settings at top of notebook\n",
    "import textwrap\n",
    "\n",
    "print(\"üèÜ INFORMATION EXTRACTION COMPARISON: Llama 3.2 Vision vs InternVL3\")\n",
    "print(\"üéØ Focus: Information extraction performance with simplified unified prompts\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# CONFIGURATION - All settings defined here\n",
    "CONFIG = {\n",
    "    \"model_paths\": {\n",
    "        \"llama\": \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision\",\n",
    "        \"internvl\": \"/home/jovyan/nfs_share/models/InternVL3-8B\"\n",
    "    },\n",
    "    # SIMPLIFIED: Back to proven KEY-VALUE format with Australian business requirements\n",
    "    \"extraction_prompt\": textwrap.dedent('''\n",
    "        <|image|>Extract data from this Australian business document in KEY-VALUE format.\n",
    "\n",
    "        Output format:\n",
    "        STORE: [business name]\n",
    "        ABN: [11-digit Australian Business Number]\n",
    "        DATE: [date in DD/MM/YYYY format]\n",
    "        TOTAL: [total amount in AUD]\n",
    "        SUBTOTAL: [subtotal amount]\n",
    "        GST: [GST amount]\n",
    "        ITEMS: [item names separated by |]\n",
    "\n",
    "        ABN is crucial - look for 11-digit numbers formatted as XX XXX XXX XXX or XXXXXXXXXXX. Use Australian date format (DD/MM/YYYY) and include currency symbols. Extract all visible text and format as KEY: VALUE pairs only. Stop after completion.\n",
    "    ''').strip(),\n",
    "\n",
    "    \"max_new_tokens\": 64,  # Back to original limit\n",
    "    \"enable_quantization\": True,\n",
    "    \"test_models\": [\"llama\", \"internvl\"],\n",
    "    \"test_images\": [\n",
    "        (\"image14.png\", \"TAX_INVOICE\"),\n",
    "        (\"image65.png\", \"TAX_INVOICE\"),\n",
    "        (\"image71.png\", \"TAX_INVOICE\"),\n",
    "        (\"image74.png\", \"TAX_INVOICE\"),\n",
    "        (\"image205.png\", \"FUEL_RECEIPT\"),\n",
    "        (\"image23.png\", \"TAX_INVOICE\"),\n",
    "        (\"image45.png\", \"TAX_INVOICE\"),\n",
    "        (\"image1.png\", \"BANK_STATEMENT\"),\n",
    "        (\"image39.png\", \"TAX_INVOICE\"),\n",
    "        (\"image76.png\", \"TAX_INVOICE\"),\n",
    "        (\"image71.png\", \"TAX_INVOICE\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded:\")\n",
    "print(f\"   - Models: {', '.join(CONFIG['test_models'])}\")\n",
    "print(f\"   - Documents: {len(CONFIG['test_images'])} test images\")\n",
    "print(\"   - Prompt: SIMPLIFIED KEY-VALUE format (Australian ABN + dates DD/MM/YYYY)\")\n",
    "print(f\"   - Max tokens: {CONFIG['max_new_tokens']}\")\n",
    "print(f\"   - Quantization: {CONFIG['enable_quantization']}\")\n",
    "print(\"\\nüìã Ready for step-by-step information extraction comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Modular Classes\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# CUDA DIAGNOSTICS - Check GPU availability\n",
    "print(\"üîç CUDA DIAGNOSTICS:\")\n",
    "print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"   GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"   Current Device: {torch.cuda.current_device()}\")\n",
    "    print(f\"   Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Device Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "else:\n",
    "    print(\"   ‚ùå CUDA not available - models will load on CPU!\")\n",
    "\n",
    "class MemoryManager:\n",
    "    \"\"\"Memory management and monitoring utilities for model testing\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def cleanup_gpu_memory():\n",
    "        \"\"\"Minimize memory footprint as requested\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_memory_usage() -> Dict[str, float]:\n",
    "        \"\"\"Get current GPU memory usage in GB\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return {\n",
    "                \"allocated\": torch.cuda.memory_allocated() / 1024**3,\n",
    "                \"reserved\": torch.cuda.memory_reserved() / 1024**3,\n",
    "                \"free\": (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024**3\n",
    "            }\n",
    "        return {\"allocated\": 0.0, \"reserved\": 0.0, \"free\": 0.0}\n",
    "\n",
    "    @staticmethod\n",
    "    def print_memory_usage(label: str = \"Memory\"):\n",
    "        \"\"\"Print formatted memory usage\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            memory = MemoryManager.get_memory_usage()\n",
    "            total_gpu = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f\"   üíæ {label}: {memory['allocated']:.1f}GB allocated | {memory['reserved']:.1f}GB reserved | {memory['free']:.1f}GB free | {total_gpu:.1f}GB total\")\n",
    "        else:\n",
    "            print(f\"   üíæ {label}: No CUDA available\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_memory_delta(before: Dict[str, float], after: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate memory usage delta\"\"\"\n",
    "        return {\n",
    "            \"allocated_delta\": after[\"allocated\"] - before[\"allocated\"],\n",
    "            \"reserved_delta\": after[\"reserved\"] - before[\"reserved\"]\n",
    "        }\n",
    "\n",
    "class UltraAggressiveRepetitionController:\n",
    "    \"\"\"Business document repetition detection and cleanup\"\"\"\n",
    "\n",
    "    def __init__(self, word_threshold: float = 0.15, phrase_threshold: int = 2):\n",
    "        self.word_threshold = word_threshold\n",
    "        self.phrase_threshold = phrase_threshold\n",
    "\n",
    "        # Business document specific repetition patterns\n",
    "        self.toxic_patterns = [\n",
    "            r\"THANK YOU FOR SHOPPING WITH US[^.]*\",\n",
    "            r\"All prices include GST where applicable[^.]*\",\n",
    "            r\"applicable\\.\\s*applicable\\.\",\n",
    "            r\"GST where applicable[^.]*applicable\",\n",
    "            r\"\\\\+[a-zA-Z]*\\{[^}]*\\}\",  # LaTeX artifacts\n",
    "            r\"\\(\\s*\\)\",  # Empty parentheses\n",
    "            r\"[.-]\\s*THANK YOU\",\n",
    "        ]\n",
    "\n",
    "    def clean_response(self, response: str) -> str:\n",
    "        \"\"\"Clean business document extraction response\"\"\"\n",
    "        if not response or len(response.strip()) == 0:\n",
    "            return \"\"\n",
    "\n",
    "        # Remove toxic business document patterns\n",
    "        response = self._remove_business_patterns(response)\n",
    "\n",
    "        # Remove repetitive words and phrases\n",
    "        response = self._remove_word_repetition(response)\n",
    "        response = self._remove_phrase_repetition(response)\n",
    "\n",
    "        # Clean artifacts\n",
    "        response = re.sub(r'\\s+', ' ', response)\n",
    "        response = re.sub(r'[.]{2,}', '.', response)\n",
    "        response = re.sub(r'[!]{2,}', '!', response)\n",
    "\n",
    "        return response.strip()\n",
    "\n",
    "    def _remove_business_patterns(self, text: str) -> str:\n",
    "        \"\"\"Remove business document specific repetitive patterns\"\"\"\n",
    "        for pattern in self.toxic_patterns:\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove excessive \"applicable\" repetition\n",
    "        text = re.sub(r'(applicable\\.\\s*){2,}', 'applicable. ', text, flags=re.IGNORECASE)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _remove_word_repetition(self, text: str) -> str:\n",
    "        \"\"\"Remove word repetition in business documents\"\"\"\n",
    "        # Remove consecutive identical words\n",
    "        text = re.sub(r'\\b(\\w+)(\\s+\\1){1,}', r'\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _remove_phrase_repetition(self, text: str) -> str:\n",
    "        \"\"\"Remove phrase repetition\"\"\"\n",
    "        for phrase_length in range(2, 7):\n",
    "            pattern = r'\\b((?:\\w+\\s+){' + str(phrase_length-1) + r'}\\w+)(\\s+\\1){1,}'\n",
    "            text = re.sub(pattern, r'\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "        return text\n",
    "\n",
    "class KeyValueExtractionAnalyzer:\n",
    "    \"\"\"Analyzer for KEY-VALUE extraction results with realistic Australian business requirements\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def analyze(response: str, img_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze KEY-VALUE extraction results with Australian format but realistic success criteria\"\"\"\n",
    "        response_clean = response.strip()\n",
    "\n",
    "        # Detect KEY-VALUE format (including ABN patterns)\n",
    "        is_structured = bool(re.search(r'(STORE:|ABN:|DATE:|TOTAL:|store_name:|abn:|date:|total:)', response_clean, re.IGNORECASE))\n",
    "\n",
    "        # Extract data from KEY-VALUE format\n",
    "        store_match = re.search(r'(?:STORE|store_name):\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)\n",
    "        abn_match = re.search(r'(?:ABN|abn):\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)\n",
    "        date_match = re.search(r'(?:DATE|date):\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)\n",
    "        total_match = re.search(r'(?:TOTAL|total_amount|total):\\s*\"?([^\"\\n]+)\"?', response_clean, re.IGNORECASE)\n",
    "\n",
    "        # Fallback detection for non-structured responses\n",
    "        if not store_match:\n",
    "            # Australian store patterns\n",
    "            store_match = re.search(r'(spotlight|woolworths|coles|bunnings|officeworks|kmart|target|harvey norman|jb hi-fi)', response_clean, re.IGNORECASE)\n",
    "\n",
    "        if not abn_match:\n",
    "            # ABN patterns: 11 digits, formatted as XX XXX XXX XXX or XXXXXXXXXXX\n",
    "            abn_match = re.search(r'\\b(\\d{2}\\s*\\d{3}\\s*\\d{3}\\s*\\d{3}|\\d{11})\\b', response_clean)\n",
    "            # Also look for \"ABN:\" prefix patterns\n",
    "            if not abn_match:\n",
    "                abn_match = re.search(r'(?:ABN|A\\.B\\.N\\.?)\\s*:?\\s*(\\d{2}\\s*\\d{3}\\s*\\d{3}\\s*\\d{3}|\\d{11})', response_clean, re.IGNORECASE)\n",
    "\n",
    "        if not date_match:\n",
    "            # Australian date format patterns: DD/MM/YYYY, DD-MM-YYYY, DD.MM.YYYY\n",
    "            date_match = re.search(r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{1,2}\\.\\d{1,2}\\.\\d{2,4})\\b', response_clean)\n",
    "\n",
    "        if not total_match:\n",
    "            # Australian currency patterns: $X.XX, AUD X.XX\n",
    "            total_match = re.search(r'(\\$\\d+\\.\\d{2}|\\$\\d+|AUD\\s*\\d+\\.\\d{2})', response_clean)\n",
    "\n",
    "        has_store = bool(store_match)\n",
    "        has_abn = bool(abn_match)\n",
    "        has_date = bool(date_match)\n",
    "        has_total = bool(total_match)\n",
    "\n",
    "        # Core business fields for extraction (STORE, DATE, TOTAL are essential)\n",
    "        core_fields = [has_store, has_date, has_total]\n",
    "        all_fields = [has_store, has_abn, has_date, has_total]\n",
    "\n",
    "        extraction_score = sum(all_fields)\n",
    "        core_score = sum(core_fields)\n",
    "\n",
    "        # REALISTIC SUCCESS CRITERIA:\n",
    "        # Success = at least 2/3 core fields (STORE, DATE, TOTAL)\n",
    "        # ABN is bonus but not required for all document types\n",
    "        successful = core_score >= 2\n",
    "\n",
    "        return {\n",
    "            \"img_name\": img_name,\n",
    "            \"response\": response_clean,\n",
    "            \"is_structured\": is_structured,\n",
    "            \"has_store\": has_store,\n",
    "            \"has_abn\": has_abn,\n",
    "            \"has_date\": has_date,\n",
    "            \"has_total\": has_total,\n",
    "            \"extraction_score\": extraction_score,\n",
    "            \"core_score\": core_score,\n",
    "            \"successful\": successful  # Based on core fields, not ABN requirement\n",
    "        }\n",
    "\n",
    "class DatasetManager:\n",
    "    \"\"\"Dataset verification and management\"\"\"\n",
    "\n",
    "    def __init__(self, datasets_path: str = \"datasets\"):\n",
    "        self.datasets_path = Path(datasets_path)\n",
    "\n",
    "    def verify_images(self, test_images: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Verify that test images exist and return verified list\"\"\"\n",
    "        verified_images = []\n",
    "\n",
    "        for img_name, doc_type in test_images:\n",
    "            img_path = self.datasets_path / img_name\n",
    "            if img_path.exists():\n",
    "                verified_images.append((img_name, doc_type))\n",
    "\n",
    "        return verified_images\n",
    "\n",
    "    def print_verification_report(self, test_images: List[Tuple[str, str]], verified_images: List[Tuple[str, str]]):\n",
    "        \"\"\"Print dataset verification report\"\"\"\n",
    "        print(\"üìä DATASET VERIFICATION\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        for img_name, doc_type in test_images:\n",
    "            img_path = self.datasets_path / img_name\n",
    "            if img_path.exists():\n",
    "                print(f\"   ‚úÖ {img_name:<12} ‚Üí {doc_type}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå {img_name:<12} ‚Üí {doc_type} (MISSING)\")\n",
    "\n",
    "        print(\"\\nüìã Dataset Summary:\")\n",
    "        print(f\"   - Expected: {len(test_images)} documents\")\n",
    "        print(f\"   - Found: {len(verified_images)} documents\")\n",
    "        print(f\"   - Missing: {len(test_images) - len(verified_images)} documents\")\n",
    "\n",
    "        if len(verified_images) == 0:\n",
    "            print(\"‚ùå No test images found! Check datasets/ directory\")\n",
    "            raise FileNotFoundError(\"No test images found\")\n",
    "        elif len(verified_images) < len(test_images):\n",
    "            print(\"‚ö†Ô∏è Some test images missing but proceeding with available images\")\n",
    "        else:\n",
    "            print(\"‚úÖ All test images found\")\n",
    "\n",
    "# Initialize global utilities\n",
    "memory_manager = MemoryManager()\n",
    "repetition_controller = UltraAggressiveRepetitionController()\n",
    "extraction_analyzer = KeyValueExtractionAnalyzer()  # Updated for realistic ABN requirements\n",
    "dataset_manager = DatasetManager()\n",
    "\n",
    "print(\"\\n‚úÖ Modular classes initialized:\")\n",
    "print(\"   - MemoryManager for GPU cleanup and monitoring\")\n",
    "print(\"   - UltraAggressiveRepetitionController for text cleanup\")\n",
    "print(\"   - KeyValueExtractionAnalyzer for REALISTIC Australian business analysis\")\n",
    "print(\"   - DatasetManager for image verification\")\n",
    "print(\"\\nüíæ Initial GPU Status:\")\n",
    "memory_manager.print_memory_usage(\"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Verification\n",
    "# Use the modular DatasetManager class\n",
    "\n",
    "verified_extraction_images = dataset_manager.verify_images(CONFIG[\"test_images\"])\n",
    "dataset_manager.print_verification_report(CONFIG[\"test_images\"], verified_extraction_images)\n",
    "\n",
    "print(\"\\nüî¨ Unified Extraction Prompt:\")\n",
    "print(f\"   {CONFIG['extraction_prompt'][:80]}...\")\n",
    "print(\"\\nüìã Ready for sequential model testing with unified prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading Classes\n",
    "class LlamaModelLoader:\n",
    "    \"\"\"Modular Llama model loader with validation\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(model_path: str, enable_quantization: bool = True):\n",
    "        \"\"\"Load Llama model with proper configuration\"\"\"\n",
    "        from transformers import AutoProcessor, BitsAndBytesConfig, MllamaForConditionalGeneration\n",
    "\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_path, trust_remote_code=True, local_files_only=True\n",
    "        )\n",
    "\n",
    "        model_kwargs = {\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "\n",
    "        if enable_quantization:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                llm_int8_enable_fp32_cpu_offload=True,\n",
    "                llm_int8_skip_modules=[\"vision_tower\", \"multi_modal_projector\"],\n",
    "            )\n",
    "            model_kwargs[\"quantization_config\"] = quantization_config\n",
    "\n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_path, **model_kwargs\n",
    "        )\n",
    "\n",
    "        # EXPLICIT: Set model to eval mode for inference\n",
    "        model.eval()\n",
    "\n",
    "        return model, processor\n",
    "\n",
    "    @staticmethod\n",
    "    def run_inference(model, processor, prompt: str, image, max_new_tokens: int = 64):\n",
    "        \"\"\"Run inference with proper device handling\"\"\"\n",
    "        # SAFETY: Ensure model is in eval mode\n",
    "        model.eval()\n",
    "\n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "        device = next(model.parameters()).device\n",
    "        if device.type != \"cpu\":\n",
    "            device_target = str(device).split(\":\")[0]\n",
    "            inputs = {k: v.to(device_target) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=processor.tokenizer.eos_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "        raw_response = processor.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Cleanup tensors immediately\n",
    "        del inputs, outputs\n",
    "\n",
    "        return raw_response\n",
    "\n",
    "class InternVLModelLoader:\n",
    "    \"\"\"Modular InternVL model loader with validation\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(model_path: str, enable_quantization: bool = True):\n",
    "        \"\"\"Load InternVL model with proper configuration\"\"\"\n",
    "        import warnings\n",
    "\n",
    "        from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "        # Comprehensive warning suppression for InternVL\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_path, trust_remote_code=True, local_files_only=True\n",
    "            )\n",
    "\n",
    "            # Set pad_token_id to eos_token_id to prevent warnings\n",
    "            if tokenizer.pad_token_id is None:\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        model_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"local_files_only\": True\n",
    "        }\n",
    "\n",
    "        if enable_quantization:\n",
    "            # Use BitsAndBytesConfig for proper quantization setup\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                llm_int8_enable_fp32_cpu_offload=True,\n",
    "            )\n",
    "            model_kwargs[\"quantization_config\"] = quantization_config\n",
    "\n",
    "        # Suppress additional loading warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_path, **model_kwargs\n",
    "            )\n",
    "\n",
    "        # EXPLICIT: Set model to eval mode for inference\n",
    "        model.eval()\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def run_inference(model, tokenizer, prompt: str, image, max_new_tokens: int = 64):\n",
    "        \"\"\"Run inference with comprehensive warning suppression\"\"\"\n",
    "        import io\n",
    "        import sys\n",
    "        import warnings\n",
    "\n",
    "        # SAFETY: Ensure model is in eval mode\n",
    "        model.eval()\n",
    "\n",
    "        import torchvision.transforms as T\n",
    "        from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "        transform = T.Compose([\n",
    "            T.Resize((448, 448), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "        pixel_values = transform(image).unsqueeze(0)\n",
    "        if torch.cuda.is_available():\n",
    "            pixel_values = pixel_values.cuda().to(torch.bfloat16).contiguous()\n",
    "\n",
    "        # COMPREHENSIVE warning suppression for model.chat()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "            warnings.filterwarnings(\"ignore\", message=\"Setting `pad_token_id`\")\n",
    "            warnings.filterwarnings(\"ignore\", message=\".*pad_token_id.*\")\n",
    "\n",
    "            # Temporarily capture stderr to suppress print statements\n",
    "            old_stderr = sys.stderr\n",
    "            sys.stderr = buffer = io.StringIO()\n",
    "\n",
    "            try:\n",
    "                raw_response = model.chat(\n",
    "                    tokenizer=tokenizer,\n",
    "                    pixel_values=pixel_values,\n",
    "                    question=prompt,\n",
    "                    generation_config={\"max_new_tokens\": max_new_tokens, \"do_sample\": False}\n",
    "                )\n",
    "            finally:\n",
    "                # Restore stderr\n",
    "                sys.stderr = old_stderr\n",
    "\n",
    "        if isinstance(raw_response, tuple):\n",
    "            raw_response = raw_response[0]\n",
    "\n",
    "        # Cleanup tensors immediately\n",
    "        del pixel_values\n",
    "\n",
    "        return raw_response\n",
    "\n",
    "def validate_model(model_loader_class, model_path: str, config: Dict, model_name: str) -> Tuple[bool, Optional[Any], Optional[Any], float]:\n",
    "    \"\"\"STEP 1: LOAD MODEL FIRST - separate from prompt testing with memory monitoring\"\"\"\n",
    "\n",
    "    # Clean up before loading\n",
    "    memory_manager.cleanup_gpu_memory()\n",
    "    memory_before = memory_manager.get_memory_usage()\n",
    "    memory_manager.print_memory_usage(f\"Pre-{model_name}\")\n",
    "\n",
    "    model_start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        print(f\"üîÑ STEP 1: Loading {model_name.upper()} model from {model_path}...\")\n",
    "\n",
    "        # LOAD MODEL FIRST - no prompts yet\n",
    "        model, processor_or_tokenizer = model_loader_class.load_model(\n",
    "            model_path, config[\"enable_quantization\"]\n",
    "        )\n",
    "\n",
    "        # EXPLICIT: Ensure model is in eval mode\n",
    "        model.eval()\n",
    "\n",
    "        model_load_time = time.time() - model_start_time\n",
    "\n",
    "        # Monitor memory after loading\n",
    "        memory_after = memory_manager.get_memory_usage()\n",
    "        memory_delta = memory_manager.get_memory_delta(memory_before, memory_after)\n",
    "\n",
    "        print(f\"‚úÖ {model_name.upper()} model loaded successfully in {model_load_time:.1f}s (eval mode)\")\n",
    "        memory_manager.print_memory_usage(f\"Post-{model_name}\")\n",
    "        print(f\"   üìä Memory usage: +{memory_delta['allocated_delta']:.1f}GB allocated | +{memory_delta['reserved_delta']:.1f}GB reserved\")\n",
    "\n",
    "        # STEP 2: Simple validation that model can run basic inference\n",
    "        print(f\"üîç STEP 2: Testing basic {model_name.upper()} model functionality...\")\n",
    "        img_path = dataset_manager.datasets_path / \"image14.png\"\n",
    "\n",
    "        if not img_path.exists():\n",
    "            print(f\"‚ùå Test image not found: {img_path}\")\n",
    "            del model, processor_or_tokenizer\n",
    "            memory_manager.cleanup_gpu_memory()\n",
    "            return False, None, None, model_load_time\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Use the simplest possible prompt to test model loading (not extraction quality)\n",
    "        simple_test_prompt = \"<|image|>What do you see?\"\n",
    "\n",
    "        try:\n",
    "            validation_start = time.time()\n",
    "            raw_response = model_loader_class.run_inference(\n",
    "                model, processor_or_tokenizer, simple_test_prompt,\n",
    "                image, 32  # Short response for validation\n",
    "            )\n",
    "            validation_time = time.time() - validation_start\n",
    "\n",
    "            # MODEL VALIDATION: Just check that inference works\n",
    "            if raw_response and len(raw_response.strip()) > 0:\n",
    "                print(f\"‚úÖ {model_name.upper()} model validation passed in {validation_time:.1f}s - inference works\")\n",
    "                print(f\"   Test response: {raw_response[:50]}...\")\n",
    "                memory_manager.print_memory_usage(f\"Ready-{model_name}\")\n",
    "                return True, model, processor_or_tokenizer, model_load_time\n",
    "            else:\n",
    "                print(f\"‚ùå {model_name.upper()} model validation failed - no response\")\n",
    "                del model, processor_or_tokenizer\n",
    "                memory_manager.cleanup_gpu_memory()\n",
    "                return False, None, None, model_load_time\n",
    "\n",
    "        except Exception as inference_error:\n",
    "            print(f\"‚ùå {model_name.upper()} model validation failed - inference error: {str(inference_error)[:100]}...\")\n",
    "            del model, processor_or_tokenizer\n",
    "            memory_manager.cleanup_gpu_memory()\n",
    "            return False, None, None, model_load_time\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {model_name.upper()} model loading failed: {str(e)[:100]}...\")\n",
    "        memory_manager.cleanup_gpu_memory()\n",
    "        return False, None, None, 0.0\n",
    "\n",
    "print(\"‚úÖ Model loader classes defined:\")\n",
    "print(\"   - LlamaModelLoader with validation\")\n",
    "print(\"   - InternVLModelLoader with COMPREHENSIVE warning suppression\")\n",
    "print(\"   - validate_model() - STEP 1: Load model, STEP 2: Test basic inference\")\n",
    "print(\"   - üíæ MEMORY MONITORING: Before/after loading + usage deltas\")\n",
    "print(\"   - üîá SILENCE: Complete suppression of InternVL pad_token_id warnings\")\n",
    "print(\"   - EXPLICIT: Models set to .eval() mode for inference\")\n",
    "print(\"   - UNIFIED: Both models use same prompt (robust for both)\")\n",
    "print(\"   - SEPARATED: Model loading from prompt application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Model Testing - Llama First\n",
    "print(\"üî¨ SEQUENTIAL MODEL TESTING: LLAMA ‚Üí CLEANUP ‚Üí INTERNVL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize results storage\n",
    "extraction_results = {\n",
    "    \"llama\": {\"documents\": [], \"successful\": 0, \"total_time\": 0},\n",
    "    \"internvl\": {\"documents\": [], \"successful\": 0, \"total_time\": 0}\n",
    "}\n",
    "\n",
    "print(\"üî• STEP 1: LOAD LLAMA ‚Üí RUN ALL INFERENCE ‚Üí CLEANUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load Llama model with memory monitoring\n",
    "llama_valid, llama_model, llama_processor, llama_load_time = validate_model(\n",
    "    LlamaModelLoader,\n",
    "    CONFIG[\"model_paths\"][\"llama\"],\n",
    "    CONFIG,\n",
    "    \"llama\"\n",
    ")\n",
    "\n",
    "if llama_valid:\n",
    "    print(\"‚úÖ Llama model loaded - running full extraction test\")\n",
    "    print(\"üéØ Using SIMPLIFIED KEY-VALUE prompt (Australian ABN + dates)\")\n",
    "\n",
    "    # SHOW MEMORY USAGE AFTER MODEL IS LOADED\n",
    "    print(\"\\nüìä LLAMA MODEL MEMORY FOOTPRINT:\")\n",
    "    memory_manager.print_memory_usage(\"Llama-loaded\")\n",
    "    llama_memory = memory_manager.get_memory_usage()\n",
    "    print(f\"   üîß Model size: ~{llama_memory['allocated']:.1f}GB allocated\")\n",
    "    print(f\"   üîß Reserved: ~{llama_memory['reserved']:.1f}GB reserved\")\n",
    "    print(f\"   üîß Available: {llama_memory['free']:.1f}GB remaining\")\n",
    "    print(f\"   ‚è±Ô∏è Load time: {llama_load_time:.1f}s\")\n",
    "    print()\n",
    "\n",
    "    total_inference_time = 0\n",
    "\n",
    "    for i, (img_name, doc_type) in enumerate(verified_extraction_images, 1):\n",
    "        try:\n",
    "            img_path = dataset_manager.datasets_path / img_name\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            inference_start = time.time()\n",
    "\n",
    "            # Use simplified KEY-VALUE extraction prompt with ABN\n",
    "            raw_response = LlamaModelLoader.run_inference(\n",
    "                llama_model, llama_processor, CONFIG[\"extraction_prompt\"],\n",
    "                image, CONFIG[\"max_new_tokens\"]\n",
    "            )\n",
    "\n",
    "            inference_time = time.time() - inference_start\n",
    "            total_inference_time += inference_time\n",
    "\n",
    "            cleaned_response = repetition_controller.clean_response(raw_response)\n",
    "            analysis = extraction_analyzer.analyze(cleaned_response, img_name)\n",
    "            analysis[\"inference_time\"] = inference_time\n",
    "            analysis[\"doc_type\"] = doc_type\n",
    "\n",
    "            extraction_results[\"llama\"][\"documents\"].append(analysis)\n",
    "\n",
    "            if analysis[\"successful\"]:\n",
    "                extraction_results[\"llama\"][\"successful\"] += 1\n",
    "\n",
    "            # DETAILED field-by-field output\n",
    "            status = \"‚úÖ\" if analysis[\"successful\"] else \"‚ùå\"\n",
    "            structured_status = \"S\" if analysis[\"is_structured\"] else \"T\"\n",
    "            abn_status = \"A\" if analysis[\"has_abn\"] else \"-\"\n",
    "\n",
    "            # Show which specific fields were detected\n",
    "            fields_detected = []\n",
    "            if analysis[\"has_store\"]: fields_detected.append(\"STORE\")\n",
    "            if analysis[\"has_abn\"]: fields_detected.append(\"ABN\")\n",
    "            if analysis[\"has_date\"]: fields_detected.append(\"DATE\")\n",
    "            if analysis[\"has_total\"]: fields_detected.append(\"TOTAL\")\n",
    "\n",
    "            fields_str = \"|\".join(fields_detected) if fields_detected else \"none\"\n",
    "\n",
    "            print(f\"   {i:2d}. {img_name:<12} {status} {inference_time:.1f}s | {structured_status}{abn_status} | {analysis['core_score']}/3 core | Fields: {fields_str}\")\n",
    "\n",
    "            # Show raw response for key images that should have ABN\n",
    "            if img_name in [\"image39.png\", \"image76.png\", \"image71.png\"]:\n",
    "                print(f\"       Raw response: {cleaned_response[:100]}...\")\n",
    "\n",
    "            # Immediate tensor cleanup\n",
    "            del image\n",
    "\n",
    "            # Periodic GPU cleanup every 3 images\n",
    "            if i % 3 == 0:\n",
    "                memory_manager.cleanup_gpu_memory()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   {i:2d}. {img_name:<12} ‚ùå Error: {str(e)[:30]}...\")\n",
    "\n",
    "    # Calculate Llama results\n",
    "    extraction_results[\"llama\"][\"total_time\"] = total_inference_time\n",
    "    extraction_results[\"llama\"][\"avg_time\"] = total_inference_time / len(verified_extraction_images)\n",
    "\n",
    "    # Count ABN detection rate\n",
    "    llama_abn_count = sum(1 for doc in extraction_results[\"llama\"][\"documents\"] if doc.get(\"has_abn\", False))\n",
    "\n",
    "    print(\"\\nüìä Llama Results:\")\n",
    "    print(f\"   Success rate: {extraction_results['llama']['successful']}/{len(verified_extraction_images)}\")\n",
    "    print(f\"   ABN detection: {llama_abn_count}/{len(verified_extraction_images)} ({llama_abn_count/len(verified_extraction_images)*100:.1f}%)\")\n",
    "    print(f\"   Average time: {extraction_results['llama']['avg_time']:.1f}s per document\")\n",
    "    print(f\"   Total time: {extraction_results['llama']['total_time']:.1f}s for {len(verified_extraction_images)} documents\")\n",
    "\n",
    "    # DETAILED FIELD ANALYSIS\n",
    "    print(\"\\nüîç DETAILED FIELD ANALYSIS:\")\n",
    "    store_count = sum(1 for doc in extraction_results[\"llama\"][\"documents\"] if doc.get(\"has_store\", False))\n",
    "    abn_count = sum(1 for doc in extraction_results[\"llama\"][\"documents\"] if doc.get(\"has_abn\", False))\n",
    "    date_count = sum(1 for doc in extraction_results[\"llama\"][\"documents\"] if doc.get(\"has_date\", False))\n",
    "    total_count = sum(1 for doc in extraction_results[\"llama\"][\"documents\"] if doc.get(\"has_total\", False))\n",
    "\n",
    "    print(f\"   STORE detection: {store_count}/{len(verified_extraction_images)} ({store_count/len(verified_extraction_images)*100:.1f}%)\")\n",
    "    print(f\"   ABN detection: {abn_count}/{len(verified_extraction_images)} ({abn_count/len(verified_extraction_images)*100:.1f}%)\")\n",
    "    print(f\"   DATE detection: {date_count}/{len(verified_extraction_images)} ({date_count/len(verified_extraction_images)*100:.1f}%)\")\n",
    "    print(f\"   TOTAL detection: {total_count}/{len(verified_extraction_images)} ({total_count/len(verified_extraction_images)*100:.1f}%)\")\n",
    "\n",
    "    # Monitor memory before cleanup\n",
    "    memory_manager.print_memory_usage(\"Before-cleanup\")\n",
    "\n",
    "    # COMPLETE LLAMA CLEANUP\n",
    "    print(\"\\nüßπ STEP 2: COMPLETE LLAMA CLEANUP\")\n",
    "    memory_before_cleanup = memory_manager.get_memory_usage()\n",
    "\n",
    "    del llama_model, llama_processor\n",
    "    memory_manager.cleanup_gpu_memory()\n",
    "\n",
    "    memory_after_cleanup = memory_manager.get_memory_usage()\n",
    "    memory_freed = memory_manager.get_memory_delta(memory_after_cleanup, memory_before_cleanup)\n",
    "\n",
    "    print(\"‚úÖ Llama model cleaned up - memory freed\")\n",
    "    memory_manager.print_memory_usage(\"After-cleanup\")\n",
    "    print(f\"   üìä Memory freed: {abs(memory_freed['allocated_delta']):.1f}GB allocated | {abs(memory_freed['reserved_delta']):.1f}GB reserved\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Llama model validation failed - skipping\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üéØ STEP 3: LOAD INTERNVL ‚Üí RUN ALL INFERENCE ‚Üí CLEANUP\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Model Testing - InternVL Second\n",
    "# Load InternVL model with memory monitoring\n",
    "internvl_valid, internvl_model, internvl_tokenizer, internvl_load_time = validate_model(\n",
    "    InternVLModelLoader,\n",
    "    CONFIG[\"model_paths\"][\"internvl\"],\n",
    "    CONFIG,\n",
    "    \"internvl\"\n",
    ")\n",
    "\n",
    "if internvl_valid:\n",
    "    print(\"‚úÖ InternVL model loaded - running full extraction test\")\n",
    "    print(\"üéØ Using SIMPLIFIED KEY-VALUE prompt (Australian ABN + dates)\")\n",
    "\n",
    "    # SHOW MEMORY USAGE AFTER MODEL IS LOADED\n",
    "    print(\"\\nüìä INTERNVL MODEL MEMORY FOOTPRINT:\")\n",
    "    memory_manager.print_memory_usage(\"InternVL-loaded\")\n",
    "    internvl_memory = memory_manager.get_memory_usage()\n",
    "    print(f\"   üîß Model size: ~{internvl_memory['allocated']:.1f}GB allocated\")\n",
    "    print(f\"   üîß Reserved: ~{internvl_memory['reserved']:.1f}GB reserved\")\n",
    "    print(f\"   üîß Available: {internvl_memory['free']:.1f}GB remaining\")\n",
    "    print(f\"   ‚è±Ô∏è Load time: {internvl_load_time:.1f}s\")\n",
    "    print()\n",
    "\n",
    "    total_inference_time = 0\n",
    "\n",
    "    for i, (img_name, doc_type) in enumerate(verified_extraction_images, 1):\n",
    "        try:\n",
    "            img_path = dataset_manager.datasets_path / img_name\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            inference_start = time.time()\n",
    "\n",
    "            # Use simplified KEY-VALUE extraction prompt with ABN\n",
    "            raw_response = InternVLModelLoader.run_inference(\n",
    "                internvl_model, internvl_tokenizer, CONFIG[\"extraction_prompt\"],\n",
    "                image, CONFIG[\"max_new_tokens\"]\n",
    "            )\n",
    "\n",
    "            inference_time = time.time() - inference_start\n",
    "            total_inference_time += inference_time\n",
    "\n",
    "            cleaned_response = repetition_controller.clean_response(raw_response)\n",
    "            analysis = extraction_analyzer.analyze(cleaned_response, img_name)\n",
    "            analysis[\"inference_time\"] = inference_time\n",
    "            analysis[\"doc_type\"] = doc_type\n",
    "\n",
    "            extraction_results[\"internvl\"][\"documents\"].append(analysis)\n",
    "\n",
    "            if analysis[\"successful\"]:\n",
    "                extraction_results[\"internvl\"][\"successful\"] += 1\n",
    "\n",
    "            # Updated output format to include ABN tracking\n",
    "            status = \"‚úÖ\" if analysis[\"successful\"] else \"‚ùå\"\n",
    "            structured_status = \"S\" if analysis[\"is_structured\"] else \"T\"\n",
    "            abn_status = \"A\" if analysis[\"has_abn\"] else \"-\"\n",
    "            print(f\"   {i:2d}. {img_name:<12} {status} {inference_time:.1f}s | {structured_status}{abn_status} | {analysis['extraction_score']}/4\")\n",
    "\n",
    "            # Immediate tensor cleanup\n",
    "            del image\n",
    "\n",
    "            # Periodic GPU cleanup every 3 images\n",
    "            if i % 3 == 0:\n",
    "                memory_manager.cleanup_gpu_memory()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   {i:2d}. {img_name:<12} ‚ùå Error: {str(e)[:30]}...\")\n",
    "\n",
    "    # Calculate InternVL results\n",
    "    extraction_results[\"internvl\"][\"total_time\"] = total_inference_time\n",
    "    extraction_results[\"internvl\"][\"avg_time\"] = total_inference_time / len(verified_extraction_images)\n",
    "\n",
    "    # Count ABN detection rate\n",
    "    internvl_abn_count = sum(1 for doc in extraction_results[\"internvl\"][\"documents\"] if doc.get(\"has_abn\", False))\n",
    "\n",
    "    print(\"\\nüìä InternVL Results:\")\n",
    "    print(f\"   Success rate: {extraction_results['internvl']['successful']}/{len(verified_extraction_images)}\")\n",
    "    print(f\"   ABN detection: {internvl_abn_count}/{len(verified_extraction_images)} ({internvl_abn_count/len(verified_extraction_images)*100:.1f}%)\")\n",
    "    print(f\"   Average time: {extraction_results['internvl']['avg_time']:.1f}s per document\")\n",
    "    print(f\"   Total time: {extraction_results['internvl']['total_time']:.1f}s for {len(verified_extraction_images)} documents\")\n",
    "\n",
    "    # Monitor memory before cleanup\n",
    "    memory_manager.print_memory_usage(\"Before-cleanup\")\n",
    "\n",
    "    # COMPLETE INTERNVL CLEANUP\n",
    "    print(\"\\nüßπ STEP 4: COMPLETE INTERNVL CLEANUP\")\n",
    "    memory_before_cleanup = memory_manager.get_memory_usage()\n",
    "\n",
    "    del internvl_model, internvl_tokenizer\n",
    "    memory_manager.cleanup_gpu_memory()\n",
    "\n",
    "    memory_after_cleanup = memory_manager.get_memory_usage()\n",
    "    memory_freed = memory_manager.get_memory_delta(memory_after_cleanup, memory_before_cleanup)\n",
    "\n",
    "    print(\"‚úÖ InternVL model cleaned up - memory freed\")\n",
    "    memory_manager.print_memory_usage(\"After-cleanup\")\n",
    "    print(f\"   üìä Memory freed: {abs(memory_freed['allocated_delta']):.1f}GB allocated | {abs(memory_freed['reserved_delta']):.1f}GB reserved\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå InternVL model validation failed - skipping\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üèÜ STEP 5: FINAL COMPARISON\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comparison with Comprehensive Analytics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "class ComprehensiveResultsAnalyzer:\n",
    "    \"\"\"Advanced results analysis with statistical metrics and visualizations\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "\n",
    "    def create_detailed_dataframe(self, extraction_results: Dict, verified_images: List) -> pd.DataFrame:\n",
    "        \"\"\"Create comprehensive DataFrame for analysis\"\"\"\n",
    "        all_results = []\n",
    "\n",
    "        for model_name, results in extraction_results.items():\n",
    "            if not results[\"documents\"]:\n",
    "                continue\n",
    "\n",
    "            for doc in results[\"documents\"]:\n",
    "                all_results.append({\n",
    "                    'model': model_name.upper(),\n",
    "                    'image': doc['img_name'],\n",
    "                    'doc_type': doc['doc_type'],\n",
    "                    'inference_time': doc['inference_time'],\n",
    "                    'is_structured': doc['is_structured'],\n",
    "                    'has_store': doc['has_store'],\n",
    "                    'has_abn': doc['has_abn'],\n",
    "                    'has_date': doc['has_date'],\n",
    "                    'has_total': doc['has_total'],\n",
    "                    'extraction_score': doc['extraction_score'],\n",
    "                    'core_score': doc['core_score'],\n",
    "                    'successful': doc['successful']\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(all_results)\n",
    "\n",
    "    def calculate_field_f1_scores(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Calculate F1 scores for each field and model\"\"\"\n",
    "        fields = ['has_store', 'has_abn', 'has_date', 'has_total']\n",
    "        f1_results = {}\n",
    "\n",
    "        # Ground truth: assume all documents should have these fields (except ABN)\n",
    "        # For synthetic data, we'll use a simplified approach\n",
    "        ground_truth = {\n",
    "            'has_store': [1] * len(df),  # All should have store\n",
    "            'has_abn': [1 if img in ['image39.png', 'image76.png', 'image71.png'] else 0\n",
    "                       for img in df['image']],  # Only specific images have ABN\n",
    "            'has_date': [1] * len(df),   # All should have date\n",
    "            'has_total': [1] * len(df)   # All should have total\n",
    "        }\n",
    "\n",
    "        for model in df['model'].unique():\n",
    "            model_df = df[df['model'] == model]\n",
    "            f1_results[model] = {}\n",
    "\n",
    "            for field in fields:\n",
    "                if len(model_df) > 0:\n",
    "                    # Get predictions for this model\n",
    "                    predictions = model_df[field].values.astype(int)\n",
    "                    # Get corresponding ground truth\n",
    "                    gt_indices = model_df.index\n",
    "                    gt = [ground_truth[field][i] for i in range(len(predictions))]\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    f1 = f1_score(gt, predictions, zero_division=0)\n",
    "                    precision = precision_score(gt, predictions, zero_division=0)\n",
    "                    recall = recall_score(gt, predictions, zero_division=0)\n",
    "\n",
    "                    f1_results[model][field] = {\n",
    "                        'f1': f1,\n",
    "                        'precision': precision,\n",
    "                        'recall': recall\n",
    "                    }\n",
    "\n",
    "        return f1_results\n",
    "\n",
    "    def create_performance_visualizations(self, df: pd.DataFrame, f1_results: Dict):\n",
    "        \"\"\"Create comprehensive performance visualizations\"\"\"\n",
    "\n",
    "        # Set up the plotting environment\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "        # 1. Field Detection Rates Comparison\n",
    "        plt.subplot(2, 3, 1)\n",
    "        fields = ['has_store', 'has_abn', 'has_date', 'has_total']\n",
    "        field_names = ['STORE', 'ABN', 'DATE', 'TOTAL']\n",
    "\n",
    "        detection_rates = []\n",
    "        models = df['model'].unique()\n",
    "\n",
    "        for model in models:\n",
    "            model_df = df[df['model'] == model]\n",
    "            rates = [model_df[field].mean() * 100 for field in fields]\n",
    "            detection_rates.append(rates)\n",
    "\n",
    "        x = np.arange(len(field_names))\n",
    "        width = 0.35\n",
    "\n",
    "        for i, (model, rates) in enumerate(zip(models, detection_rates, strict=False)):\n",
    "            plt.bar(x + i*width, rates, width, label=model, alpha=0.8)\n",
    "\n",
    "        plt.xlabel('Fields')\n",
    "        plt.ylabel('Detection Rate (%)')\n",
    "        plt.title('Field Detection Rates by Model')\n",
    "        plt.xticks(x + width/2, field_names)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. F1 Scores Heatmap\n",
    "        plt.subplot(2, 3, 2)\n",
    "        f1_matrix = []\n",
    "        for model in models:\n",
    "            if model in f1_results:\n",
    "                f1_scores = [f1_results[model][field]['f1'] for field in fields]\n",
    "                f1_matrix.append(f1_scores)\n",
    "\n",
    "        if f1_matrix:\n",
    "            sns.heatmap(f1_matrix, annot=True, fmt='.3f',\n",
    "                       xticklabels=field_names, yticklabels=models,\n",
    "                       cmap='RdYlGn', vmin=0, vmax=1, cbar_kws={'label': 'F1 Score'})\n",
    "            plt.title('F1 Scores by Model and Field')\n",
    "\n",
    "        # 3. Inference Time Distribution\n",
    "        plt.subplot(2, 3, 3)\n",
    "        for model in models:\n",
    "            model_df = df[df['model'] == model]\n",
    "            if len(model_df) > 0:\n",
    "                plt.hist(model_df['inference_time'], alpha=0.7, label=model, bins=10, density=True)\n",
    "\n",
    "        plt.xlabel('Inference Time (seconds)')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Inference Time Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 4. Success Rate by Document Type\n",
    "        plt.subplot(2, 3, 4)\n",
    "        success_by_type = df.groupby(['model', 'doc_type'])['successful'].mean().unstack(fill_value=0)\n",
    "        success_by_type.plot(kind='bar', ax=plt.gca(), width=0.8)\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Success Rate')\n",
    "        plt.title('Success Rate by Document Type')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.legend(title='Document Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 5. Core Score Distribution\n",
    "        plt.subplot(2, 3, 5)\n",
    "        for model in models:\n",
    "            model_df = df[df['model'] == model]\n",
    "            if len(model_df) > 0:\n",
    "                scores = model_df['core_score'].value_counts().sort_index()\n",
    "                plt.plot(scores.index, scores.values, marker='o', label=model, linewidth=2)\n",
    "\n",
    "        plt.xlabel('Core Score (0-3)')\n",
    "        plt.ylabel('Number of Documents')\n",
    "        plt.title('Core Score Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 6. Structured vs Unstructured Output\n",
    "        plt.subplot(2, 3, 6)\n",
    "        structured_rates = df.groupby('model')['is_structured'].mean() * 100\n",
    "        colors = sns.color_palette(\"husl\", len(structured_rates))\n",
    "        bars = plt.bar(structured_rates.index, structured_rates.values, color=colors, alpha=0.8)\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Structured Output Rate (%)')\n",
    "        plt.title('Structured Output Rate by Model')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def print_comprehensive_summary(self, df: pd.DataFrame, f1_results: Dict,\n",
    "                                  extraction_results: Dict, llama_memory: Dict = None,\n",
    "                                  internvl_memory: Dict = None):\n",
    "        \"\"\"Print detailed statistical summary\"\"\"\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"üìä COMPREHENSIVE PERFORMANCE ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # Overall Statistics\n",
    "        print(\"\\nüéØ OVERALL PERFORMANCE METRICS:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        for model in df['model'].unique():\n",
    "            model_df = df[df['model'] == model]\n",
    "            if len(model_df) > 0:\n",
    "                success_rate = model_df['successful'].mean() * 100\n",
    "                avg_time = model_df['inference_time'].mean()\n",
    "                std_time = model_df['inference_time'].std()\n",
    "                structured_rate = model_df['is_structured'].mean() * 100\n",
    "\n",
    "                print(f\"{model}:\")\n",
    "                print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "                print(f\"  Avg Inference Time: {avg_time:.2f}s ¬± {std_time:.2f}s\")\n",
    "                print(f\"  Structured Output: {structured_rate:.1f}%\")\n",
    "                print()\n",
    "\n",
    "        # Field-specific F1 Scores\n",
    "        print(\"üîç FIELD-SPECIFIC F1 SCORES:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        f1_df_data = []\n",
    "        for model, fields in f1_results.items():\n",
    "            for field, metrics in fields.items():\n",
    "                f1_df_data.append({\n",
    "                    'Model': model,\n",
    "                    'Field': field.replace('has_', '').upper(),\n",
    "                    'F1': metrics['f1'],\n",
    "                    'Precision': metrics['precision'],\n",
    "                    'Recall': metrics['recall']\n",
    "                })\n",
    "\n",
    "        if f1_df_data:\n",
    "            f1_df = pd.DataFrame(f1_df_data)\n",
    "            print(f1_df.pivot(index='Field', columns='Model', values='F1').round(3))\n",
    "            print()\n",
    "\n",
    "        # Memory and Performance Summary\n",
    "        print(\"üíæ MEMORY AND PERFORMANCE SUMMARY:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if llama_memory:\n",
    "            print(f\"LLAMA Memory Footprint: {llama_memory.get('allocated', 0):.1f}GB allocated\")\n",
    "        if internvl_memory:\n",
    "            print(f\"INTERNVL Memory Footprint: {internvl_memory.get('allocated', 0):.1f}GB allocated\")\n",
    "\n",
    "        for model, results in extraction_results.items():\n",
    "            if results.get(\"documents\"):\n",
    "                total_time = results.get(\"total_time\", 0)\n",
    "                avg_time = results.get(\"avg_time\", 0)\n",
    "                num_docs = len(results[\"documents\"])\n",
    "\n",
    "                print(f\"{model.upper()}:\")\n",
    "                print(f\"  Total Processing Time: {total_time:.1f}s\")\n",
    "                print(f\"  Throughput: {num_docs/total_time:.2f} docs/sec\")\n",
    "                print(f\"  Avg Time per Document: {avg_time:.2f}s\")\n",
    "\n",
    "        # Recommendations\n",
    "        print(\"\\nü•á RECOMMENDATIONS:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if len(df['model'].unique()) >= 2:\n",
    "            # Compare models\n",
    "            model_summary = df.groupby('model').agg({\n",
    "                'successful': 'mean',\n",
    "                'inference_time': 'mean',\n",
    "                'is_structured': 'mean',\n",
    "                'has_abn': 'mean'\n",
    "            }).round(3)\n",
    "\n",
    "            best_accuracy = model_summary['successful'].idxmax()\n",
    "            best_speed = model_summary['inference_time'].idxmin()\n",
    "            best_abn = model_summary['has_abn'].idxmax()\n",
    "\n",
    "            print(f\"Best Overall Accuracy: {best_accuracy}\")\n",
    "            print(f\"Fastest Inference: {best_speed}\")\n",
    "            print(f\"Best ABN Detection: {best_abn}\")\n",
    "\n",
    "            # Production recommendation\n",
    "            if best_accuracy == best_speed:\n",
    "                print(f\"\\nüéØ PRODUCTION RECOMMENDATION: {best_accuracy}\")\n",
    "                print(\"   Reason: Best accuracy AND fastest inference\")\n",
    "            else:\n",
    "                acc_score = model_summary.loc[best_accuracy, 'successful']\n",
    "                speed_score = 1 / model_summary.loc[best_speed, 'inference_time']\n",
    "\n",
    "                print(f\"\\nüéØ PRODUCTION RECOMMENDATION: {best_accuracy}\")\n",
    "                print(f\"   Reason: Higher accuracy ({acc_score:.1%}) is more important than speed\")\n",
    "\n",
    "# Create analyzer and generate comprehensive results\n",
    "analyzer = ComprehensiveResultsAnalyzer()\n",
    "\n",
    "# Create detailed DataFrame\n",
    "results_df = analyzer.create_detailed_dataframe(extraction_results, verified_extraction_images)\n",
    "\n",
    "if not results_df.empty:\n",
    "    # Calculate F1 scores\n",
    "    f1_scores = analyzer.calculate_field_f1_scores(results_df)\n",
    "\n",
    "    # Get memory information if available\n",
    "    llama_mem = None\n",
    "    internvl_mem = None\n",
    "\n",
    "    # Create visualizations\n",
    "    analyzer.create_performance_visualizations(results_df, f1_scores)\n",
    "\n",
    "    # Print comprehensive summary\n",
    "    analyzer.print_comprehensive_summary(results_df, f1_scores, extraction_results,\n",
    "                                       llama_mem, internvl_mem)\n",
    "\n",
    "    # Export detailed results\n",
    "    print(\"\\nüìä DETAILED RESULTS DATAFRAME:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(results_df.head(10))\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No results available for analysis\")\n",
    "\n",
    "print(\"\\n‚úÖ COMPREHENSIVE ANALYSIS COMPLETE!\")\n",
    "print(\"üìä Generated: Performance metrics, F1 scores, visualizations, and recommendations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
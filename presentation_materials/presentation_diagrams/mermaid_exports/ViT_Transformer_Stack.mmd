graph LR
    subgraph transformer ["Transformer Encoder Stack"]
        A[Encoded Patches] --> B[Multi-Head Self-Attention<br/>Global Context Understanding] --> C[Feed Forward Network<br/>Feature Processing] --> D[Layer Normalization<br/>+ Residual Connection] --> E{More Layers?<br/>12-24 layers}
        E -->|Yes| B
        E -->|No| F[Final Layer Output<br/>Rich semantic representations]
    end
    
    subgraph innovation ["Key Innovation: Self-Attention"]
        G[Each patch sees<br/>EVERY other patch<br/>simultaneously]
        H[Global context<br/>understanding<br/>No information loss]
        I[No OCR dependency<br/>Direct visual learning]
    end
    
    B -.-> G
    G --> H --> I
    
    %% Styling
    classDef transformerNode fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef innovationNode fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef outputNode fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000
    
    class A,B,C,D,E transformerNode
    class F outputNode
    class G,H,I innovationNode
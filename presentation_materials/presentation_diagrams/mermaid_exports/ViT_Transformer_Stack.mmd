graph TB
    subgraph transformer ["Transformer Encoder Stack"]
        A[Encoded Patches<br/>with Position Info] --> B[Multi-Head Self-Attention<br/>Global Context Understanding<br/>Each patch sees EVERY other patch]
        B --> C[Feed Forward Network<br/>Feature Processing<br/>Non-linear transformations]
        C --> D[Layer Normalization<br/>+ Residual Connection<br/>Stable training]
        D --> E{More Layers?<br/>Typically 12-24 layers}
        E -->|Yes| B
        E -->|No| F[Final Layer Output<br/>Rich semantic representations]
    end
    
    subgraph innovation ["Key Innovation: Self-Attention"]
        G[Each patch sees<br/>EVERY other patch<br/>simultaneously]
        H[Global context<br/>understanding<br/>No information loss]
        I[No OCR dependency<br/>Direct visual learning]
        
        G --> H
        H --> I
    end
    
    B -.-> G
    
    %% Styling
    classDef transformerNode fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef innovationNode fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef outputNode fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000
    
    class A,B,C,D,E transformerNode
    class F outputNode
    class G,H,I innovationNode
graph TB
    subgraph "Input Processing"
        A[Document Image<br/>e.g., Invoice, Receipt] --> B[Split into 16x16 Patches]
        B --> C[Linear Projection<br/>Patch Embedding]
        C --> D[Add Position Encoding<br/>Spatial Relationships]
    end
    
    subgraph "Transformer Encoder Stack"
        D --> E[Multi-Head Self-Attention<br/>Global Context Understanding]
        E --> F[Feed Forward Network<br/>Feature Processing]
        F --> G[Layer Normalization<br/>+ Residual Connection]
        G --> H{More Layers?}
        H -->|Yes| E
        H -->|No| I[Final Layer Output]
    end
    
    subgraph "Language Generation Head"
        I --> J[Vision-Language Fusion<br/>Semantic Understanding]
        J --> K[Language Model Head<br/>Text Generation]
        K --> L[Structured Output<br/>KEY: VALUE pairs]
    end
    
    subgraph "Key Innovation: Self-Attention"
        M[Each patch sees<br/>EVERY other patch]
        N[Global context<br/>understanding]
        O[No information loss<br/>like OCR pipelines]
        
        M --> N
        N --> O
    end
    
    %% Connect attention explanation
    E -.-> M
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef transformer fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef output fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef innovation fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    
    class A,B,C,D input
    class E,F,G,H,I transformer
    class J,K,L output
    class M,N,O innovation